# -*- coding: utf-8 -*-
"""
Created on Mon Jul 28 16:55:12 2025

@author: Zhou Yujie
"""

#%% åŒ¹é…LakeATLASã€å…¥æ¹–è´Ÿè·ã€æ°´æ¸©å†°æœŸé—´ç­‰æ¹–æ³Šå±æ€§å€¼  å½¢æˆ'GHGdata_LakeATLAS_final250714.csv'

import pandas as pd

# 1. è¯»å–å¹¶åˆå¹¶ LakeATLAS æ•°æ®
LakeATLAS1 = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\LakeATLAS_v10_pnt_east.csv")
LakeATLAS2 = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\LakeATLAS_v10_pnt_west.csv")
LakeATLAS = pd.concat([LakeATLAS1, LakeATLAS2], axis=0, ignore_index=True)

# 2. è¯»å–åŸºç¡€ GHG æ•°æ®å¹¶åˆ é™¤ä¸éœ€è¦çš„åˆ—
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
# æ·»åŠ è¿™è¡Œï¼šå°†Excelæ•°æ®ä¿å­˜ä¸ºCSVæ–‡ä»¶
GHGdata.to_csv('GHGdata_All250724_attributes_means.csv', index=False, encoding='utf-8')
 
# 3. è¯»å– HydroLAKES æ•°æ®
HydroLAKES = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\HydroLAKES_polys_v10_adjusted.csv")

# 4. è¯»å–è¥å…»ç›è´Ÿè·æ•°æ®
lake_nutrients = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\lake_nutrients_syx.csv")

# 5. è¯»å–æ°´æ¸©æ•°æ®
LakeTEMP = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\LakeTEMP_v1\LakeTEMP_aggregated_v1.csv")

# äººå£ä»¥åŠchlaçš„æ•°æ®-chlaæ•°æ®æ¥è‡ª å­™å»¶é‘«ï¼›äººå£æ•°æ®æ¥è‡ªâ€˜gpw_v4_population_density_rev11_2020_30_sec.tifâ€™ å¤§çº¦ç­‰äº1å…¬é‡ŒÃ—1å…¬é‡Œ
pop_chla = pd.read_csv(r"D:\Code_running\Global_lake_GHG\GSLAKES_model\GHGdata_hjl_add_zyjnewdata\GHGdata_attribute\lakes_with_population_density0221.csv")

# 6. é€æ­¥åˆå¹¶æ‰€æœ‰æ•°æ®
# é¦–å…ˆåˆå¹¶ LakeATLAS
merged_data = pd.merge(
    GHGdata, 
    LakeATLAS, 
    how="left", 
    on='Hylak_id'
)

# åˆå¹¶ HydroLAKES ä¸­çš„æŒ‡å®šåˆ—
merged_data = pd.merge(
    merged_data,
    HydroLAKES[['Hylak_id', 'Continent', 'Centr_lat', 'Centr_long']],
    how="left",
    on='Hylak_id'
)

# åˆå¹¶è¥å…»ç›è´Ÿè·æ•°æ®
merged_data = pd.merge(
    merged_data,
    lake_nutrients,
    how="left",
    on='Hylak_id'
)

# åˆå¹¶æ°´æ¸©æ•°æ®
merged_data = pd.merge(
    merged_data,
    LakeTEMP[['Hylak_id', 'ice_days', 'Tyear_mean_open', 'Tyear_mean']],
    how="left",
    on='Hylak_id'
)


# åˆå¹¶äººå£å’Œchlaæ•°æ®
merged_data = pd.merge(
    merged_data,
    pop_chla[['Hylak_id','Population_Density','Chla_pred_RF']],
    how="left",
    on='Hylak_id'
)

# è®¡ç®—TN_Load_Per_Volumeå’ŒTP_Load_Per_Volume
merged_data['TN_Load_Per_Volume'] = merged_data['TN_Inputs_Mean'] / merged_data['Vol_total']
merged_data['TP_Load_Per_Volume'] = merged_data['TP_Inputs_Mean'] / merged_data['Vol_total']


# 7. åˆ é™¤Hylak_id N2Oä¸ºç©ºçš„è¡Œ
merged_data.dropna(subset=['Hylak_id', 'N2O'], inplace=True)

# 8. åˆ é™¤é‡å¤è¡Œï¼Œä¿ç•™å”¯ä¸€å€¼
merged_data_unique = merged_data.drop_duplicates(
    subset=merged_data.columns.difference(['Num'])
)

# 10. è¾“å‡ºæœ€ç»ˆç»“æœ
merged_data_unique.to_csv('GHGdata_LakeATLAS_final250714.csv', encoding='utf-8', index=False)

# 11. æ‰“å°å„GHGæ°”ä½“çš„æ ·æœ¬é‡ç»Ÿè®¡
print("Sample counts for each GHG:")
print(f"CO2: {merged_data_unique['CO2'].count()}")
print(f"N2O: {merged_data_unique['N2O'].count()}")
print(f"CH4D: {merged_data_unique['CH4D'].count()}")
print(f"CH4E: {merged_data_unique['CH4E'].count()}")


#%% å¯¹æ•°æ®è¿›è¡Œæ¸…æ´— 

import pandas as pd
import numpy as np
import os

def clean_lake_data(input_file, output_file):
    print(f"Reading data from {input_file}...")
    data = pd.read_csv(input_file)
    
    variables = [
        'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
        'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
        'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
        'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
        'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
        'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
        'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
    ]
    
    print("æ­£åœ¨æ¸…ç†æ•°æ®å¼‚å¸¸å€¼...")
    data_cleaned = data.copy()
    
    # 1. æ›¿æ¢-9999çš„å¼‚å¸¸å€¼æ ‡è®°ä¸ºNaN
    for column in data_cleaned.columns:
        # æ£€æŸ¥æ•°å€¼åˆ—
        if pd.api.types.is_numeric_dtype(data_cleaned[column]):
            # æ›¿æ¢-9999ç­‰å¸¸è§çš„ç¼ºå¤±å€¼æ ‡è®°
            mask = (data_cleaned[column] == -9999)
            if mask.any():
                count = mask.sum()
                data_cleaned.loc[mask, column] = np.nan
    
    # 2. ç‰¹æ®Šå¤„ç†ï¼šRes_timeçš„è´Ÿå€¼
    if 'Res_time' in data_cleaned.columns:
        res_time_neg = (data_cleaned['Res_time'] < 0) & (data_cleaned['Res_time'] > -9999)
        if res_time_neg.any():
            count = res_time_neg.sum()
            data_cleaned.loc[res_time_neg, 'Res_time'] = np.nan
    
    # 3. ç‰¹æ®Šå¤„ç†ï¼šå…¶ä»–ç‰¹å®šå˜é‡çš„å¼‚å¸¸è´Ÿå€¼
    hydro_vars = ['Wshd_area', 'ero_kh_vav', 'gwt_cm_vav', 'Dis_avg']
    for var in hydro_vars:
        if var in data_cleaned.columns:
            # åªå¤„ç†ä¸æ˜¯-9999çš„è´Ÿå€¼
            neg_mask = (data_cleaned[var] < 0) & (data_cleaned[var] > -9999)
            if neg_mask.any():
                count = neg_mask.sum()
                data_cleaned.loc[neg_mask, var] = np.nan
    
    # 4. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    print(f"ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ° {output_file}...")
    data_cleaned.to_csv(output_file, index=False)
    print(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç† {len(data_cleaned)} æ¡è®°å½•")
    
    # 5. è¾“å‡ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»è®°å½•æ•°: {len(data_cleaned)}")
    missing_stats = data_cleaned[variables].isnull().sum()
    print("\nå„å˜é‡ç¼ºå¤±å€¼æ•°é‡:")
    for var, count in missing_stats.items():
        if count > 0:
            print(f"  {var}: {count} ({count/len(data_cleaned)*100:.1f}%)")
    
    return data_cleaned

# ä¸»å‡½æ•°
def main():

    # å¤„ç†æ¸©å®¤æ°”ä½“æ•°æ®
    ghg_input = "GHGdata_LakeATLAS_final250714.csv"
    ghg_output = "GHGdata_LakeATLAS_final250714_cleaned.csv"
    clean_lake_data(ghg_input, ghg_output)
    
    print("æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()

#%% å¡«è¡¥ç¼ºå¤±å€¼åï¼Œå®Œå–„GHGdata_LakeATLAS_final250714.csvçš„ç¼ºå¤±æ•°æ®ï¼Œç›´æ¥æ ¹æ®idè¿›è¡ŒåŒ¹é… 250725

import pandas as pd

# è¯»å–å¡«è¡¥ç¼ºå¤±å€¼åçš„ LakeATLAS æ•°æ®
# åªè¯»å–éœ€è¦çš„åˆ—ï¼Œé¿å…å†…å­˜æº¢å‡º
print("æ­£åœ¨è¯»å–LakeATLASæ•°æ®çš„æŒ‡å®šåˆ—...")
LakeATLAS_subset = pd.read_csv(
    'Hydrolakes_LakeATLAS_final250714_cleaned_imputation_simplified.csv',
    usecols=['Hylak_id', 'ice_days', 'Tyear_mean_open','Chla_pred_RF']  # åªè¯»å–éœ€è¦çš„åˆ—
)

GHGdata = pd.read_csv("GHGdata_LakeATLAS_final250714_cleaned.csv")

# åœ¨åˆå¹¶å‰åˆ é™¤GHGdataä¸­çš„é‡å¤åˆ—ï¼Œè¿™æ ·mergeåä¼šä½¿ç”¨LakeATLAS_subsetçš„å€¼
columns_to_replace = ['ice_days', 'Tyear_mean_open', 'Chla_pred_RF']
GHGdata_clean = GHGdata.drop(columns=columns_to_replace, errors='ignore')

# åˆå¹¶æ•°æ®
merged_data = pd.merge(
    GHGdata_clean,
    LakeATLAS_subset,
    how="left",
    on='Hylak_id'
)

# data_n2o = merged_data[~merged_data['N2O'].isna()]
# print(f"å­˜åœ¨çš„ç©ºå€¼åˆ—:\n{data_n2o.isnull().sum()}")


# æ£€æŸ¥åˆå¹¶åçš„æ•°æ®
print(f"åˆå¹¶åçš„æ•°æ®è¡Œæ•°: {len(merged_data)}")
print(f"å­˜åœ¨çš„ç©ºå€¼åˆ—:\n{merged_data.isnull().sum()}")

# ä¿å­˜åˆ°CSVæ–‡ä»¶
merged_data.to_csv("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv", index=False)
print("æ•°æ®å·²ä¿å­˜åˆ° 'GHGdata_LakeATLAS_final250714_cleaned_imputation.csv'")


#%%  ä»¥RMSEä¸ºç›®æ ‡ï¼Œæ„å»ºéšæœºæ£®æ—æ¨¡å‹ï¼Œä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯   0725


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

class ImprovedN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # ä½¿ç”¨RobustScalerè¿›è¡Œç¼©æ”¾
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        return X_scaled, y

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """
        ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯çš„æ”¹è¿›æ¨¡å‹è®­ç»ƒ
        
        Parameters:
        -----------
        X : pandas.DataFrame
            ç‰¹å¾æ•°æ®
        y : pandas.Series  
            ç›®æ ‡å˜é‡
        scoring_metric : str
            è¯„åˆ†æŒ‡æ ‡ï¼Œå¯é€‰ 'neg_mean_squared_error' æˆ– 'r2'
        """
        
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [800, 1000, 1200],
            'max_features': [10, 13, 15],
            'min_samples_leaf': [6, 8, 10],
            'min_samples_split': [15, 20, 25],
            'max_depth': [15, 20, None]
        }
        
        # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
        rf_reg = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True
        )
        
        # ä½¿ç”¨é‡å¤5æŠ˜äº¤å‰éªŒè¯
        # n_repeats=3 è¡¨ç¤ºé‡å¤3æ¬¡ï¼Œæ¯æ¬¡éƒ½æœ‰ä¸åŒçš„éšæœºåˆ’åˆ†
        repeated_cv = RepeatedKFold(
            n_splits=5, 
            n_repeats=3, 
            random_state=self.random_state
        )
        
        print(f"\nUsing Repeated 5-Fold Cross-Validation (3 repeats = 15 total folds)")
        print(f"Scoring metric: {scoring_metric}")
        print("This will take longer but provide more robust parameter selection...")
        
        # ç½‘æ ¼æœç´¢ä¸é‡å¤äº¤å‰éªŒè¯
        grid_search = GridSearchCV(
            estimator=rf_reg,
            param_grid=param_grid,
            cv=repeated_cv,  # ä½¿ç”¨é‡å¤äº¤å‰éªŒè¯
            scoring=scoring_metric,
            n_jobs=-1,
            verbose=1,
            return_train_score=True  # è¿”å›è®­ç»ƒåˆ†æ•°ä»¥æ£€æŸ¥è¿‡æ‹Ÿåˆ
        )
        
        print("Training model with repeated cross-validation...")
        grid_search.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        self.cv_results = grid_search.cv_results_
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœ
        best_score = grid_search.best_score_
        if scoring_metric == 'neg_mean_squared_error':
            print(f"Best CV RMSE: {np.sqrt(-best_score):.4f}")
        else:
            print(f"Best CV RÂ²: {best_score:.4f}")
            
        print("Best parameters:", self.best_params)
        
        # åˆ†æè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°å·®å¼‚ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
        cv_results_df = pd.DataFrame(self.cv_results)
        best_idx = grid_search.best_index_
        
        train_scores = cv_results_df.loc[best_idx, 'mean_train_score']
        val_scores = cv_results_df.loc[best_idx, 'mean_test_score']
        
        if scoring_metric == 'neg_mean_squared_error':
            train_rmse = np.sqrt(-train_scores)
            val_rmse = np.sqrt(-val_scores)
            overfitting_gap = train_rmse - val_rmse
            print(f"Training RMSE: {train_rmse:.4f}")
            print(f"Validation RMSE: {val_rmse:.4f}")
            print(f"Overfitting Gap (Train RMSE - Val RMSE): {overfitting_gap:.4f}")
        else:
            overfitting_gap = train_scores - val_scores
            print(f"Training RÂ²: {train_scores:.4f}")
            print(f"Validation RÂ²: {val_scores:.4f}")
            print(f"Overfitting Gap (Train RÂ² - Val RÂ²): {overfitting_gap:.4f}")
        
        return self.best_model

    def optimized_comprehensive_evaluation(self, X, y):
        """ä¼˜åŒ–çš„é‡å¤äº¤å‰éªŒè¯è¯„ä¼° - å‡å°‘å†—ä½™è®¡ç®—"""
        print("\nPerforming optimized evaluation with Repeated CV...")
        
        # ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=self.random_state)
        
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        from sklearn.model_selection import cross_validate
        
        scoring = ['r2', 'neg_mean_squared_error']
        cv_results = cross_validate(
            self.best_model, X, y, 
            cv=repeated_cv, 
            scoring=scoring,
            return_train_score=False,  # ä¸éœ€è¦è®­ç»ƒåˆ†æ•°ï¼Œå‡å°‘è®¡ç®—
            n_jobs=-1
        )
        
        # æå–ç»“æœ
        r2_scores = cv_results['test_r2']
        mse_scores = cv_results['test_neg_mean_squared_error']
        rmse_log_scores = np.sqrt(-mse_scores)
        
        # åªè®¡ç®—ä¸€æ¬¡åŸå§‹å°ºåº¦çš„RMSEï¼ˆä½¿ç”¨å°‘é‡foldæ ·æœ¬ï¼‰
        original_rmse_scores = []
        sample_folds = list(repeated_cv.split(X))[:5]  # åªç”¨5ä¸ªfoldä»£è¡¨æ€§ä¼°ç®—
        
        for train_idx, val_idx in sample_folds:
            X_val_cv = X.iloc[val_idx]
            y_val_cv = y.iloc[val_idx]
            
            y_pred_cv = self.best_model.predict(X_val_cv)
            
            # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
            y_val_original = 10 ** y_val_cv - 1e-10
            y_pred_original = 10 ** y_pred_cv - 1e-10
            
            original_rmse = np.sqrt(mean_squared_error(y_val_original, y_pred_original))
            original_rmse_scores.append(original_rmse)
        
        original_rmse_scores = np.array(original_rmse_scores)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        results = {
            'cv_r2_mean': r2_scores.mean(),
            'cv_r2_std': r2_scores.std(),
            'cv_r2_scores': r2_scores,
            'cv_rmse_log_mean': rmse_log_scores.mean(),
            'cv_rmse_log_std': rmse_log_scores.std(), 
            'cv_rmse_log_scores': rmse_log_scores,
            'cv_rmse_original_mean': original_rmse_scores.mean(),
            'cv_rmse_original_std': original_rmse_scores.std(),
            'cv_rmse_original_scores': original_rmse_scores,
            'oob_score': getattr(self.best_model, 'oob_score_', None),
            'n_cv_folds': len(r2_scores)
        }
        
        return results
    
    def print_literature_ready_results(self, results):
        """æ‰“å°é€‚åˆæ–‡çŒ®æŠ¥å‘Šçš„ç»“æœ"""
        print("\n" + "="*70)
        print("ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION)")
        print("="*70)
        
        print(f"ğŸ”¬ Model: Random Forest with Repeated 5-Fold Cross-Validation")
        print(f"ğŸ“ˆ Sample size: {len(results['cv_r2_scores'])} folds")
        print(f"ğŸ¯ Features used: {len(self.analysis_vars)}")
        
        print(f"\nğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:")
        print(f"   â€¢ RÂ² = {results['cv_r2_mean']:.3f} Â± {results['cv_r2_std']:.3f}")
        print(f"   â€¢ RMSE = {results['cv_rmse_original_mean']:.2f} Â± {results['cv_rmse_original_std']:.2f} mmol mâ»Â³")
        print(f"   â€¢ Log-scale RMSE = {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
        
        if results['oob_score'] is not None:
            print(f"   â€¢ Out-of-bag Score = {results['oob_score']:.3f}")
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR METHODS SECTION:")
        print(f'   "A Random Forest model was trained using repeated 5-fold cross-validation')
        print(f'    (3 repeats, {results["n_cv_folds"]} total folds) with the following parameters:')
        for param, value in self.best_params.items():
            print(f'    {param}={value},', end=' ')
        print('"')
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:")
        print(f'   "The Random Forest model achieved an RÂ² of {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f}')
        print(f'    and RMSE of {results["cv_rmse_original_mean"]:.2f} Â± {results["cv_rmse_original_std"]:.2f} mmol mâ»Â³')
        print(f'    based on repeated cross-validation."')
        
        print(f"\nâš ï¸  IMPORTANT NOTES:")
        print(f"   â€¢ Use CV results (above) for literature, NOT single split results")
        print(f"   â€¢ The plot RÂ² is from ONE representative split (different from CV RÂ²)")
        print(f"   â€¢ CV results are more robust and should be your primary metrics")
        
        return results

    def plot_cv_stability_analysis(self, results, filename="cv_stability_analysis.png"):
        """ç»˜åˆ¶äº¤å‰éªŒè¯ç¨³å®šæ€§åˆ†æ"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. RÂ²åˆ†æ•°åˆ†å¸ƒ
        axes[0, 0].hist(results['cv_r2_scores'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[0, 0].set_xlabel('RÂ² Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title(f'Distribution of RÂ² Scores\n(Mean Â± Std: {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f})')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSEåˆ†æ•°åˆ†å¸ƒ (log scale)
        axes[0, 1].hist(results['cv_rmse_log_scores'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[0, 1].axvline(results['cv_rmse_log_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_log_mean"]:.3f}')
        axes[0, 1].set_xlabel('RMSE (Log Scale)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Distribution of RMSE (Log Scale)\n(Mean Â± Std: {results["cv_rmse_log_mean"]:.3f} Â± {results["cv_rmse_log_std"]:.3f})')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. åŸå§‹å°ºåº¦RMSEåˆ†å¸ƒ
        axes[1, 0].hist(results['cv_rmse_original_scores'], bins=15, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[1, 0].axvline(results['cv_rmse_original_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_original_mean"]:.2f}')
        axes[1, 0].set_xlabel('RMSE (Original Scale)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title(f'Distribution of RMSE (Original Scale)\n(Mean Â± Std: {results["cv_rmse_original_mean"]:.2f} Â± {results["cv_rmse_original_std"]:.2f})')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. RÂ²åˆ†æ•°è¶‹åŠ¿
        axes[1, 1].plot(results['cv_r2_scores'], 'o-', alpha=0.7, color='darkblue')
        axes[1, 1].axhline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2, 
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[1, 1].fill_between(range(len(results['cv_r2_scores'])), 
                               results['cv_r2_mean'] - results['cv_r2_std'],
                               results['cv_r2_mean'] + results['cv_r2_std'],
                               alpha=0.2, color='red', label=f'Â±1 Std')
        axes[1, 1].set_xlabel('CV Fold Number')
        axes[1, 1].set_ylabel('RÂ² Score')
        axes[1, 1].set_title('RÂ² Score Across CV Folds')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle(f'Cross-Validation Stability Analysis\n({results["n_cv_folds"]} total folds from Repeated 5-Fold CV)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"CV stability analysis saved as: {filename}")
        plt.show()
        plt.close()

    def plot_improved_results_with_repeated_cv(self, X, y, filename="improved_prediction_results_repeated_cv.png"):
        """ä½¿ç”¨é‡å¤äº¤å‰éªŒè¯ç»“æœçš„å¯è§†åŒ–"""
        
        # ä½¿ç”¨ä¸€ä¸ªä»£è¡¨æ€§çš„åˆ’åˆ†è¿›è¡Œå¯è§†åŒ–
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        y_train_pred = self.best_model.predict(X_train)
        y_val_pred = self.best_model.predict(X_val)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred))
        
        # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
        y_val_original = 10 ** y_val - 1e-10
        y_val_pred_original = 10 ** y_val_pred - 1e-10
        y_train_original = 10 ** y_train - 1e-10
        y_train_pred_original = 10 ** y_train_pred - 1e-10
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. éªŒè¯é›†é¢„æµ‹ç»“æœ
        axes[0, 0].scatter(y_val_pred_original, y_val_original, alpha=0.6, c='darkblue', s=30)
        min_val = min(y_val_original.min(), y_val_pred_original.min())
        max_val = max(y_val_original.max(), y_val_pred_original.max())
        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 0].set_xscale('log')
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 0].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 0].set_title(f'Validation Performance (Representative Split)\nRÂ² = {val_r2:.3f}')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è®­ç»ƒé›†é¢„æµ‹ç»“æœ
        axes[0, 1].scatter(y_train_pred_original, y_train_original, alpha=0.6, c='green', s=30)
        min_val = min(y_train_original.min(), y_train_pred_original.min())
        max_val = max(y_train_original.max(), y_train_pred_original.max())
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 1].set_xscale('log')
        axes[0, 1].set_yscale('log')
        axes[0, 1].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 1].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 1].set_title(f'Training Performance (Representative Split)\nRÂ² = {train_r2:.3f}')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®åˆ†æ
        val_residuals = y_val - y_val_pred
        axes[1, 0].scatter(y_val_pred_original, val_residuals, alpha=0.6, c='red', s=30)
        axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=2)
        axes[1, 0].set_xscale('log')
        axes[1, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[1, 0].set_ylabel('Residuals (log scale)')
        axes[1, 0].set_title('Validation Residuals vs Predictions')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ®‹å·®ç›´æ–¹å›¾
        axes[1, 1].hist(val_residuals, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')
        axes[1, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)
        axes[1, 1].set_xlabel('Residuals (log scale)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Distribution of Validation Residuals')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle('Model Performance (Trained with Repeated 5-Fold CV)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Results plot saved as: {filename}")
        plt.show()
        plt.close()
        
    def plot_feature_importance(self, filename="improved_feature_importance.png"):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        features = self.analysis_vars
            
        importances = pd.DataFrame({
            'feature': features,
            'importance': self.best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importances)), importances['importance'])
        plt.yticks(range(len(importances)), importances['feature'])
        plt.xlabel('Feature Importance')
        plt.title('Feature Importance for N2O Prediction (Repeated CV Model)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Feature importance plot saved as: {filename}")
        plt.show()
        plt.close()
        
        return importances


def main():
    """ä¸»å‡½æ•°"""
    predictor = ImprovedN2OPredictor()
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("Loading and preprocessing data...")
    X_scaled, y = predictor.load_and_preprocess_data("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
    
    print(f"Using all {X_scaled.shape[1]} features")
    
    # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
    # å¯ä»¥é€‰æ‹© 'neg_mean_squared_error' (æ¨è) æˆ– 'r2'
    scoring_metric = 'neg_mean_squared_error'  # æˆ–è€…æ”¹ä¸º 'r2'
    
    # ä½¿ç”¨é‡å¤äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
    best_model = predictor.train_improved_model_with_repeated_cv(X_scaled, y, scoring_metric)
    
    # æ¨¡å‹å…¨é¢è¯„ä¼°
    results = predictor.optimized_comprehensive_evaluation(X_scaled, y)
    predictor.print_literature_ready_results(results)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("IMPROVED MODEL PERFORMANCE WITH REPEATED CV")
    print("="*60)
    print(f"Using {X_scaled.shape[1]} features")
    print(f"Scoring metric for GridSearch: {scoring_metric}")
    print(f"Total CV folds for evaluation: {results['n_cv_folds']}")
    print(f"\nRepeated CV Results (5-fold Ã— 5 repeats = 25 folds):")
    print(f"RÂ² (mean Â± std): {results['cv_r2_mean']:.4f} Â± {results['cv_r2_std']:.4f}")
    print(f"Log Scale RMSE (mean Â± std): {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
    print(f"Original Scale RMSE (mean Â± std): {results['cv_rmse_original_mean']:.2f} Â± {results['cv_rmse_original_std']:.2f}")
    
    if results['oob_score'] is not None:
        print(f"OOB Score: {results['oob_score']:.4f}")
    
    print(f"\nBest Model Parameters:")
    for param, value in predictor.best_params.items():
        print(f"  {param}: {value}")
    
    # ç»˜åˆ¶ç¨³å®šæ€§åˆ†æ
    predictor.plot_cv_stability_analysis(results)
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
    predictor.plot_improved_results_with_repeated_cv(X_scaled, y)
    
    # ç‰¹å¾é‡è¦æ€§
    importance_df = predictor.plot_feature_importance()
    print(f"\nTop 5 Most Important Features:")
    print(importance_df.head())
    
    return predictor, results

if __name__ == "__main__":
    predictor, results = main()    
        
    
#%% æ¨¡å‹è¿è¡Œç»“æœ  0725 å­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜

Original data count: 3078
Data count after filtering: 2995

Using Repeated 5-Fold Cross-Validation (3 repeats = 15 total folds)
Scoring metric: neg_mean_squared_error
This will take longer but provide more robust parameter selection...
Training model with repeated cross-validation...
Fitting 15 folds for each of 243 candidates, totalling 3645 fits
Best CV RMSE: 0.5006
Best parameters: {'max_depth': None, 'max_features': 15, 'min_samples_leaf': 6, 'min_samples_split': 15, 'n_estimators': 1200}
Training RMSE: 0.3475
Validation RMSE: 0.5006
Overfitting Gap (Train RMSE - Val RMSE): -0.1531


ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION)
======================================================================
ğŸ”¬ Model: Random Forest with Repeated 5-Fold Cross-Validation
ğŸ“ˆ Sample size: 15 folds
ğŸ¯ Features used: 24

ğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:
   â€¢ RÂ² = 0.586 Â± 0.022
   â€¢ RMSE = 0.39 Â± 0.01 mmol mâ»Â³
   â€¢ Log-scale RMSE = 0.5002 Â± 0.0199
   â€¢ Out-of-bag Score = 0.614

ğŸ“ SUGGESTED TEXT FOR METHODS SECTION:
   "A Random Forest model was trained using repeated 5-fold cross-validation
    (3 repeats, 15 total folds) with the following parameters:
    max_depth=None,     max_features=15,     min_samples_leaf=6,     min_samples_split=15,     n_estimators=1200, "

ğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:
   "The Random Forest model achieved an RÂ² of 0.586 Â± 0.022
    and RMSE of 0.39 Â± 0.01 mmol mâ»Â³
    based on repeated cross-validation."

âš ï¸  IMPORTANT NOTES:
   â€¢ Use CV results (above) for literature, NOT single split results
   â€¢ The plot RÂ² is from ONE representative split (different from CV RÂ²)
   â€¢ CV results are more robust and should be your primary metrics

============================================================
IMPROVED MODEL PERFORMANCE WITH REPEATED CV
============================================================
Using 24 features
Scoring metric for GridSearch: neg_mean_squared_error
Total CV folds for evaluation: 15

Repeated CV Results (5-fold Ã— 5 repeats = 25 folds):
RÂ² (mean Â± std): 0.5863 Â± 0.0224
Log Scale RMSE (mean Â± std): 0.5002 Â± 0.0199
Original Scale RMSE (mean Â± std): 0.39 Â± 0.01
OOB Score: 0.6139

Best Model Parameters:
  max_depth: None
  max_features: 15
  min_samples_leaf: 6
  min_samples_split: 15
  n_estimators: 1200

Top 5 Most Important Features:
                     feature  importance
1                  Elevation    0.150945
19  Log1p_Population_Density    0.116880
5                 ari_ix_lav    0.105159
3                 run_mm_vyr    0.094807
2                 pre_mm_uyr    0.083352


#%%  ä»¥RMSEä¸ºç›®æ ‡ï¼Œæ„å»ºéšæœºæ£®æ—æ¨¡å‹ï¼Œä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯ï¼Œè§£å†³æ•°æ®æ³„éœ²é—®é¢˜   0802


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

class ImprovedN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç† - ä¸è¿›è¡Œscalingï¼Œç•™ç»™Pipelineå¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        print(f"Final data count after removing NaN: {len(X)}")
        
        return X, y

    def create_cv_pipeline(self, X, y):
        """åˆ›å»ºåŒ…å«æ•°æ®é¢„å¤„ç†çš„äº¤å‰éªŒè¯ç®¡é“"""
        
        class RobustScalerTransformer(BaseEstimator, TransformerMixin):
            def __init__(self):
                self.scaler = RobustScaler()
                
            def fit(self, X, y=None):
                self.scaler.fit(X)
                return self
                
            def transform(self, X):
                return self.scaler.transform(X)
        
        # åˆ›å»ºç®¡é“
        pipeline = Pipeline([
            ('scaler', RobustScalerTransformer()),
            ('rf', RandomForestRegressor(
                random_state=self.random_state,
                n_jobs=-1,
                oob_score=True
            ))
        ])
        
        return pipeline

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """
        ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯çš„æ”¹è¿›æ¨¡å‹è®­ç»ƒ - ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜
        
        Parameters:
        -----------
        X : pandas.DataFrame
            ç‰¹å¾æ•°æ®
        y : pandas.Series  
            ç›®æ ‡å˜é‡
        scoring_metric : str
            è¯„åˆ†æŒ‡æ ‡ï¼Œå¯é€‰ 'neg_mean_squared_error' æˆ– 'r2'
        """
        
        # å¹³è¡¡çš„å‚æ•°ç½‘æ ¼ï¼Œä¿æŒæ¨¡å‹å¤æ‚åº¦åŒæ—¶é¿å…ä¸¥é‡è¿‡æ‹Ÿåˆ
        param_grid = {
            'rf__n_estimators': [500, 800, 1000],     # ä¿æŒè¾ƒé«˜çš„æ ‘æ•°é‡
            'rf__max_features': [8, 10, 13, 15],      # æ›´å¤šç‰¹å¾é€‰æ‹©é€‰é¡¹
            'rf__min_samples_leaf': [3, 5, 8],        # é€‚ä¸­çš„å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
            'rf__min_samples_split': [8, 12, 16],     # é€‚ä¸­çš„åˆ†è£‚æœ€å°æ ·æœ¬æ•°
            'rf__max_depth': [15, 20, 25, None]       # åŒ…å«Noneï¼Œå…è®¸æ›´æ·±çš„æ ‘
        }
        
        # åˆ›å»ºç®¡é“
        pipeline = self.create_cv_pipeline(X, y)
        
        # ä½¿ç”¨é‡å¤5æŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(
            n_splits=5, 
            n_repeats=3, 
            random_state=self.random_state
        )
        
        print(f"\nUsing Repeated 5-Fold Cross-Validation (3 repeats = 15 total folds)")
        print(f"Scoring metric: {scoring_metric}")
        print("Training Random Forest model with pipeline to prevent data leakage...")
        
        # ç½‘æ ¼æœç´¢ä¸é‡å¤äº¤å‰éªŒè¯
        grid_search = GridSearchCV(
            estimator=pipeline,
            param_grid=param_grid,
            cv=repeated_cv,
            scoring=scoring_metric,
            n_jobs=-1,
            verbose=1,
            return_train_score=True
        )
        
        print("Training model with repeated cross-validation...")
        grid_search.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        self.cv_results = grid_search.cv_results_
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœ
        best_score = grid_search.best_score_
        if scoring_metric == 'neg_mean_squared_error':
            print(f"Best CV RMSE: {np.sqrt(-best_score):.4f}")
        else:
            print(f"Best CV RÂ²: {best_score:.4f}")
            
        print("Best Random Forest parameters:")
        for key, value in self.best_params.items():
            print(f"  {key}: {value}")
        
        # åˆ†æè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°å·®å¼‚ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
        cv_results_df = pd.DataFrame(self.cv_results)
        best_idx = grid_search.best_index_
        
        train_scores = cv_results_df.loc[best_idx, 'mean_train_score']
        val_scores = cv_results_df.loc[best_idx, 'mean_test_score']
        
        if scoring_metric == 'neg_mean_squared_error':
            train_rmse = np.sqrt(-train_scores)
            val_rmse = np.sqrt(-val_scores)
            overfitting_gap = train_rmse - val_rmse
            print(f"Training RMSE: {train_rmse:.4f}")
            print(f"Validation RMSE: {val_rmse:.4f}")
            print(f"Overfitting Gap (Train RMSE - Val RMSE): {overfitting_gap:.4f}")
        else:
            overfitting_gap = train_scores - val_scores
            print(f"Training RÂ²: {train_scores:.4f}")
            print(f"Validation RÂ²: {val_scores:.4f}")
            print(f"Overfitting Gap (Train RÂ² - Val RÂ²): {overfitting_gap:.4f}")
        
        return self.best_model

    def optimized_comprehensive_evaluation(self, X, y):
        """ä¼˜åŒ–çš„é‡å¤äº¤å‰éªŒè¯è¯„ä¼° - ä¿®å¤ç‰ˆæœ¬"""
        print("\nPerforming optimized evaluation with Repeated CV for Random Forest...")
        
        # ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=self.random_state)
        
        # æ‰‹åŠ¨è¿›è¡Œäº¤å‰éªŒè¯ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ
        r2_scores = []
        rmse_log_scores = []
        rmse_original_scores = []
        oob_scores = []
        
        for train_idx, val_idx in repeated_cv.split(X):
            # åˆ†ç¦»è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            X_train_cv = X.iloc[train_idx]
            X_val_cv = X.iloc[val_idx]
            y_train_cv = y.iloc[train_idx]
            y_val_cv = y.iloc[val_idx]
            
            # åœ¨è®­ç»ƒé›†ä¸Šfit scaler
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train_cv)
            X_val_scaled = scaler.transform(X_val_cv)
            
            # è®­ç»ƒæ¨¡å‹
            rf_model = RandomForestRegressor(**{k.replace('rf__', ''): v for k, v in self.best_params.items()},
                                           random_state=self.random_state,
                                           n_jobs=-1,
                                           oob_score=True)
            
            rf_model.fit(X_train_scaled, y_train_cv)
            
            # é¢„æµ‹
            y_pred_cv = rf_model.predict(X_val_scaled)
            
            # è®¡ç®—æŒ‡æ ‡
            r2 = r2_score(y_val_cv, y_pred_cv)
            rmse_log = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))
            
            # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
            y_val_original = 10 ** y_val_cv - 1e-10
            y_pred_original = 10 ** y_pred_cv - 1e-10
            rmse_original = np.sqrt(mean_squared_error(y_val_original, y_pred_original))
            
            r2_scores.append(r2)
            rmse_log_scores.append(rmse_log)
            rmse_original_scores.append(rmse_original)
            oob_scores.append(rf_model.oob_score_)
        
        r2_scores = np.array(r2_scores)
        rmse_log_scores = np.array(rmse_log_scores)
        rmse_original_scores = np.array(rmse_original_scores)
        oob_scores = np.array(oob_scores)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        results = {
            'cv_r2_mean': r2_scores.mean(),
            'cv_r2_std': r2_scores.std(),
            'cv_r2_scores': r2_scores,
            'cv_rmse_log_mean': rmse_log_scores.mean(),
            'cv_rmse_log_std': rmse_log_scores.std(), 
            'cv_rmse_log_scores': rmse_log_scores,
            'cv_rmse_original_mean': rmse_original_scores.mean(),
            'cv_rmse_original_std': rmse_original_scores.std(),
            'cv_rmse_original_scores': rmse_original_scores,
            'oob_score_mean': oob_scores.mean(),
            'oob_score_std': oob_scores.std(),
            'oob_scores': oob_scores,
            'n_cv_folds': len(r2_scores)
        }
        
        return results
    
    def print_literature_ready_results(self, results):
        """æ‰“å°é€‚åˆæ–‡çŒ®æŠ¥å‘Šçš„ç»“æœ - ä¿®å¤ç‰ˆæœ¬"""
        print("\n" + "="*70)
        print("ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION) - RANDOM FOREST (FIXED)")
        print("="*70)
        
        print(f"ğŸ”¬ Model: Random Forest with Repeated 5-Fold Cross-Validation (No Data Leakage)")
        print(f"ğŸ“ˆ Sample size: {len(results['cv_r2_scores'])} folds")
        print(f"ğŸ¯ Features used: {len(self.analysis_vars)}")
        
        print(f"\nğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:")
        print(f"   â€¢ RÂ² = {results['cv_r2_mean']:.3f} Â± {results['cv_r2_std']:.3f}")
        print(f"   â€¢ RMSE = {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f} mmol mâ»Â³")
        print(f"   â€¢ Log-scale RMSE = {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
        print(f"   â€¢ Out-of-bag Score = {results['oob_score_mean']:.3f} Â± {results['oob_score_std']:.3f}")
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR METHODS SECTION:")
        print(f'   "A Random Forest model was trained using repeated 5-fold cross-validation')
        print(f'    (3 repeats, {results["n_cv_folds"]} total folds) with proper data preprocessing')
        print(f'    to prevent data leakage. The following parameters were optimized:')
        for param, value in self.best_params.items():
            clean_param = param.replace('rf__', '')
            print(f'    {clean_param}={value},', end=' ')
        print('"')
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:")
        print(f'   "The Random Forest model achieved an RÂ² of {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f}')
        print(f'    and RMSE of {results["cv_rmse_original_mean"]:.4f} Â± {results["cv_rmse_original_std"]:.4f} mmol mâ»Â³')
        print(f'    based on repeated cross-validation with proper data preprocessing.')
        print(f'    The out-of-bag score was {results["oob_score_mean"]:.3f} Â± {results["oob_score_std"]:.3f}."')
        
        print(f"\nâœ… IMPROVEMENTS MADE:")
        print(f"   â€¢ Fixed data leakage by preprocessing within each CV fold")
        print(f"   â€¢ Reduced model complexity to prevent overfitting")
        print(f"   â€¢ Proper evaluation without information leakage")
        print(f"   â€¢ Added OOB score evaluation for additional validation")
        
        return results

    def plot_cv_stability_analysis(self, results, filename="rf_cv_stability_analysis_fixed.png"):
        """ç»˜åˆ¶äº¤å‰éªŒè¯ç¨³å®šæ€§åˆ†æ - ä¿®å¤ç‰ˆæœ¬"""
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. RÂ²åˆ†æ•°åˆ†å¸ƒ
        axes[0, 0].hist(results['cv_r2_scores'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[0, 0].set_xlabel('RÂ² Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title(f'Random Forest: Distribution of RÂ² Scores (Fixed)\n(Mean Â± Std: {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f})')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSEåˆ†æ•°åˆ†å¸ƒ (log scale)
        axes[0, 1].hist(results['cv_rmse_log_scores'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[0, 1].axvline(results['cv_rmse_log_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_log_mean"]:.3f}')
        axes[0, 1].set_xlabel('RMSE (Log Scale)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Random Forest: Distribution of RMSE (Log Scale) (Fixed)\n(Mean Â± Std: {results["cv_rmse_log_mean"]:.3f} Â± {results["cv_rmse_log_std"]:.3f})')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. OOBåˆ†æ•°åˆ†å¸ƒ
        axes[0, 2].hist(results['oob_scores'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[0, 2].axvline(results['oob_score_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["oob_score_mean"]:.3f}')
        axes[0, 2].set_xlabel('OOB Score')
        axes[0, 2].set_ylabel('Frequency')
        axes[0, 2].set_title(f'Random Forest: Distribution of OOB Scores (Fixed)\n(Mean Â± Std: {results["oob_score_mean"]:.3f} Â± {results["oob_score_std"]:.3f})')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. åŸå§‹å°ºåº¦RMSEåˆ†å¸ƒ
        axes[1, 0].hist(results['cv_rmse_original_scores'], bins=10, alpha=0.7, color='orange', edgecolor='black')
        axes[1, 0].axvline(results['cv_rmse_original_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_original_mean"]:.4f}')
        axes[1, 0].set_xlabel('RMSE (Original Scale)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title(f'Random Forest: Distribution of RMSE (Original Scale) (Fixed)\n(Mean Â± Std: {results["cv_rmse_original_mean"]:.4f} Â± {results["cv_rmse_original_std"]:.4f})')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. RÂ²åˆ†æ•°è¶‹åŠ¿
        axes[1, 1].plot(results['cv_r2_scores'], 'o-', alpha=0.7, color='darkblue')
        axes[1, 1].axhline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2, 
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[1, 1].fill_between(range(len(results['cv_r2_scores'])), 
                               results['cv_r2_mean'] - results['cv_r2_std'],
                               results['cv_r2_mean'] + results['cv_r2_std'],
                               alpha=0.2, color='red', label=f'Â±1 Std')
        axes[1, 1].set_xlabel('CV Fold Number')
        axes[1, 1].set_ylabel('RÂ² Score')
        axes[1, 1].set_title('Random Forest: RÂ² Score Across CV Folds (Fixed)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. RÂ² vs OOBåˆ†æ•°å¯¹æ¯”
        axes[1, 2].scatter(results['cv_r2_scores'], results['oob_scores'], alpha=0.7, c='purple', s=50)
        axes[1, 2].plot([results['cv_r2_scores'].min(), results['cv_r2_scores'].max()],
                       [results['cv_r2_scores'].min(), results['cv_r2_scores'].max()], 
                       'r--', linewidth=2, label='Perfect Agreement')
        axes[1, 2].set_xlabel('CV RÂ² Score')
        axes[1, 2].set_ylabel('OOB Score')
        axes[1, 2].set_title('Random Forest: CV RÂ² vs OOB Score (Fixed)')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.suptitle(f'Random Forest Cross-Validation Stability Analysis (Fixed - No Data Leakage)\n({results["n_cv_folds"]} total folds from Repeated 5-Fold CV)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Random Forest CV stability analysis (fixed) saved as: {filename}")
        plt.show()
        plt.close()

    def plot_improved_results_with_proper_cv(self, X, y, filename="rf_prediction_results_fixed.png"):
        """ä½¿ç”¨æ­£ç¡®çš„äº¤å‰éªŒè¯æ–¹æ³•çš„å¯è§†åŒ–"""
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ï¼šåœ¨åˆ†ç¦»æ•°æ®åå†è¿›è¡Œé¢„å¤„ç†
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šfit scalerï¼Œç„¶åtransforméªŒè¯é›†
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰
        final_model = RandomForestRegressor(**{k.replace('rf__', ''): v for k, v in self.best_params.items()},
                                          random_state=self.random_state,
                                          n_jobs=-1,
                                          oob_score=True)
        
        final_model.fit(X_train_scaled, y_train)
        
        y_train_pred = final_model.predict(X_train_scaled)
        y_val_pred = final_model.predict(X_val_scaled)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred))
        oob_score = final_model.oob_score_
        
        # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
        y_val_original = 10 ** y_val - 1e-10
        y_val_pred_original = 10 ** y_val_pred - 1e-10
        y_train_original = 10 ** y_train - 1e-10
        y_train_pred_original = 10 ** y_train_pred - 1e-10
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. éªŒè¯é›†é¢„æµ‹ç»“æœ
        axes[0, 0].scatter(y_val_pred_original, y_val_original, alpha=0.6, c='darkblue', s=30)
        min_val = min(y_val_original.min(), y_val_pred_original.min())
        max_val = max(y_val_original.max(), y_val_pred_original.max())
        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 0].set_xscale('log')
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 0].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 0].set_title(f'Random Forest Validation Performance (Fixed)\nRÂ² = {val_r2:.3f}, OOB = {oob_score:.3f}')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è®­ç»ƒé›†é¢„æµ‹ç»“æœ
        axes[0, 1].scatter(y_train_pred_original, y_train_original, alpha=0.6, c='green', s=30)
        min_val = min(y_train_original.min(), y_train_pred_original.min())
        max_val = max(y_train_original.max(), y_train_pred_original.max())
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 1].set_xscale('log')
        axes[0, 1].set_yscale('log')
        axes[0, 1].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 1].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 1].set_title(f'Random Forest Training Performance (Fixed)\nRÂ² = {train_r2:.3f}')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®åˆ†æ
        val_residuals = y_val - y_val_pred
        axes[1, 0].scatter(y_val_pred_original, val_residuals, alpha=0.6, c='red', s=30)
        axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=2)
        axes[1, 0].set_xscale('log')
        axes[1, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[1, 0].set_ylabel('Residuals (log scale)')
        axes[1, 0].set_title('Random Forest Validation Residuals vs Predictions (Fixed)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ®‹å·®ç›´æ–¹å›¾
        axes[1, 1].hist(val_residuals, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')
        axes[1, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)
        axes[1, 1].set_xlabel('Residuals (log scale)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Random Forest Distribution of Validation Residuals (Fixed)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle('Random Forest Model Performance (Fixed - No Data Leakage)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Random Forest results plot (fixed) saved as: {filename}")
        plt.show()
        plt.close()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ä»¥ä¾›ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.final_model = final_model
        
    def plot_feature_importance(self, filename="rf_feature_importance_fixed.png"):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ - ä¿®å¤ç‰ˆæœ¬"""
        if not hasattr(self, 'final_model'):
            print("Warning: No final model available. Please run plot_improved_results_with_proper_cv first.")
            return None
            
        features = self.analysis_vars
            
        importances = pd.DataFrame({
            'feature': features,
            'importance': self.final_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importances)), importances['importance'])
        plt.yticks(range(len(importances)), importances['feature'])
        plt.xlabel('Feature Importance')
        plt.title('Random Forest Feature Importance for N2O Prediction (Fixed Model - No Data Leakage)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Random Forest feature importance plot (fixed) saved as: {filename}")
        plt.show()
        plt.close()
        
        return importances


def main():
    """ä¸»å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    predictor = ImprovedN2OPredictor()
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("Loading and preprocessing data for Random Forest (Fixed Version)...")
    X, y = predictor.load_and_preprocess_data("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
    
    print(f"Using all {X.shape[1]} features for Random Forest")
    
    # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
    scoring_metric = 'neg_mean_squared_error'
    
    # ä½¿ç”¨ä¿®å¤çš„é‡å¤äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
    best_model = predictor.train_improved_model_with_repeated_cv(X, y, scoring_metric)
    
    # æ¨¡å‹å…¨é¢è¯„ä¼° - ä½¿ç”¨ä¿®å¤çš„æ–¹æ³•
    results = predictor.optimized_comprehensive_evaluation(X, y)
    predictor.print_literature_ready_results(results)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("RANDOM FOREST MODEL PERFORMANCE (FIXED - NO DATA LEAKAGE)")
    print("="*60)
    print(f"Using {X.shape[1]} features")
    print(f"Scoring metric for GridSearch: {scoring_metric}")
    print(f"Total CV folds for evaluation: {results['n_cv_folds']}")
    print(f"\nRepeated CV Results (5-fold Ã— 3 repeats = 15 folds):")
    print(f"RÂ² (mean Â± std): {results['cv_r2_mean']:.4f} Â± {results['cv_r2_std']:.4f}")
    print(f"Log Scale RMSE (mean Â± std): {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
    print(f"Original Scale RMSE (mean Â± std): {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f}")
    print(f"OOB Score (mean Â± std): {results['oob_score_mean']:.4f} Â± {results['oob_score_std']:.4f}")
    
    print(f"\nBest Random Forest Parameters (Fixed):")
    for param, value in predictor.best_params.items():
        print(f"  {param}: {value}")
    
    # ç»˜åˆ¶ç¨³å®šæ€§åˆ†æ
    predictor.plot_cv_stability_analysis(results)
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ - ä½¿ç”¨ä¿®å¤çš„æ–¹æ³•
    predictor.plot_improved_results_with_proper_cv(X, y)
    
    # ç‰¹å¾é‡è¦æ€§
    importance_df = predictor.plot_feature_importance()
    if importance_df is not None:
        print(f"\nTop 5 Most Important Features in Random Forest (Fixed):")
        print(importance_df.head())
    
    return predictor, results

if __name__ == "__main__":
    print("Starting Random Forest N2O Prediction Analysis (Fixed Version)...")
    print("="*60)
    predictor, results = main()
    print("\nRandom Forest analysis (fixed) completed successfully!")
    print("\nğŸ”§ FIXES IMPLEMENTED:")
    print("âœ… Eliminated data leakage by preprocessing within each CV fold")
    print("âœ… Used more conservative hyperparameters to prevent overfitting")  
    print("âœ… Proper train/validation split with separate scaling")
    print("âœ… Accurate performance evaluation without information leakage")
    print("âœ… Added comprehensive OOB score analysis for Random Forest")



#%% éšæœºæ£®æ—è¿è¡Œç»“æœ 0802

Original data count: 3078
Data count after filtering: 2995
Final data count after removing NaN: 2862
Using all 24 features for Random Forest

Using Repeated 5-Fold Cross-Validation (3 repeats = 15 total folds)
Scoring metric: neg_mean_squared_error
Training Random Forest model with pipeline to prevent data leakage...
Training model with repeated cross-validation...
Fitting 15 folds for each of 432 candidates, totalling 6480 fits
Best CV RMSE: 0.4814
Best Random Forest parameters:
  rf__max_depth: 25
  rf__max_features: 15
  rf__min_samples_leaf: 3
  rf__min_samples_split: 8
  rf__n_estimators: 800
Training RMSE: 0.2660
Validation RMSE: 0.4814
Overfitting Gap (Train RMSE - Val RMSE): -0.2154


ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION) - RANDOM FOREST (FIXED)
======================================================================
ğŸ”¬ Model: Random Forest with Repeated 5-Fold Cross-Validation (No Data Leakage)
ğŸ“ˆ Sample size: 15 folds
ğŸ¯ Features used: 24

ğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:
   â€¢ RÂ² = 0.611 Â± 0.029
   â€¢ RMSE = 0.4673 Â± 0.0394 mmol mâ»Â³
   â€¢ Log-scale RMSE = 0.4808 Â± 0.0222
   â€¢ Out-of-bag Score = 0.612 Â± 0.006

ğŸ“ SUGGESTED TEXT FOR METHODS SECTION:
   "A Random Forest model was trained using repeated 5-fold cross-validation
    (3 repeats, 15 total folds) with proper data preprocessing
    to prevent data leakage. The following parameters were optimized:
    max_depth=25,     max_features=15,     min_samples_leaf=3,     min_samples_split=8,     n_estimators=800, "

ğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:
   "The Random Forest model achieved an RÂ² of 0.611 Â± 0.029
    and RMSE of 0.4673 Â± 0.0394 mmol mâ»Â³
    based on repeated cross-validation with proper data preprocessing.
    The out-of-bag score was 0.612 Â± 0.006."

âœ… IMPROVEMENTS MADE:
   â€¢ Fixed data leakage by preprocessing within each CV fold
   â€¢ Reduced model complexity to prevent overfitting
   â€¢ Proper evaluation without information leakage
   â€¢ Added OOB score evaluation for additional validation
   
   
   
#%% éšæœºæ£®æ—å‡ºå›¾ 0814


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

class N2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        # ä½¿ç”¨æ‚¨æä¾›çš„æœ€ä½³å‚æ•°
        self.best_params = {
            'rf__max_depth': 25,
            'rf__max_features': 15,
            'rf__min_samples_leaf': 3,
            'rf__min_samples_split': 8,
            'rf__n_estimators': 800
        }
        
    def load_and_preprocess_data(self, filepath):
        """æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        print(f"Final data count after removing NaN: {len(X)}")
        print(f"Using {X.shape[1]} features")
        
        return X, y

def plot_prediction_results_with_marginals(X, y, best_params, random_state=1113, 
                                         filename="rf_prediction_results_with_marginals.png"):
    """
    é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶ç»˜åˆ¶é¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾ï¼ŒåŒ…å«è¾¹ç¼˜æŸ±çŠ¶å›¾
    
    Parameters:
    -----------
    X : pandas.DataFrame
        ç‰¹å¾æ•°æ®
    y : pandas.Series
        ç›®æ ‡å˜é‡
    best_params : dict
        æœ€ä½³æ¨¡å‹å‚æ•°
    random_state : int
        éšæœºç§å­
    filename : str
        ä¿å­˜çš„æ–‡ä»¶å
    """
    
    # è‡ªå®šä¹‰è°ƒè‰²æ¿
    palette = {'Train': '#b4d4e1', 'Test': '#f4ba8a'}
    
    # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ï¼šåœ¨åˆ†ç¦»æ•°æ®åå†è¿›è¡Œé¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )
    
    # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šfit scalerï¼Œç„¶åtransformæµ‹è¯•é›†
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰
    model_params = {k.replace('rf__', ''): v for k, v in best_params.items()}
    model_params.update({
        'random_state': random_state,
        'n_jobs': -1,
        'oob_score': True
    })
    
    final_model = RandomForestRegressor(**model_params)
    print("Training Random Forest model with best parameters...")
    final_model.fit(X_train_scaled, y_train)
    
    # é¢„æµ‹
    y_train_pred = final_model.predict(X_train_scaled)
    y_test_pred = final_model.predict(X_test_scaled)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred))
    oob_score = final_model.oob_score_
    
    # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
    y_train_original = 10 ** y_train - 1e-10
    y_train_pred_original = 10 ** y_train_pred - 1e-10
    y_test_original = 10 ** y_test - 1e-10
    y_test_pred_original = 10 ** y_test_pred - 1e-10
    
    # è®¡ç®—åŸå§‹å°ºåº¦çš„RMSE
    train_rmse_original = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))
    test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))
    
    # åˆ›å»ºæ•°æ®æ¡†ç”¨äºç»˜å›¾
    train_data = pd.DataFrame({
        'Observed': y_train_original,
        'Predicted': y_train_pred_original,
        'Dataset': 'Train'
    })
    
    test_data = pd.DataFrame({
        'Observed': y_test_original,
        'Predicted': y_test_pred_original,
        'Dataset': 'Test'
    })
    
    # åˆå¹¶æ•°æ®
    plot_data = pd.concat([train_data, test_data], ignore_index=True)
    
    # è®¾ç½®matplotlibå’Œseabornæ ·å¼
    plt.style.use('default')
    sns.set_palette("husl")
    
    # åˆ›å»º JointGrid å¯¹è±¡
    g = sns.JointGrid(data=plot_data, x="Observed", y="Predicted", hue="Dataset", 
                      palette=palette, height=8, ratio=5)
    
    # ç»˜åˆ¶ä¸»æ•£ç‚¹å›¾
    g.plot_joint(sns.scatterplot, alpha=0.6, s=30)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
    min_val = min(plot_data['Observed'].min(), plot_data['Predicted'].min())
    max_val = max(plot_data['Observed'].max(), plot_data['Predicted'].max())
    g.ax_joint.plot([min_val, max_val], [min_val, max_val], color='gray', linestyle='--', linewidth=2, 
                    label='Perfect Prediction', alpha=0.8)
    
    # è®¾ç½®å¯¹æ•°åˆ»åº¦
    g.ax_joint.set_xscale('log')
    g.ax_joint.set_yscale('log')
    
    # æ·»åŠ è¾¹ç¼˜çš„æŸ±çŠ¶å›¾
    g.plot_marginals(sns.histplot, kde=False, element='bars', multiple='stack', alpha=0.5)
    # å…³é—­ y è½´çš„è¾¹ç¼˜æŸ±çŠ¶å›¾
    g.ax_marg_y.set_visible(False)
    
    
    # è®¾ç½®åæ ‡è½´æ ‡ç­¾
    g.set_axis_labels('Observed Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', 'Predicted Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼
    g.ax_joint.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
    g.ax_joint.legend(fontsize=10)
    # g.fig.suptitle(f'Random Forest N2O Prediction Results\nTrain RÂ² = {train_r2:.3f}, Test RÂ² = {test_r2:.3f}, OOB = {oob_score:.3f}', 
    #                fontsize=14, y=0.98)
    
    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬æ¡†
    g.ax_joint.text(0.95, 0.05, f'Test $R^2$ = {test_r2:.3f}', 
                    transform=g.ax_joint.transAxes, fontsize=12, 
                    verticalalignment='bottom', horizontalalignment='right',
                    bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
    
    # åœ¨å·¦ä¸Šè§’æ·»åŠ æ¨¡å‹åç§°æ–‡æœ¬
    g.ax_joint.text(0.5, 0.99, 'Random Forest', 
                    transform=g.ax_joint.transAxes, fontsize=12, 
                    verticalalignment='top', horizontalalignment='center',
                    bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
    
       
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    
    # åˆ›å»ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡
    plt.figure(figsize=(8, 6), dpi=1200)
    plt.close()  # å…³é—­ç©ºç™½å›¾
    
    # é‡æ–°ä¿å­˜JointGridå›¾
    g.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"é¢„æµ‹ç»“æœå¯è§†åŒ–å›¾å·²ä¿å­˜ä¸º: {filename}")
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœæ‘˜è¦
    print(f"\n" + "="*60)
    print(f"Random Forest æ¨¡å‹æ€§èƒ½æ‘˜è¦")
    print(f"="*60)
    print(f"æ¨¡å‹å‚æ•°:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")
    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(y_train)}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}")
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"  æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"  OOB åˆ†æ•°: {oob_score:.4f}")
    print(f"  è®­ç»ƒé›† RMSE (log): {train_rmse_log:.4f}")
    print(f"  æµ‹è¯•é›† RMSE (log): {test_rmse_log:.4f}")
    print(f"  è®­ç»ƒé›† RMSE (åŸå§‹): {train_rmse_original:.4f}")
    print(f"  æµ‹è¯•é›† RMSE (åŸå§‹): {test_rmse_original:.4f}")
    
    return final_model, (train_r2, test_r2, oob_score)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
    predictor = N2OPredictor()
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("Loading and preprocessing data...")
    X, y = predictor.load_and_preprocess_data("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶ç»˜åˆ¶ç»“æœ
    print("\nTraining model and creating visualization...")
    model, performance = plot_prediction_results_with_marginals(
        X, y, predictor.best_params, predictor.random_state
    )
    
    print("\nè®­ç»ƒå’Œå¯è§†åŒ–å®Œæˆ!")
    return model, X, y, performance

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    print("å¼€å§‹éšæœºæ£®æ—N2Oé¢„æµ‹åˆ†æ...")
    print("="*60)
    model, X, y, performance = main()

   
   

#%% XGboostæ¨¡å‹ 0801

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

class XGBoostN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        print(f"Final data count after removing NaN: {len(X)}")
        
        return X, y

    def select_features(self, X, y, k=15):
        """ç‰¹å¾é€‰æ‹©ä»¥å‡å°‘è¿‡æ‹Ÿåˆé£é™©"""
        print(f"\nPerforming feature selection (selecting top {k} features)...")
        
        # ä½¿ç”¨SelectKBestè¿›è¡Œç‰¹å¾é€‰æ‹©
        selector = SelectKBest(score_func=f_regression, k=k)
        X_selected = selector.fit_transform(X, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾åç§°
        selected_features = [self.analysis_vars[i] for i in selector.get_support(indices=True)]
        selected_scores = selector.scores_[selector.get_support()]
        
        print(f"Selected {len(selected_features)} features:")
        for feat, score in zip(selected_features, selected_scores):
            print(f"  {feat}: {score:.2f}")
        
        self.selected_features = selected_features
        return pd.DataFrame(X_selected, columns=selected_features, index=X.index)

    def create_cv_pipeline(self):
        """åˆ›å»ºåŒ…å«æ•°æ®é¢„å¤„ç†çš„äº¤å‰éªŒè¯ç®¡é“"""
        from sklearn.pipeline import Pipeline
        from sklearn.base import BaseEstimator, TransformerMixin
        
        class RobustScalerTransformer(BaseEstimator, TransformerMixin):
            def __init__(self):
                self.scaler = RobustScaler()
                
            def fit(self, X, y=None):
                self.scaler.fit(X)
                return self
                
            def transform(self, X):
                return self.scaler.transform(X)
        
        # åˆ›å»ºç®¡é“
        pipeline = Pipeline([
            ('scaler', RobustScalerTransformer()),
            ('xgb', xgb.XGBRegressor(
                random_state=self.random_state,
                n_jobs=-1,
                objective='reg:squarederror',
                eval_metric='rmse'
            ))
        ])
        
        return pipeline

    def train_anti_overfitting_model(self, X, y, scoring_metric='neg_mean_squared_error'):
        """è®­ç»ƒé˜²è¿‡æ‹Ÿåˆçš„XGBoostæ¨¡å‹"""
        
        # é€‚åº¦ä¿å®ˆçš„å‚æ•°ç½‘æ ¼ï¼Œä¸“é—¨é˜²æ­¢è¿‡æ‹Ÿåˆ
        param_grid = {
            # é€‚åº¦å‡å°‘æ ‘çš„æ•°é‡
            'xgb__n_estimators': [50, 100, 200],  # é€‚åº¦å‡å°‘ï¼Œä¸è¿‡äºä¿å®ˆ
            # é™åˆ¶æ ‘çš„æ·±åº¦
            'xgb__max_depth': [2, 3],  # ç¨å¾®æµ…ä¸€äº›
            # é€‚ä¸­çš„å­¦ä¹ ç‡
            'xgb__learning_rate': [0.05, 0.1, 0.15],  # é€‚ä¸­çš„å­¦ä¹ ç‡
            # æ›´å¼ºçš„å­é‡‡æ ·
            'xgb__subsample': [0.7, 0.8],  # é€‚åº¦å­é‡‡æ ·
            'xgb__colsample_bytree': [0.7, 0.8],  # é€‚åº¦ç‰¹å¾é‡‡æ ·
            # æ›´å¼ºçš„æ­£åˆ™åŒ–
            'xgb__reg_alpha': [0.5, 1, 2],  # é€‚åº¦L1æ­£åˆ™åŒ–
            'xgb__reg_lambda': [2, 5, 10],  # é€‚åº¦L2æ­£åˆ™åŒ–
            # æ›´é«˜çš„æœ€å°åˆ†å‰²æŸå¤±
            'xgb__gamma': [0.5, 1, 2],  # é€‚åº¦gammaå‚æ•°
            # å¶å­èŠ‚ç‚¹æœ€å°æƒé‡
            'xgb__min_child_weight': [2, 3, 5]  # é€‚åº¦æœ€å°å­æƒé‡
        }
        
        # åˆ›å»ºç®¡é“
        pipeline = self.create_cv_pipeline()
        
        # ä½¿ç”¨é‡å¤5æŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(
            n_splits=5, 
            n_repeats=3, 
            random_state=self.random_state
        )
        
        print(f"\nUsing Moderate Anti-Overfitting XGBoost Parameters:")
        print(f"- Moderate estimators: [150, 250, 400]")
        print(f"- Controlled tree depth: max_depth [3, 4]")
        print(f"- Balanced learning rate: [0.05, 0.1, 0.15]")
        print(f"- Strong regularization: alpha [0.5, 1, 2], lambda [2, 5, 10]")
        print(f"- Moderate subsampling: [0.7, 0.8]")
        print(f"- Added gamma and min_child_weight constraints")
        print(f"- Using all {X.shape[1]} features")
        
        # ç½‘æ ¼æœç´¢ä¸é‡å¤äº¤å‰éªŒè¯
        grid_search = GridSearchCV(
            estimator=pipeline,
            param_grid=param_grid,
            cv=repeated_cv,
            scoring=scoring_metric,
            n_jobs=-1,
            verbose=1,
            return_train_score=True
        )
        
        print("Training Anti-Overfitting XGBoost model...")
        grid_search.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        self.cv_results = grid_search.cv_results_
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœ
        best_score = grid_search.best_score_
        if scoring_metric == 'neg_mean_squared_error':
            print(f"Best CV RMSE: {np.sqrt(-best_score):.4f}")
        else:
            print(f"Best CV RÂ²: {best_score:.4f}")
            
        print("Best Anti-Overfitting XGBoost parameters:")
        for key, value in self.best_params.items():
            print(f"  {key}: {value}")
        
        # åˆ†æè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°å·®å¼‚ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
        cv_results_df = pd.DataFrame(self.cv_results)
        best_idx = grid_search.best_index_
        
        train_scores = cv_results_df.loc[best_idx, 'mean_train_score']
        val_scores = cv_results_df.loc[best_idx, 'mean_test_score']
        
        if scoring_metric == 'neg_mean_squared_error':
            train_rmse = np.sqrt(-train_scores)
            val_rmse = np.sqrt(-val_scores)
            overfitting_gap = train_rmse - val_rmse
            print(f"Training RMSE: {train_rmse:.4f}")
            print(f"Validation RMSE: {val_rmse:.4f}")
            print(f"Overfitting Gap (Train RMSE - Val RMSE): {overfitting_gap:.4f}")
        else:
            overfitting_gap = train_scores - val_scores
            print(f"Training RÂ²: {train_scores:.4f}")
            print(f"Validation RÂ²: {val_scores:.4f}")
            print(f"Overfitting Gap (Train RÂ² - Val RÂ²): {overfitting_gap:.4f}")
        
        return self.best_model

    def comprehensive_evaluation(self, X, y):
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nPerforming comprehensive evaluation with Repeated CV...")
        
        # ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=self.random_state)
        
        # æ‰‹åŠ¨è¿›è¡Œäº¤å‰éªŒè¯ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ
        r2_scores = []
        rmse_log_scores = []
        rmse_original_scores = []
        
        for train_idx, val_idx in repeated_cv.split(X):
            # åˆ†ç¦»è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            X_train_cv = X.iloc[train_idx]
            X_val_cv = X.iloc[val_idx]
            y_train_cv = y.iloc[train_idx]
            y_val_cv = y.iloc[val_idx]
            
            # åœ¨è®­ç»ƒé›†ä¸Šfit scaler
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train_cv)
            X_val_scaled = scaler.transform(X_val_cv)
            
            # è®­ç»ƒæ¨¡å‹
            xgb_model = xgb.XGBRegressor(**{k.replace('xgb__', ''): v for k, v in self.best_params.items()},
                                        random_state=self.random_state,
                                        n_jobs=-1,
                                        objective='reg:squarederror',
                                        eval_metric='rmse')
            
            xgb_model.fit(X_train_scaled, y_train_cv)
            
            # é¢„æµ‹
            y_pred_cv = xgb_model.predict(X_val_scaled)
            
            # è®¡ç®—æŒ‡æ ‡
            r2 = r2_score(y_val_cv, y_pred_cv)
            rmse_log = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))
            
            # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
            y_val_original = 10 ** y_val_cv - 1e-10
            y_pred_original = 10 ** y_pred_cv - 1e-10
            rmse_original = np.sqrt(mean_squared_error(y_val_original, y_pred_original))
            
            r2_scores.append(r2)
            rmse_log_scores.append(rmse_log)
            rmse_original_scores.append(rmse_original)
        
        r2_scores = np.array(r2_scores)
        rmse_log_scores = np.array(rmse_log_scores)
        rmse_original_scores = np.array(rmse_original_scores)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        results = {
            'cv_r2_mean': r2_scores.mean(),
            'cv_r2_std': r2_scores.std(),
            'cv_r2_scores': r2_scores,
            'cv_rmse_log_mean': rmse_log_scores.mean(),
            'cv_rmse_log_std': rmse_log_scores.std(), 
            'cv_rmse_log_scores': rmse_log_scores,
            'cv_rmse_original_mean': rmse_original_scores.mean(),
            'cv_rmse_original_std': rmse_original_scores.std(),
            'cv_rmse_original_scores': rmse_original_scores,
            'n_cv_folds': len(r2_scores)
        }
        
        return results
    
    def print_anti_overfitting_results(self, results):
        """æ‰“å°é˜²è¿‡æ‹Ÿåˆç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š ANTI-OVERFITTING XGBOOST RESULTS")
        print("="*80)
        
        print(f"ğŸ”¬ Model: Anti-Overfitting XGBoost (All Features)")
        print(f"ğŸ“ˆ Sample size: {len(results['cv_r2_scores'])} folds")
        print(f"ğŸ¯ Features used: All {len(self.analysis_vars)}")
        
        print(f"\nğŸ“‹ PERFORMANCE METRICS:")
        print(f"   â€¢ RÂ² = {results['cv_r2_mean']:.3f} Â± {results['cv_r2_std']:.3f}")
        print(f"   â€¢ RMSE = {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f} mmol mâ»Â³")
        print(f"   â€¢ Log-scale RMSE = {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
        
        print(f"\nğŸ›¡ï¸ ANTI-OVERFITTING MEASURES APPLIED:")
        print(f"   â€¢ Moderate estimator reduction (150-400)")
        print(f"   â€¢ Controlled tree depth (max 4)")
        print(f"   â€¢ Strong regularization (L1 & L2)")
        print(f"   â€¢ Moderate subsampling (0.7-0.8)")
        print(f"   â€¢ Higher minimum child weight")
        print(f"   â€¢ Gamma parameter for pruning")
        
        return results

    def plot_anti_overfitting_results(self, X, y, filename="anti_overfitting_results.png"):
        """å¯è§†åŒ–é˜²è¿‡æ‹Ÿåˆç»“æœ"""
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ï¼šåœ¨åˆ†ç¦»æ•°æ®åå†è¿›è¡Œé¢„å¤„ç†
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šfit scalerï¼Œç„¶åtransforméªŒè¯é›†
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰
        final_model = xgb.XGBRegressor(**{k.replace('xgb__', ''): v for k, v in self.best_params.items()},
                                      random_state=self.random_state,
                                      n_jobs=-1,
                                      objective='reg:squarederror',
                                      eval_metric='rmse')
        
        final_model.fit(X_train_scaled, y_train)
        
        y_train_pred = final_model.predict(X_train_scaled)
        y_val_pred = final_model.predict(X_val_scaled)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred))
        
        # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
        y_val_original = 10 ** y_val - 1e-10
        y_val_pred_original = 10 ** y_val_pred - 1e-10
        y_train_original = 10 ** y_train - 1e-10
        y_train_pred_original = 10 ** y_train_pred - 1e-10
        
        # è®¡ç®—è¿‡æ‹ŸåˆæŒ‡æ ‡
        r2_gap = train_r2 - val_r2
        rmse_gap = val_rmse_log - train_rmse_log
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. éªŒè¯é›†é¢„æµ‹ç»“æœ
        axes[0, 0].scatter(y_val_pred_original, y_val_original, alpha=0.6, c='darkblue', s=30)
        min_val = min(y_val_original.min(), y_val_pred_original.min())
        max_val = max(y_val_original.max(), y_val_pred_original.max())
        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 0].set_xscale('log')
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 0].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 0].set_title(f'Anti-Overfitting XGBoost - Validation\nRÂ² = {val_r2:.3f}')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è®­ç»ƒé›†é¢„æµ‹ç»“æœ
        axes[0, 1].scatter(y_train_pred_original, y_train_original, alpha=0.6, c='green', s=30)
        min_val = min(y_train_original.min(), y_train_pred_original.min())
        max_val = max(y_train_original.max(), y_train_pred_original.max())
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 1].set_xscale('log')
        axes[0, 1].set_yscale('log')
        axes[0, 1].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 1].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 1].set_title(f'Anti-Overfitting XGBoost - Training\nRÂ² = {train_r2:.3f}')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è¿‡æ‹Ÿåˆåˆ†æ
        metrics = ['RÂ² Score', 'RMSE (log)']
        train_vals = [train_r2, train_rmse_log]
        val_vals = [val_r2, val_rmse_log]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, train_vals, width, label='Training', color='green', alpha=0.7)
        axes[1, 0].bar(x + width/2, val_vals, width, label='Validation', color='blue', alpha=0.7)
        
        axes[1, 0].set_xlabel('Metrics')
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title(f'Training vs Validation Performance\nRÂ² Gap: {r2_gap:.3f}, RMSE Gap: {rmse_gap:.3f}')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(metrics)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ®‹å·®åˆ†æ
        val_residuals = y_val - y_val_pred
        axes[1, 1].scatter(y_val_pred_original, val_residuals, alpha=0.6, c='red', s=30)
        axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=2)
        axes[1, 1].set_xscale('log')
        axes[1, 1].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[1, 1].set_ylabel('Residuals (log scale)')
        axes[1, 1].set_title('Anti-Overfitting Model - Residuals')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.suptitle(f'Anti-Overfitting XGBoost Model Performance\nOverfitting Reduced: RÂ² Gap = {r2_gap:.3f}')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Anti-overfitting results plot saved as: {filename}")
        plt.show()
        plt.close()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ä»¥ä¾›ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.final_model = final_model
        
        return train_r2, val_r2, r2_gap
        
    def plot_feature_importance(self, filename="anti_overfitting_feature_importance.png"):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if not hasattr(self, 'final_model'):
            print("Warning: No final model available. Please run plot_anti_overfitting_results first.")
            return None
            
        features = self.analysis_vars  # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            
        importances = pd.DataFrame({
            'feature': features,
            'importance': self.final_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importances)), importances['importance'])
        plt.yticks(range(len(importances)), importances['feature'])
        plt.xlabel('Feature Importance')
        plt.title('Anti-Overfitting XGBoost Feature Importance (All Features)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Feature importance plot saved as: {filename}")
        plt.show()
        plt.close()
        
        return importances


def main():
    """ä¸»å‡½æ•° - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬"""
    predictor = XGBoostN2OPredictor()
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("Loading and preprocessing data for Anti-Overfitting XGBoost...")
    X, y = predictor.load_and_preprocess_data("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
    
    print(f"Using all {X.shape[1]} features for XGBoost (no feature selection)")
    
    # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
    scoring_metric = 'neg_mean_squared_error'
    
    # ä½¿ç”¨é˜²è¿‡æ‹Ÿåˆçš„æ–¹æ³•è®­ç»ƒXGBoostæ¨¡å‹
    best_model = predictor.train_anti_overfitting_model(X, y, scoring_metric)
    
    # æ¨¡å‹å…¨é¢è¯„ä¼°
    results = predictor.comprehensive_evaluation(X, y)
    predictor.print_anti_overfitting_results(results)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("ANTI-OVERFITTING XGBOOST MODEL PERFORMANCE")
    print("="*60)
    print(f"Features used: {X.shape[1]} (all features)")
    print(f"Scoring metric for GridSearch: {scoring_metric}")
    print(f"Total CV folds for evaluation: {results['n_cv_folds']}")
    
    print(f"\nRepeated CV Results (5-fold Ã— 3 repeats = 15 folds):")
    print(f"RÂ² (mean Â± std): {results['cv_r2_mean']:.4f} Â± {results['cv_r2_std']:.4f}")
    print(f"Log Scale RMSE (mean Â± std): {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
    print(f"Original Scale RMSE (mean Â± std): {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f}")
    
    print(f"\nBest Anti-Overfitting XGBoost Parameters:")
    for param, value in predictor.best_params.items():
        print(f"  {param}: {value}")
    
    # ç»˜åˆ¶é˜²è¿‡æ‹Ÿåˆç»“æœ
    train_r2, val_r2, r2_gap = predictor.plot_anti_overfitting_results(X, y)
    
    print(f"\nğŸ¯ OVERFITTING CHECK:")
    print(f"Training RÂ²: {train_r2:.3f}")
    print(f"Validation RÂ²: {val_r2:.3f}")
    print(f"RÂ² Gap (overfitting indicator): {r2_gap:.3f}")
    
    if r2_gap < 0.1:
        print("âœ… Good! Overfitting is well controlled (RÂ² gap < 0.1)")
    elif r2_gap < 0.2:
        print("âš ï¸  Moderate overfitting (RÂ² gap 0.1-0.2)")
    else:
        print("âŒ Still overfitting (RÂ² gap > 0.2)")
    
    # ç‰¹å¾é‡è¦æ€§
    importance_df = predictor.plot_feature_importance()
    if importance_df is not None:
        print(f"\nTop 5 Most Important Features:")
        print(importance_df.head())
    
    return predictor, results

if __name__ == "__main__":
    print("Starting Anti-Overfitting XGBoost N2O Prediction Analysis...")
    print("="*70)
    predictor, results = main()
    print("\nAnti-Overfitting XGBoost analysis completed successfully!")
    print("\nğŸ›¡ï¸ ANTI-OVERFITTING MEASURES IMPLEMENTED:")
    print("âœ… Keep all 24 features (no feature selection)")
    print("âœ… Moderate estimators (150-400 instead of 300-800)")
    print("âœ… Controlled tree depth (max 4 instead of 5)")
    print("âœ… Balanced learning rate (0.05-0.15)")
    print("âœ… Strong L1 and L2 regularization")
    print("âœ… Moderate subsampling (0.7-0.8)")
    print("âœ… Added gamma and min_child_weight constraints")
    print("âœ… Proper cross-validation without data leakage")


#%% XGboost è¿è¡Œç»“æœ 0801

Loading and preprocessing data for Anti-Overfitting XGBoost...
Original data count: 3078
Data count after filtering: 2995
Final data count after removing NaN: 2862
Using all 24 features for XGBoost (no feature selection)

Using Moderate Anti-Overfitting XGBoost Parameters:
- Moderate estimators: [150, 250, 400]
- Controlled tree depth: max_depth [3, 4]
- Balanced learning rate: [0.05, 0.1, 0.15]
- Strong regularization: alpha [0.5, 1, 2], lambda [2, 5, 10]
- Moderate subsampling: [0.7, 0.8]
- Added gamma and min_child_weight constraints
- Using all 24 features
Training Anti-Overfitting XGBoost model...
Fitting 15 folds for each of 5832 candidates, totalling 87480 fits
Best CV RMSE: 0.5133
Best Anti-Overfitting XGBoost parameters:
  xgb__colsample_bytree: 0.8
  xgb__gamma: 0.5
  xgb__learning_rate: 0.15
  xgb__max_depth: 3
  xgb__min_child_weight: 3
  xgb__n_estimators: 200
  xgb__reg_alpha: 1
  xgb__reg_lambda: 5
  xgb__subsample: 0.7
  
Training RMSE: 0.3671
Validation RMSE: 0.5133
Overfitting Gap (Train RMSE - Val RMSE): -0.1461


ğŸ“Š ANTI-OVERFITTING XGBOOST RESULTS
================================================================================
ğŸ”¬ Model: Anti-Overfitting XGBoost (All Features)
ğŸ“ˆ Sample size: 15 folds
ğŸ¯ Features used: All 24

ğŸ“‹ PERFORMANCE METRICS:
   â€¢ RÂ² = 0.556 Â± 0.038
   â€¢ RMSE = 0.4879 Â± 0.0411 mmol mâ»Â³
   â€¢ Log-scale RMSE = 0.5129 Â± 0.0185

ğŸ›¡ï¸ ANTI-OVERFITTING MEASURES APPLIED:
   â€¢ Moderate estimator reduction (150-400)
   â€¢ Controlled tree depth (max 4)
   â€¢ Strong regularization (L1 & L2)
   â€¢ Moderate subsampling (0.7-0.8)
   â€¢ Higher minimum child weight
   â€¢ Gamma parameter for pruning

============================================================
ANTI-OVERFITTING XGBOOST MODEL PERFORMANCE
============================================================
Features used: 24 (all features)
Scoring metric for GridSearch: neg_mean_squared_error
Total CV folds for evaluation: 15

Repeated CV Results (5-fold Ã— 3 repeats = 15 folds):
RÂ² (mean Â± std): 0.5563 Â± 0.0377
Log Scale RMSE (mean Â± std): 0.5129 Â± 0.0185
Original Scale RMSE (mean Â± std): 0.4879 Â± 0.0411

Best Anti-Overfitting XGBoost Parameters:
  xgb__colsample_bytree: 0.8
  xgb__gamma: 0.5
  xgb__learning_rate: 0.15
  xgb__max_depth: 3
  xgb__min_child_weight: 3
  xgb__n_estimators: 200
  xgb__reg_alpha: 1
  xgb__reg_lambda: 5
  xgb__subsample: 0.7
Anti-overfitting results plot saved as: anti_overfitting_results.png

ğŸ¯ OVERFITTING CHECK:
Training RÂ²: 0.776
Validation RÂ²: 0.583
RÂ² Gap (overfitting indicator): 0.194
âš ï¸  Moderate overfitting (RÂ² gap 0.1-0.2)
Feature importance plot saved as: anti_overfitting_feature_importance.png

Top 5 Most Important Features:
                     feature  importance
13           Log1p_Lake_area    0.101221
1                  Elevation    0.076635
7                 crp_pc_vse    0.075454
2                 pre_mm_uyr    0.074304
19  Log1p_Population_Density    0.063641

#%% XGboost å‡ºå›¾ 0814


import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

class XGBoostN2OVisualization:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        # æ‚¨çš„æœ€ä½³XGBoostå‚æ•°
        self.best_params = {
            'colsample_bytree': 0.8,
            'gamma': 0.5,
            'learning_rate': 0.15,
            'max_depth': 3,
            'min_child_weight': 3,
            'n_estimators': 200,
            'reg_alpha': 1,
            'reg_lambda': 5,
            'subsample': 0.7
        }
        
    def load_and_preprocess_data(self, filepath):
        """æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        print(f"Final data count after removing NaN: {len(X)}")
        print(f"Features used: {X.shape[1]} features")
        
        return X, y

    def plot_xgboost_prediction_results_with_marginals(self, filepath, 
                                                      filename="xgboost_prediction_results_with_marginals.png"):
        """
        å®Œæ•´æµç¨‹ï¼šæ•°æ®é¢„å¤„ç† -> æ¨¡å‹è®­ç»ƒ -> å¯è§†åŒ–ç»“æœ
        
        Parameters:
        -----------
        filepath : str
            æ•°æ®æ–‡ä»¶è·¯å¾„
        filename : str
            ä¿å­˜çš„æ–‡ä»¶å
        """
        
        # 1. æ•°æ®é¢„å¤„ç†
        print("="*60)
        print("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        X, y = self.load_and_preprocess_data(filepath)
        
        # 2. è‡ªå®šä¹‰è°ƒè‰²æ¿
        palette = {'Train': '#b4d4e1', 'Test': '#f4ba8a'}
        
        # 3. æ•°æ®åˆ†å‰²
        print(f"ğŸ“Š åˆ†å‰²æ•°æ®é›†...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        # 4. ç‰¹å¾ç¼©æ”¾
        print(f"ğŸ”§ ç‰¹å¾ç¼©æ”¾...")
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 5. è®­ç»ƒXGBoostæ¨¡å‹
        print(f"ğŸš€ è®­ç»ƒXGBoostæ¨¡å‹ (ä½¿ç”¨æœ€ä½³åè¿‡æ‹Ÿåˆå‚æ•°)...")
        model_params = self.best_params.copy()
        model_params.update({
            'random_state': self.random_state,
            'n_jobs': -1,
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse'
        })
        
        final_model = xgb.XGBRegressor(**model_params)
        final_model.fit(X_train_scaled, y_train)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“‹ ä½¿ç”¨çš„å‚æ•°:")
        for param, value in self.best_params.items():
            print(f"   {param}: {value}")
        
        # 6. é¢„æµ‹
        y_train_pred = final_model.predict(X_train_scaled)
        y_test_pred = final_model.predict(X_test_scaled)
        
        # 7. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred))
        
        # 8. è½¬æ¢åˆ°åŸå§‹å°ºåº¦
        y_train_original = 10 ** y_train - 1e-10
        y_train_pred_original = 10 ** y_train_pred - 1e-10
        y_test_original = 10 ** y_test - 1e-10
        y_test_pred_original = 10 ** y_test_pred - 1e-10
        
        # è®¡ç®—åŸå§‹å°ºåº¦çš„RMSE
        train_rmse_original = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))
        test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))
        
        # 9. åˆ›å»ºæ•°æ®æ¡†ç”¨äºç»˜å›¾
        train_data = pd.DataFrame({
            'Observed': y_train_original,
            'Predicted': y_train_pred_original,
            'Dataset': 'Train'
        })
        
        test_data = pd.DataFrame({
            'Observed': y_test_original,
            'Predicted': y_test_pred_original,
            'Dataset': 'Test'
        })
        
        # åˆå¹¶æ•°æ®
        plot_data = pd.concat([train_data, test_data], ignore_index=True)
        
        # 10. è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 11. åˆ›å»º JointGrid å¯¹è±¡
        g = sns.JointGrid(data=plot_data, x="Observed", y="Predicted", hue="Dataset", 
                          palette=palette, height=8, ratio=5)
        
        # 12. ç»˜åˆ¶ä¸»æ•£ç‚¹å›¾
        g.plot_joint(sns.scatterplot, alpha=0.6, s=30)
        
        # 13. æ·»åŠ å®Œç¾é¢„æµ‹çº¿
        min_val = min(plot_data['Observed'].min(), plot_data['Predicted'].min())
        max_val = max(plot_data['Observed'].max(), plot_data['Predicted'].max())
        g.ax_joint.plot([min_val, max_val], [min_val, max_val], color='gray', linestyle='--', linewidth=2, 
                        label='Perfect Prediction', alpha=0.8)
        
        # 14. è®¾ç½®å¯¹æ•°åˆ»åº¦
        g.ax_joint.set_xscale('log')
        g.ax_joint.set_yscale('log')
        
        # 15. æ·»åŠ è¾¹ç¼˜çš„æŸ±çŠ¶å›¾
        g.plot_marginals(sns.histplot, kde=False, element='bars', multiple='stack', alpha=0.5)
        # å…³é—­ y è½´çš„è¾¹ç¼˜æŸ±çŠ¶å›¾
        g.ax_marg_y.set_visible(False)
        
        # 16. è®¾ç½®åæ ‡è½´æ ‡ç­¾
        g.set_axis_labels('Observed Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', 'Predicted Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', fontsize=12)
        
        # 17. æ·»åŠ ç½‘æ ¼
        g.ax_joint.grid(True, alpha=0.3)
        
        # 18. æ·»åŠ å›¾ä¾‹
        g.ax_joint.legend(fontsize=10)
        
        # 19. æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬æ¡†
        g.ax_joint.text(0.95, 0.05, f'Test $R^2$ = {test_r2:.3f}', 
                        transform=g.ax_joint.transAxes, fontsize=12, 
                        verticalalignment='bottom', horizontalalignment='right',
                        bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
        
        # 20. åœ¨å·¦ä¸Šè§’æ·»åŠ æ¨¡å‹åç§°æ–‡æœ¬
        g.ax_joint.text(0.5, 0.99, 'XGBoost', 
                        transform=g.ax_joint.transAxes, fontsize=12, 
                        verticalalignment='top', horizontalalignment='center',
                        bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
        
        # 21. è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        
        # 22. ä¿å­˜å›¾ç‰‡
        g.savefig(filename, dpi=600, bbox_inches='tight')
        print(f"ğŸ“ˆ XGBoosté¢„æµ‹ç»“æœå¯è§†åŒ–å›¾å·²ä¿å­˜ä¸º: {filename}")
        plt.show()
        
        # 23. æ‰“å°è¯¦ç»†ç»“æœæ‘˜è¦
        print(f"\n" + "="*60)
        print(f"ğŸ¯ XGBoost Anti-Overfitting æ¨¡å‹æ€§èƒ½æ‘˜è¦")
        print(f"="*60)
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(y_train)}")
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}")
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
        print(f"   æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
        print(f"   è¿‡æ‹ŸåˆæŒ‡æ ‡ (Train RÂ² - Test RÂ²): {train_r2 - test_r2:.4f}")
        print(f"   è®­ç»ƒé›† RMSE (log): {train_rmse_log:.4f}")
        print(f"   æµ‹è¯•é›† RMSE (log): {test_rmse_log:.4f}")
        print(f"   è®­ç»ƒé›† RMSE (åŸå§‹): {train_rmse_original:.4f}")
        print(f"   æµ‹è¯•é›† RMSE (åŸå§‹): {test_rmse_original:.4f}")
        
        # 24. è¿‡æ‹Ÿåˆè¯„ä¼°
        r2_gap = train_r2 - test_r2
        print(f"\nğŸ›¡ï¸ è¿‡æ‹Ÿåˆè¯„ä¼°:")
        if r2_gap < 0.1:
            print(f"âœ… è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½ (RÂ² gap = {r2_gap:.3f} < 0.1)")
        elif r2_gap < 0.2:
            print(f"âš ï¸  è½»åº¦è¿‡æ‹Ÿåˆ (RÂ² gap = {r2_gap:.3f} åœ¨0.1-0.2ä¹‹é—´)")
        else:
            print(f"âŒ ä»å­˜åœ¨è¿‡æ‹Ÿåˆ (RÂ² gap = {r2_gap:.3f} > 0.2)")
        
        return final_model, (train_r2, test_r2, r2_gap)

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¯è§†åŒ–å¯¹è±¡
    visualizer = XGBoostN2OVisualization()
    
    # è¿è¡Œå®Œæ•´æµç¨‹ï¼šæ•°æ®é¢„å¤„ç† -> æ¨¡å‹è®­ç»ƒ -> å¯è§†åŒ–
    model, performance = visualizer.plot_xgboost_prediction_results_with_marginals(
        filepath="GHGdata_LakeATLAS_final250714_cleaned_imputation.csv",
        filename="xgboost_anti_overfitting_results.png"
    )
    
    print(f"\nğŸ‰ XGBoostæ¨¡å‹è®­ç»ƒå’Œå¯è§†åŒ–å®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜ä¸º: xgboost_anti_overfitting_results.png")
    
    return model, performance

if __name__ == "__main__":
    model, performance = main()



#%% ç¥ç»ç½‘ç»œ-MLPRegressor


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

class ImprovedN2ONeuralNetworkPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç† - ä¸è¿›è¡Œscalingï¼Œç•™ç»™Pipelineå¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[mask]
        y = y[mask]
        
        print(f"Final data count after removing NaN: {len(X)}")
        
        return X, y

    def create_cv_pipeline(self, X, y):
        """åˆ›å»ºåŒ…å«æ•°æ®é¢„å¤„ç†çš„äº¤å‰éªŒè¯ç®¡é“"""
        
        class StandardScalerTransformer(BaseEstimator, TransformerMixin):
            def __init__(self):
                self.scaler = StandardScaler()
                
            def fit(self, X, y=None):
                self.scaler.fit(X)
                return self
                
            def transform(self, X):
                return self.scaler.transform(X)
        
        # åˆ›å»ºç®¡é“
        pipeline = Pipeline([
            ('scaler', StandardScalerTransformer()),
            ('mlp', MLPRegressor(
                random_state=self.random_state,
                max_iter=1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°ç¡®ä¿æ”¶æ•›
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=25,  # å¢åŠ è€å¿ƒï¼Œé¿å…è¿‡æ—©åœæ­¢
                tol=1e-5  # é™ä½å®¹å¿åº¦ï¼Œç¡®ä¿æ›´å¥½æ”¶æ•›
            ))
        ])
        
        return pipeline

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """
        ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯çš„æ”¹è¿›æ¨¡å‹è®­ç»ƒ - MLPRegressorç‰ˆæœ¬
        
        Parameters:
        -----------
        X : pandas.DataFrame
            ç‰¹å¾æ•°æ®
        y : pandas.Series  
            ç›®æ ‡å˜é‡
        scoring_metric : str
            è¯„åˆ†æŒ‡æ ‡ï¼Œå¯é€‰ 'neg_mean_squared_error' æˆ– 'r2'
        """
        
        # MLPRegressorå‚æ•°ç½‘æ ¼ - ä¼˜åŒ–æ”¶æ•›ç‰ˆæœ¬
        param_grid = {
            # æ›´ç®€å•çš„éšè—å±‚æ¶æ„ï¼ˆå‡å°‘å¤æ‚åº¦ï¼‰
            'mlp__hidden_layer_sizes': [
                (32,),               # å•å±‚ç®€å•ç½‘ç»œ
                (64,),               # å•å±‚ä¸­ç­‰ç½‘ç»œ
                (50, 25),            # 2å±‚è¾ƒå°ç½‘ç»œ
                (64, 32),            # 2å±‚ä¸­ç­‰ç½‘ç»œ
                (80, 40),            # 2å±‚ç½‘ç»œ
            ],
            # æ¿€æ´»å‡½æ•°
            'mlp__activation': ['relu', 'tanh'],
            # è°ƒæ•´å­¦ä¹ ç‡èŒƒå›´
            'mlp__learning_rate_init': [0.001, 0.005, 0.01],
            # æ›´å¼ºçš„æ­£åˆ™åŒ–å‚æ•°
            'mlp__alpha': [0.1, 0.5, 1.0],
            # æ±‚è§£å™¨
            'mlp__solver': ['adam'],
            # æ‰¹æ¬¡å¤§å°
            'mlp__batch_size': [64, 128, 'auto']
        }
        
        # åˆ›å»ºç®¡é“
        pipeline = self.create_cv_pipeline(X, y)
        
        # ä½¿ç”¨é‡å¤5æŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(
            n_splits=5, 
            n_repeats=3, 
            random_state=self.random_state
        )
        
        print(f"\nUsing Repeated 5-Fold Cross-Validation (3 repeats = 15 total folds)")
        print(f"Scoring metric: {scoring_metric}")
        print("Training MLPRegressor model with pipeline to prevent data leakage...")
        
        # ç½‘æ ¼æœç´¢ä¸é‡å¤äº¤å‰éªŒè¯
        grid_search = GridSearchCV(
            estimator=pipeline,
            param_grid=param_grid,
            cv=repeated_cv,
            scoring=scoring_metric,
            n_jobs=-1,
            verbose=1,
            return_train_score=True
        )
        
        print("Training model with repeated cross-validation...")
        grid_search.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = grid_search.best_estimator_
        self.best_params = grid_search.best_params_
        self.cv_results = grid_search.cv_results_
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœ
        best_score = grid_search.best_score_
        if scoring_metric == 'neg_mean_squared_error':
            print(f"Best CV RMSE: {np.sqrt(-best_score):.4f}")
        else:
            print(f"Best CV RÂ²: {best_score:.4f}")
            
        print("Best MLPRegressor parameters:")
        for key, value in self.best_params.items():
            print(f"  {key}: {value}")
        
        # åˆ†æè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°å·®å¼‚ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰
        cv_results_df = pd.DataFrame(self.cv_results)
        best_idx = grid_search.best_index_
        
        train_scores = cv_results_df.loc[best_idx, 'mean_train_score']
        val_scores = cv_results_df.loc[best_idx, 'mean_test_score']
        
        if scoring_metric == 'neg_mean_squared_error':
            train_rmse = np.sqrt(-train_scores)
            val_rmse = np.sqrt(-val_scores)
            overfitting_gap = train_rmse - val_rmse
            print(f"Training RMSE: {train_rmse:.4f}")
            print(f"Validation RMSE: {val_rmse:.4f}")
            print(f"Overfitting Gap (Train RMSE - Val RMSE): {overfitting_gap:.4f}")
        else:
            overfitting_gap = train_scores - val_scores
            print(f"Training RÂ²: {train_scores:.4f}")
            print(f"Validation RÂ²: {val_scores:.4f}")
            print(f"Overfitting Gap (Train RÂ² - Val RÂ²): {overfitting_gap:.4f}")
        
        return self.best_model

    def optimized_comprehensive_evaluation(self, X, y):
        """ä¼˜åŒ–çš„é‡å¤äº¤å‰éªŒè¯è¯„ä¼° - MLPRegressorç‰ˆæœ¬"""
        print("\nPerforming optimized evaluation with Repeated CV for MLPRegressor...")
        
        # ä½¿ç”¨é‡å¤KæŠ˜äº¤å‰éªŒè¯
        repeated_cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=self.random_state)
        
        # æ‰‹åŠ¨è¿›è¡Œäº¤å‰éªŒè¯ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ
        r2_scores = []
        rmse_log_scores = []
        rmse_original_scores = []
        loss_scores = []
        
        for train_idx, val_idx in repeated_cv.split(X):
            # åˆ†ç¦»è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            X_train_cv = X.iloc[train_idx]
            X_val_cv = X.iloc[val_idx]
            y_train_cv = y.iloc[train_idx]
            y_val_cv = y.iloc[val_idx]
            
            # åœ¨è®­ç»ƒé›†ä¸Šfit scaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_cv)
            X_val_scaled = scaler.transform(X_val_cv)
            
            # è®­ç»ƒæ¨¡å‹
            mlp_model = MLPRegressor(**{k.replace('mlp__', ''): v for k, v in self.best_params.items()},
                                   random_state=self.random_state,
                                   max_iter=1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                                   early_stopping=True,
                                   validation_fraction=0.15,
                                   n_iter_no_change=25,  # å¢åŠ è€å¿ƒ
                                   tol=1e-5)  # é™ä½å®¹å¿åº¦
            
            mlp_model.fit(X_train_scaled, y_train_cv)
            
            # é¢„æµ‹
            y_pred_cv = mlp_model.predict(X_val_scaled)
            
            # è®¡ç®—æŒ‡æ ‡
            r2 = r2_score(y_val_cv, y_pred_cv)
            rmse_log = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))
            
            # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
            y_val_original = 10 ** y_val_cv - 1e-10
            y_pred_original = 10 ** y_pred_cv - 1e-10
            rmse_original = np.sqrt(mean_squared_error(y_val_original, y_pred_original))
            
            # è·å–è®­ç»ƒæŸå¤±
            loss = mlp_model.loss_
            
            r2_scores.append(r2)
            rmse_log_scores.append(rmse_log)
            rmse_original_scores.append(rmse_original)
            loss_scores.append(loss)
        
        r2_scores = np.array(r2_scores)
        rmse_log_scores = np.array(rmse_log_scores)
        rmse_original_scores = np.array(rmse_original_scores)
        loss_scores = np.array(loss_scores)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        results = {
            'cv_r2_mean': r2_scores.mean(),
            'cv_r2_std': r2_scores.std(),
            'cv_r2_scores': r2_scores,
            'cv_rmse_log_mean': rmse_log_scores.mean(),
            'cv_rmse_log_std': rmse_log_scores.std(), 
            'cv_rmse_log_scores': rmse_log_scores,
            'cv_rmse_original_mean': rmse_original_scores.mean(),
            'cv_rmse_original_std': rmse_original_scores.std(),
            'cv_rmse_original_scores': rmse_original_scores,
            'loss_mean': loss_scores.mean(),
            'loss_std': loss_scores.std(),
            'loss_scores': loss_scores,
            'n_cv_folds': len(r2_scores)
        }
        
        return results
    
    def print_literature_ready_results(self, results):
        """æ‰“å°é€‚åˆæ–‡çŒ®æŠ¥å‘Šçš„ç»“æœ - MLPRegressorç‰ˆæœ¬"""
        print("\n" + "="*70)
        print("ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION) - NEURAL NETWORK (MLPRegressor)")
        print("="*70)
        
        print(f"ğŸ”¬ Model: MLPRegressor with Repeated 5-Fold Cross-Validation (No Data Leakage)")
        print(f"ğŸ“ˆ Sample size: {len(results['cv_r2_scores'])} folds")
        print(f"ğŸ¯ Features used: {len(self.analysis_vars)}")
        
        print(f"\nğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:")
        print(f"   â€¢ RÂ² = {results['cv_r2_mean']:.3f} Â± {results['cv_r2_std']:.3f}")
        print(f"   â€¢ RMSE = {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f} mmol mâ»Â³")
        print(f"   â€¢ Log-scale RMSE = {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
        print(f"   â€¢ Training Loss = {results['loss_mean']:.4f} Â± {results['loss_std']:.4f}")
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR METHODS SECTION:")
        print(f'   "A Multi-layer Perceptron (MLPRegressor) was trained using repeated 5-fold cross-validation')
        print(f'    (3 repeats, {results["n_cv_folds"]} total folds) with proper data preprocessing')
        print(f'    to prevent data leakage. The following parameters were optimized:')
        for param, value in self.best_params.items():
            clean_param = param.replace('mlp__', '')
            print(f'    {clean_param}={value},', end=' ')
        print('"')
        
        print(f"\nğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:")
        print(f'   "The MLPRegressor model achieved an RÂ² of {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f}')
        print(f'    and RMSE of {results["cv_rmse_original_mean"]:.4f} Â± {results["cv_rmse_original_std"]:.4f} mmol mâ»Â³')
        print(f'    based on repeated cross-validation with proper data preprocessing.')
        print(f'    The training loss was {results["loss_mean"]:.4f} Â± {results["loss_std"]:.4f}."')
        
        print(f"\nâœ… NEURAL NETWORK FEATURES:")
        print(f"   â€¢ Multiple hidden layer architectures tested")
        print(f"   â€¢ Early stopping to prevent overfitting")
        print(f"   â€¢ Standard scaling for feature normalization")
        print(f"   â€¢ Adam optimizer with learning rate tuning")
        print(f"   â€¢ L2 regularization (alpha parameter)")
        
        return results

    def plot_cv_stability_analysis(self, results, filename="mlp_cv_stability_analysis.png"):
        """ç»˜åˆ¶äº¤å‰éªŒè¯ç¨³å®šæ€§åˆ†æ - MLPRegressorç‰ˆæœ¬"""
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. RÂ²åˆ†æ•°åˆ†å¸ƒ
        axes[0, 0].hist(results['cv_r2_scores'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[0, 0].set_xlabel('RÂ² Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title(f'MLPRegressor: Distribution of RÂ² Scores\n(Mean Â± Std: {results["cv_r2_mean"]:.3f} Â± {results["cv_r2_std"]:.3f})')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RMSEåˆ†æ•°åˆ†å¸ƒ (log scale)
        axes[0, 1].hist(results['cv_rmse_log_scores'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[0, 1].axvline(results['cv_rmse_log_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_log_mean"]:.3f}')
        axes[0, 1].set_xlabel('RMSE (Log Scale)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'MLPRegressor: Distribution of RMSE (Log Scale)\n(Mean Â± Std: {results["cv_rmse_log_mean"]:.3f} Â± {results["cv_rmse_log_std"]:.3f})')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è®­ç»ƒæŸå¤±åˆ†å¸ƒ
        axes[0, 2].hist(results['loss_scores'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[0, 2].axvline(results['loss_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["loss_mean"]:.3f}')
        axes[0, 2].set_xlabel('Training Loss')
        axes[0, 2].set_ylabel('Frequency')
        axes[0, 2].set_title(f'MLPRegressor: Distribution of Training Loss\n(Mean Â± Std: {results["loss_mean"]:.3f} Â± {results["loss_std"]:.3f})')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. åŸå§‹å°ºåº¦RMSEåˆ†å¸ƒ
        axes[1, 0].hist(results['cv_rmse_original_scores'], bins=10, alpha=0.7, color='orange', edgecolor='black')
        axes[1, 0].axvline(results['cv_rmse_original_mean'], color='red', linestyle='--', linewidth=2,
                          label=f'Mean: {results["cv_rmse_original_mean"]:.4f}')
        axes[1, 0].set_xlabel('RMSE (Original Scale)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title(f'MLPRegressor: Distribution of RMSE (Original Scale)\n(Mean Â± Std: {results["cv_rmse_original_mean"]:.4f} Â± {results["cv_rmse_original_std"]:.4f})')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. RÂ²åˆ†æ•°è¶‹åŠ¿
        axes[1, 1].plot(results['cv_r2_scores'], 'o-', alpha=0.7, color='darkblue')
        axes[1, 1].axhline(results['cv_r2_mean'], color='red', linestyle='--', linewidth=2, 
                          label=f'Mean: {results["cv_r2_mean"]:.3f}')
        axes[1, 1].fill_between(range(len(results['cv_r2_scores'])), 
                               results['cv_r2_mean'] - results['cv_r2_std'],
                               results['cv_r2_mean'] + results['cv_r2_std'],
                               alpha=0.2, color='red', label=f'Â±1 Std')
        axes[1, 1].set_xlabel('CV Fold Number')
        axes[1, 1].set_ylabel('RÂ² Score')
        axes[1, 1].set_title('MLPRegressor: RÂ² Score Across CV Folds')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. RÂ² vs æŸå¤±å…³ç³»
        axes[1, 2].scatter(results['cv_r2_scores'], results['loss_scores'], alpha=0.7, c='purple', s=50)
        axes[1, 2].set_xlabel('RÂ² Score')
        axes[1, 2].set_ylabel('Training Loss')
        axes[1, 2].set_title('MLPRegressor: RÂ² vs Training Loss')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.suptitle(f'MLPRegressor Cross-Validation Stability Analysis\n({results["n_cv_folds"]} total folds from Repeated 5-Fold CV)')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"MLPRegressor CV stability analysis saved as: {filename}")
        plt.show()
        plt.close()

    def plot_improved_results_with_proper_cv(self, X, y, filename="mlp_prediction_results.png"):
        """ä½¿ç”¨æ­£ç¡®çš„äº¤å‰éªŒè¯æ–¹æ³•çš„å¯è§†åŒ– - MLPRegressorç‰ˆæœ¬"""
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ï¼šåœ¨åˆ†ç¦»æ•°æ®åå†è¿›è¡Œé¢„å¤„ç†
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šfit scalerï¼Œç„¶åtransforméªŒè¯é›†
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰ - ä¼˜åŒ–æ”¶æ•›ç‰ˆæœ¬
        final_model = MLPRegressor(**{k.replace('mlp__', ''): v for k, v in self.best_params.items()},
                                 random_state=self.random_state,
                                 max_iter=1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                                 early_stopping=True,
                                 validation_fraction=0.15,
                                 n_iter_no_change=25,  # å¢åŠ è€å¿ƒ
                                 tol=1e-5)  # é™ä½å®¹å¿åº¦
        
        final_model.fit(X_train_scaled, y_train)
        
        y_train_pred = final_model.predict(X_train_scaled)
        y_val_pred = final_model.predict(X_val_scaled)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse_log = np.sqrt(mean_squared_error(y_val, y_val_pred))
        training_loss = final_model.loss_
        
        # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
        y_val_original = 10 ** y_val - 1e-10
        y_val_pred_original = 10 ** y_val_pred - 1e-10
        y_train_original = 10 ** y_train - 1e-10
        y_train_pred_original = 10 ** y_train_pred - 1e-10
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. éªŒè¯é›†é¢„æµ‹ç»“æœ
        axes[0, 0].scatter(y_val_pred_original, y_val_original, alpha=0.6, c='darkblue', s=30)
        min_val = min(y_val_original.min(), y_val_pred_original.min())
        max_val = max(y_val_original.max(), y_val_pred_original.max())
        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 0].set_xscale('log')
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 0].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 0].set_title(f'MLPRegressor Validation Performance\nRÂ² = {val_r2:.3f}, Loss = {training_loss:.3f}')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è®­ç»ƒé›†é¢„æµ‹ç»“æœ
        axes[0, 1].scatter(y_train_pred_original, y_train_original, alpha=0.6, c='green', s=30)
        min_val = min(y_train_original.min(), y_train_pred_original.min())
        max_val = max(y_train_original.max(), y_train_pred_original.max())
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        axes[0, 1].set_xscale('log')
        axes[0, 1].set_yscale('log')
        axes[0, 1].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[0, 1].set_ylabel('N2O Observations (mmol mâ»Â³)')
        axes[0, 1].set_title(f'MLPRegressor Training Performance\nRÂ² = {train_r2:.3f}')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æŸå¤±æ›²çº¿ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if hasattr(final_model, 'loss_curve_'):
            axes[0, 2].plot(final_model.loss_curve_, alpha=0.7, color='blue')
            axes[0, 2].set_xlabel('Iteration')
            axes[0, 2].set_ylabel('Loss')
            axes[0, 2].set_title('MLPRegressor Training Loss Curve')
            axes[0, 2].grid(True, alpha=0.3)
            axes[0, 2].set_yscale('log')
        else:
            axes[0, 2].text(0.5, 0.5, 'Loss curve not available\n(early stopping used)', 
                           ha='center', va='center', transform=axes[0, 2].transAxes)
            axes[0, 2].set_title('MLPRegressor Training Information')
        
        # 4. æ®‹å·®åˆ†æ
        val_residuals = y_val - y_val_pred
        axes[1, 0].scatter(y_val_pred_original, val_residuals, alpha=0.6, c='red', s=30)
        axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=2)
        axes[1, 0].set_xscale('log')
        axes[1, 0].set_xlabel('N2O Predictions (mmol mâ»Â³)')
        axes[1, 0].set_ylabel('Residuals (log scale)')
        axes[1, 0].set_title('MLPRegressor Validation Residuals vs Predictions')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. æ®‹å·®ç›´æ–¹å›¾
        axes[1, 1].hist(val_residuals, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')
        axes[1, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)
        axes[1, 1].set_xlabel('Residuals (log scale)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('MLPRegressor Distribution of Validation Residuals')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. ç½‘ç»œæ¶æ„ä¿¡æ¯
        architecture_info = f"Architecture: {self.best_params.get('mlp__hidden_layer_sizes', 'Unknown')}"
        activation_info = f"Activation: {self.best_params.get('mlp__activation', 'Unknown')}"
        solver_info = f"Solver: {self.best_params.get('mlp__solver', 'Unknown')}"
        alpha_info = f"Alpha: {self.best_params.get('mlp__alpha', 'Unknown')}"
        lr_info = f"Learning Rate: {self.best_params.get('mlp__learning_rate_init', 'Unknown')}"
        
        axes[1, 2].text(0.1, 0.9, architecture_info, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.8, activation_info, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.7, solver_info, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.6, alpha_info, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.5, lr_info, transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.3, f'Training Loss: {training_loss:.4f}', transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.2, f'Validation RÂ²: {val_r2:.4f}', transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].text(0.1, 0.1, f'Training RÂ²: {train_r2:.4f}', transform=axes[1, 2].transAxes, fontsize=10)
        axes[1, 2].set_title('MLPRegressor Model Information')
        axes[1, 2].axis('off')
        
        plt.suptitle('MLPRegressor Model Performance Analysis')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"MLPRegressor results plot saved as: {filename}")
        plt.show()
        plt.close()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ä»¥ä¾›ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.final_model = final_model
        self.scaler = scaler
        
    def plot_feature_importance(self, X, y, filename="mlp_feature_importance.png"):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ - MLPRegressorç‰ˆæœ¬ä½¿ç”¨æ’åˆ—é‡è¦æ€§"""
        if not hasattr(self, 'final_model'):
            print("Warning: No final model available. Please run plot_improved_results_with_proper_cv first.")
            return None
            
        print("Calculating feature importance using permutation method for MLPRegressor...")
        
        # ä½¿ç”¨æ’åˆ—é‡è¦æ€§
        X_scaled = self.scaler.transform(X)
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        perm_importance = permutation_importance(
            self.final_model, X_scaled, y, 
            n_repeats=10, 
            random_state=self.random_state,
            scoring='r2'
        )
        
        importances_df = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance_mean': perm_importance.importances_mean,
            'importance_std': perm_importance.importances_std
        }).sort_values('importance_mean', ascending=False)
        
        plt.figure(figsize=(12, 8))
        colors = ['darkred' if x < 0 else 'darkgreen' for x in importances_df['importance_mean']]
        bars = plt.barh(range(len(importances_df)), importances_df['importance_mean'], 
                       xerr=importances_df['importance_std'], color=colors, alpha=0.7)
        
        plt.yticks(range(len(importances_df)), importances_df['feature'])
        plt.xlabel('Feature Importance (Permutation Score)')
        plt.title('MLPRegressor Feature Importance for N2O Prediction\n(Permutation Importance Method)')
        plt.grid(True, alpha=0.3, axis='x')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, importance, std) in enumerate(zip(bars, importances_df['importance_mean'], importances_df['importance_std'])):
            plt.text(importance + std + 0.001 if importance >= 0 else importance - std - 0.001, 
                    bar.get_y() + bar.get_height()/2, 
                    f'{importance:.3f}Â±{std:.3f}', 
                    ha='left' if importance >= 0 else 'right', 
                    va='center', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"MLPRegressor feature importance plot saved as: {filename}")
        plt.show()
        plt.close()
        
        return importances_df

    def diagnose_overfitting(self, results):
        """è¯Šæ–­è¿‡æ‹Ÿåˆé—®é¢˜"""
        print("\n" + "="*60)
        print("ğŸ” OVERFITTING DIAGNOSIS")
        print("="*60)
        
        # åˆ†æè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°å·®å¼‚
        cv_results_df = pd.DataFrame(self.cv_results)
        best_idx = self.cv_results['best_index_'] if 'best_index_' in self.cv_results else np.argmax(cv_results_df['mean_test_score'])
        
        train_score = cv_results_df.loc[best_idx, 'mean_train_score']
        val_score = cv_results_df.loc[best_idx, 'mean_test_score']
        
        # è½¬æ¢ä¸ºRÂ²å’ŒRMSE
        train_r2 = -train_score if train_score < 0 else train_score
        val_r2 = -val_score if val_score < 0 else val_score
        
        gap = abs(train_r2 - val_r2)
        
        print(f"ğŸ“Š Performance Gap Analysis:")
        print(f"   Training Score: {train_r2:.4f}")
        print(f"   Validation Score: {val_r2:.4f}")
        print(f"   Gap: {gap:.4f}")
        
        if gap > 0.1:
            print("ğŸš¨ OVERFITTING DETECTED!")
            print("   Recommendations:")
            print("   â€¢ Increase alpha (regularization)")
            print("   â€¢ Reduce network complexity")
            print("   â€¢ Increase early stopping patience")
            print("   â€¢ Use smaller learning rate")
        elif gap > 0.05:
            print("âš ï¸  MILD OVERFITTING")
            print("   Consider stronger regularization")
        else:
            print("âœ… NO SIGNIFICANT OVERFITTING")
        
        # åˆ†æäº¤å‰éªŒè¯ç¨³å®šæ€§
        r2_std = results['cv_r2_std']
        if r2_std > 0.05:
            print(f"\nâš ï¸  HIGH VARIANCE (std={r2_std:.4f})")
            print("   Model predictions are unstable across folds")
        else:
            print(f"\nâœ… STABLE PREDICTIONS (std={r2_std:.4f})")
        
        return gap

    def check_convergence_status(self):
        """æ£€æŸ¥æ¨¡å‹æ”¶æ•›çŠ¶æ€"""
        if not hasattr(self, 'final_model'):
            print("Warning: No final model available.")
            return
            
        print("\n" + "="*60)
        print("ğŸ”„ CONVERGENCE STATUS CHECK")
        print("="*60)
        
        model = self.final_model
        
        if hasattr(model, 'n_iter_'):
            print(f"ğŸ“Š Training iterations completed: {model.n_iter_}")
            print(f"ğŸ¯ Maximum iterations allowed: {model.max_iter}")
            
            if model.n_iter_ >= model.max_iter:
                print("âš ï¸  WARNING: Model reached max iterations without convergence!")
                print("   Recommendations:")
                print("   â€¢ Increase max_iter (try 2000)")
                print("   â€¢ Decrease learning_rate_init")
                print("   â€¢ Increase tol (tolerance)")
                print("   â€¢ Simplify network architecture")
            else:
                print("âœ… Model converged successfully")
                print(f"   Stopped early after {model.n_iter_} iterations")
        
        if hasattr(model, 'loss_'):
            print(f"ğŸ“‰ Final training loss: {model.loss_:.6f}")
            
        if hasattr(model, 'loss_curve_'):
            final_losses = model.loss_curve_[-10:]  # æœ€å10æ¬¡è¿­ä»£çš„æŸå¤±
            loss_change = abs(final_losses[-1] - final_losses[0]) if len(final_losses) > 1 else 0
            print(f"ğŸ“ˆ Loss change in last 10 iterations: {loss_change:.6f}")
            
            if loss_change > 1e-4:
                print("âš ï¸  Loss still changing significantly")
                print("   Model may benefit from more iterations")
            else:
                print("âœ… Loss stabilized")
        
        return model.n_iter_ if hasattr(model, 'n_iter_') else None


def main():
    """ä¸»å‡½æ•° - MLPRegressorç‰ˆæœ¬"""
    predictor = ImprovedN2ONeuralNetworkPredictor()
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    print("Loading and preprocessing data for MLPRegressor...")
    X, y = predictor.load_and_preprocess_data("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
    
    print(f"Using all {X.shape[1]} features for MLPRegressor")
    
    # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
    scoring_metric = 'neg_mean_squared_error'
    
    # ä½¿ç”¨é‡å¤äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
    best_model = predictor.train_improved_model_with_repeated_cv(X, y, scoring_metric)
    
    # æ¨¡å‹å…¨é¢è¯„ä¼°
    results = predictor.optimized_comprehensive_evaluation(X, y)
    predictor.print_literature_ready_results(results)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("MLPREGRESSOR MODEL PERFORMANCE")
    print("="*60)
    print(f"Using {X.shape[1]} features")
    print(f"Scoring metric for GridSearch: {scoring_metric}")
    print(f"Total CV folds for evaluation: {results['n_cv_folds']}")
    print(f"\nRepeated CV Results (5-fold Ã— 3 repeats = 15 folds):")
    print(f"RÂ² (mean Â± std): {results['cv_r2_mean']:.4f} Â± {results['cv_r2_std']:.4f}")
    print(f"Log Scale RMSE (mean Â± std): {results['cv_rmse_log_mean']:.4f} Â± {results['cv_rmse_log_std']:.4f}")
    print(f"Original Scale RMSE (mean Â± std): {results['cv_rmse_original_mean']:.4f} Â± {results['cv_rmse_original_std']:.4f}")
    print(f"Training Loss (mean Â± std): {results['loss_mean']:.4f} Â± {results['loss_std']:.4f}")
    
    print(f"\nBest MLPRegressor Parameters:")
    for param, value in predictor.best_params.items():
        print(f"  {param}: {value}")
    
    # ç»˜åˆ¶ç¨³å®šæ€§åˆ†æ
    predictor.plot_cv_stability_analysis(results)
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
    predictor.plot_improved_results_with_proper_cv(X, y)
    
    # ç‰¹å¾é‡è¦æ€§
    importance_df = predictor.plot_feature_importance(X, y)
    if importance_df is not None:
        print(f"\nTop 5 Most Important Features in MLPRegressor:")
        print(importance_df.head())
        print(f"\nTop 5 Least Important Features in MLPRegressor:")
        print(importance_df.tail())
    
    return predictor, results

if __name__ == "__main__":
    print("Starting MLPRegressor N2O Prediction Analysis...")
    print("="*60)
    predictor, results = main()
    print("\nMLPRegressor analysis completed successfully!")
    print("\nğŸ§  MLPREGRESSOR FEATURES (OPTIMIZED CONVERGENCE VERSION):")
    print("âœ… Balanced network architectures (1-2 layers)")
    print("âœ… Strong regularization (alpha: 0.1-1.0)")
    print("âœ… Optimized learning rates (0.001-0.01)")
    print("âœ… Extended max iterations (1000)")
    print("âœ… Improved early stopping (25 iterations patience)")
    print("âœ… Convergence monitoring included")
    print("âœ… Overfitting diagnosis included")
    print("âœ… Permutation-based feature importance analysis")
    print("âœ… Comprehensive cross-validation evaluation")
    print("âœ… No data leakage in preprocessing")

#%% ç¥ç»ç½‘ç»œè¿è¡Œç»“æœ 0802

Loading and preprocessing data for MLPRegressor...
Original data count: 3078
Data count after filtering: 2995
Final data count after removing NaN: 2862
Using all 24 features for MLPRegressor

Best MLPRegressor parameters:
  mlp__activation: relu
  mlp__alpha: 0.5
  mlp__batch_size: auto
  mlp__hidden_layer_sizes: (80, 40)
  mlp__learning_rate_init: 0.01
  mlp__solver: adam
Training RMSE: 0.4391
Validation RMSE: 0.5646
Overfitting Gap (Train RMSE - Val RMSE): -0.1255


ğŸ“Š LITERATURE-READY RESULTS (FOR PUBLICATION) - NEURAL NETWORK (MLPRegressor)
======================================================================
ğŸ”¬ Model: MLPRegressor with Repeated 5-Fold Cross-Validation (No Data Leakage)
ğŸ“ˆ Sample size: 15 folds
ğŸ¯ Features used: 24

ğŸ“‹ PRIMARY METRICS TO REPORT IN LITERATURE:
   â€¢ RÂ² = 0.462 Â± 0.050
   â€¢ RMSE = 0.5303 Â± 0.0496 mmol mâ»Â³
   â€¢ Log-scale RMSE = 0.5643 Â± 0.0183
   â€¢ Training Loss = 0.1297 Â± 0.0072

ğŸ“ SUGGESTED TEXT FOR METHODS SECTION:
   "A Multi-layer Perceptron (MLPRegressor) was trained using repeated 5-fold cross-validation
    (3 repeats, 15 total folds) with proper data preprocessing
    to prevent data leakage. The following parameters were optimized:
    activation=relu,     alpha=0.5,     batch_size=auto,     hidden_layer_sizes=(80, 40),     learning_rate_init=0.01,     solver=adam, "

ğŸ“ SUGGESTED TEXT FOR RESULTS SECTION:
   "The MLPRegressor model achieved an RÂ² of 0.462 Â± 0.050
    and RMSE of 0.5303 Â± 0.0496 mmol mâ»Â³
    based on repeated cross-validation with proper data preprocessing.
    The training loss was 0.1297 Â± 0.0072."

âœ… NEURAL NETWORK FEATURES:
   â€¢ Multiple hidden layer architectures tested
   â€¢ Early stopping to prevent overfitting
   â€¢ Standard scaling for feature normalization
   â€¢ Adam optimizer with learning rate tuning
   â€¢ L2 regularization (alpha parameter)



#%% ç¥ç»ç½‘ç»œå‡ºå›¾ 0814


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data(filepath):
    """
    åŠ è½½å’Œé¢„å¤„ç†æ•°æ® - ä¸åŸå§‹ä»£ç ä¿æŒä¸€è‡´
    """
    variables = [
        'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
        'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
        'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
        'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
        'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
        'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
        'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
    ]
    
    variables_removed = [
        'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
        'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
    ]
    
    log_transform_vars = [
        'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
        'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
    ]
    
    # è¯»å–æ•°æ®
    data = pd.read_csv(filepath, dtype={'N2O': float})
    print(f"Original data count: {len(data)}")
    
    # åŸºç¡€è¿‡æ»¤
    data_filtered = data[
        (data['N2O'] > data['N2O'].quantile(0.01)) & 
        (data['N2O'] < data['N2O'].quantile(0.99))
    ].copy()
    print(f"Data count after filtering: {len(data_filtered)}")
    
    # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
    data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
    
    # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
    for var in log_transform_vars:
        if var in data_filtered.columns:
            data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
    
    # å‡†å¤‡åˆ†æå˜é‡
    regular_vars = [var for var in variables 
                   if var not in variables_removed 
                   and var not in log_transform_vars]
    log_vars = [f'Log1p_{var}' for var in log_transform_vars]
    analysis_vars = regular_vars + log_vars
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
    X = data_filtered[analysis_vars]
    y = data_filtered['Log_N2O']
    
    # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
    X = X.replace([np.inf, -np.inf], np.nan)
    
    # åˆ é™¤å«æœ‰NaNçš„è¡Œ
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[mask]
    y = y[mask]
    
    print(f"Final data count after removing NaN: {len(X)}")
    print(f"Using {X.shape[1]} features for MLPRegressor")
    
    return X, y

def train_and_visualize_mlp_model(filepath="GHGdata_LakeATLAS_final250714_cleaned_imputation.csv", 
                                  random_state=1113, 
                                  filename="mlp_prediction_results_with_marginals.png"):
    """
    å®Œæ•´çš„MLPRegressoræ¨¡å‹è®­ç»ƒå’Œå¯è§†åŒ–å‡½æ•°
    
    Parameters:
    -----------
    filepath : str
        æ•°æ®æ–‡ä»¶è·¯å¾„
    random_state : int
        éšæœºç§å­
    filename : str
        ä¿å­˜çš„æ–‡ä»¶å
    """
    
    # ä½¿ç”¨æ‚¨æä¾›çš„æœ€ä½³å‚æ•°
    best_params = {
        'activation': 'relu',
        'alpha': 0.5,
        'batch_size': 'auto',
        'hidden_layer_sizes': (80, 40),
        'learning_rate_init': 0.01,
        'solver': 'adam'
    }
    
    # è‡ªå®šä¹‰è°ƒè‰²æ¿
    palette = {'Train': '#b4d4e1', 'Test': '#f4ba8a'}
    
    print("Loading and preprocessing data for MLPRegressor...")
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    X, y = load_and_preprocess_data(filepath)
    
    # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ï¼šåœ¨åˆ†ç¦»æ•°æ®åå†è¿›è¡Œé¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )
    
    print(f"Training set size: {len(X_train)}")
    print(f"Test set size: {len(X_test)}")
    
    # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šfit scalerï¼Œç„¶åtransformæµ‹è¯•é›†
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰- ä¼˜åŒ–æ”¶æ•›ç‰ˆæœ¬
    model_params = best_params.copy()
    model_params.update({
        'random_state': random_state,
        'max_iter': 1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
        'early_stopping': True,
        'validation_fraction': 0.15,
        'n_iter_no_change': 25,  # å¢åŠ è€å¿ƒ
        'tol': 1e-5  # é™ä½å®¹å¿åº¦
    })
    
    final_model = MLPRegressor(**model_params)
    print("Training MLPRegressor model with best parameters...")
    print("Best parameters used:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")
    
    final_model.fit(X_train_scaled, y_train)
    
    # é¢„æµ‹
    y_train_pred = final_model.predict(X_train_scaled)
    y_test_pred = final_model.predict(X_test_scaled)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    train_rmse_log = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse_log = np.sqrt(mean_squared_error(y_test, y_test_pred))
    training_loss = final_model.loss_
    
    # è½¬æ¢åˆ°åŸå§‹å°ºåº¦
    y_train_original = 10 ** y_train - 1e-10
    y_train_pred_original = 10 ** y_train_pred - 1e-10
    y_test_original = 10 ** y_test - 1e-10
    y_test_pred_original = 10 ** y_test_pred - 1e-10
    
    # è®¡ç®—åŸå§‹å°ºåº¦çš„RMSE
    train_rmse_original = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))
    test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))
    
    # åˆ›å»ºæ•°æ®æ¡†ç”¨äºç»˜å›¾
    train_data = pd.DataFrame({
        'Observed': y_train_original,
        'Predicted': y_train_pred_original,
        'Dataset': 'Train'
    })
    
    test_data = pd.DataFrame({
        'Observed': y_test_original,
        'Predicted': y_test_pred_original,
        'Dataset': 'Test'
    })
    
    # åˆå¹¶æ•°æ®
    plot_data = pd.concat([train_data, test_data], ignore_index=True)
    
    # è®¾ç½®matplotlibå’Œseabornæ ·å¼
    plt.style.use('default')
    sns.set_palette("husl")
    
    # åˆ›å»º JointGrid å¯¹è±¡
    g = sns.JointGrid(data=plot_data, x="Observed", y="Predicted", hue="Dataset", 
                      palette=palette, height=8, ratio=5)
    
    # ç»˜åˆ¶ä¸»æ•£ç‚¹å›¾
    g.plot_joint(sns.scatterplot, alpha=0.6, s=30)
    
    # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
    min_val = min(plot_data['Observed'].min(), plot_data['Predicted'].min())
    max_val = max(plot_data['Observed'].max(), plot_data['Predicted'].max())
    g.ax_joint.plot([min_val, max_val], [min_val, max_val], color='gray', linestyle='--', linewidth=2, 
                    label='Perfect Prediction', alpha=0.8)
    
    # è®¾ç½®å¯¹æ•°åˆ»åº¦
    g.ax_joint.set_xscale('log')
    g.ax_joint.set_yscale('log')
    
    # æ·»åŠ è¾¹ç¼˜çš„æŸ±çŠ¶å›¾
    g.plot_marginals(sns.histplot, kde=False, element='bars', multiple='stack', alpha=0.5)
    # å…³é—­ y è½´çš„è¾¹ç¼˜æŸ±çŠ¶å›¾
    g.ax_marg_y.set_visible(False)
    
    # è®¾ç½®åæ ‡è½´æ ‡ç­¾
    g.set_axis_labels('Observed Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', 'Predicted Nâ‚‚O (mg N mâ»Â¹ dâ»Â¹)', fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼
    g.ax_joint.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
    g.ax_joint.legend(fontsize=10)
    
    # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬æ¡†
    g.ax_joint.text(0.95, 0.05, f'Test $R^2$ = {test_r2:.3f}', 
                    transform=g.ax_joint.transAxes, fontsize=12, 
                    verticalalignment='bottom', horizontalalignment='right',
                    bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
    
    # åœ¨å·¦ä¸Šè§’æ·»åŠ æ¨¡å‹åç§°æ–‡æœ¬
    g.ax_joint.text(0.5, 0.99, 'Neural Network (MLP)', 
                    transform=g.ax_joint.transAxes, fontsize=12, 
                    verticalalignment='top', horizontalalignment='center',
                    bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
    
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    
    # é‡æ–°ä¿å­˜JointGridå›¾
    g.savefig(filename, dpi=600, bbox_inches='tight')
    print(f"MLPRegressoré¢„æµ‹ç»“æœå¯è§†åŒ–å›¾å·²ä¿å­˜ä¸º: {filename}")
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœæ‘˜è¦
    print(f"\n" + "="*60)
    print(f"MLPRegressor æ¨¡å‹æ€§èƒ½æ‘˜è¦")
    print(f"="*60)
    print(f"æ¨¡å‹å‚æ•°:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")
    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(y_train)}")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}")
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"  æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"  è®­ç»ƒæŸå¤±: {training_loss:.4f}")
    print(f"  è®­ç»ƒé›† RMSE (log): {train_rmse_log:.4f}")
    print(f"  æµ‹è¯•é›† RMSE (log): {test_rmse_log:.4f}")
    print(f"  è®­ç»ƒé›† RMSE (åŸå§‹): {train_rmse_original:.4f}")
    print(f"  æµ‹è¯•é›† RMSE (åŸå§‹): {test_rmse_original:.4f}")
    
    # æ”¶æ•›æ€§æ£€æŸ¥
    if hasattr(final_model, 'n_iter_'):
        print(f"\næ”¶æ•›æ€§ä¿¡æ¯:")
        print(f"  å®é™…è¿­ä»£æ¬¡æ•°: {final_model.n_iter_}")
        print(f"  æœ€å¤§è¿­ä»£æ¬¡æ•°: {final_model.max_iter}")
        if final_model.n_iter_ >= final_model.max_iter:
            print("  âš ï¸ è­¦å‘Š: æ¨¡å‹è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›")
        else:
            print("  âœ… æ¨¡å‹æˆåŠŸæ”¶æ•›")
    
    return final_model, (train_r2, test_r2, training_loss), X, y

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("Starting MLPRegressor N2O Prediction Analysis and Visualization...")
    print("="*60)
    
    # è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œå¯è§†åŒ–æµç¨‹
    final_model, performance_metrics, X, y = train_and_visualize_mlp_model(
        filepath="GHGdata_LakeATLAS_final250714_cleaned_imputation.csv",
        random_state=1113,
        filename="mlp_prediction_results_with_marginals.png"
    )
    
    train_r2, test_r2, training_loss = performance_metrics
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœæ€»ç»“:")
    print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"è®­ç»ƒæŸå¤±: {training_loss:.4f}")
    print("\nMLPRegressoråˆ†æå’Œå¯è§†åŒ–å®Œæˆï¼")


#%% æ’åˆ—é‡è¦æ€§å†…åµŒåä¾èµ–å›¾å‡ºé”™ ä½†æ’åˆ—é‡è¦æ€§å‡ºå›¾æ­£å¸¸  0728 

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance, partial_dependence
import warnings
import pickle
from datetime import datetime
warnings.filterwarnings('ignore')

class EnhancedN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        self.X = None  # ä¿å­˜è®­ç»ƒæ•°æ®ç”¨äºé‡è¦æ€§åˆ†æ
        self.y = None  # ä¿å­˜ç›®æ ‡å˜é‡ç”¨äºé‡è¦æ€§åˆ†æ
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # ä½¿ç”¨RobustScalerè¿›è¡Œç¼©æ”¾
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        return X_scaled, y

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """ä½¿ç”¨é¢„è®¾æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹"""
        
        # ä¿å­˜æ•°æ®ç”¨äºåç»­åˆ†æ
        self.X = X
        self.y = y
        
        # ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°
        best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        print(f"ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹:")
        print(f"å‚æ•°: {best_params}")
        
        # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
        rf_reg = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **best_params
        )
        
        print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        rf_reg.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = rf_reg
        self.best_params = best_params
        
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"OOB Score: {rf_reg.oob_score_:.4f}")
        
        return self.best_model

    def evaluate_model(self, X_train, X_val, y_train, y_val):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…å«è¯¦ç»†çš„æ€§èƒ½åˆ†æ"""
        k_folds = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        cv_scores = cross_val_score(self.best_model, X_train, y_train, cv=k_folds, scoring='r2')
        
        # å¯¹æ•°ç©ºé—´çš„é¢„æµ‹
        y_train_pred = self.best_model.predict(X_train)
        y_val_pred = self.best_model.predict(X_val)
        
        # å¯¹æ•°ç©ºé—´çš„R2
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        
        # åŸå§‹å°ºåº¦çš„RMSEè®¡ç®—
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
        
        # æ·»åŠ OOBåˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        oob_score = getattr(self.best_model, 'oob_score_', None)
        
        return {
            'cv_scores': cv_scores,
            'train_r2': train_r2,
            'val_r2': val_r2,
            'train_rmse': train_rmse,
            'val_rmse': val_rmse,
            'oob_score': oob_score,
            'y_val_true': y_val,
            'y_val_pred': y_val_pred
        }

    def feature_importance_builtin(self, filename="feature_importance_builtin.png"):
        """
        è®¡ç®—å¹¶å±•ç¤ºéšæœºæ£®æ—å†…ç½®ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºåŸºå°¼ä¸çº¯åº¦ï¼‰
        """
        if self.best_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
            
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': self.best_model.feature_importances_
        })
        importances = importances.sort_values('importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        plt.style.use('default')
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_features = importances.head(20)
        plt.barh(np.arange(len(top_features)), 
                top_features['importance'],
                align='center',
                color='lightblue',
                edgecolor='black')
        plt.yticks(np.arange(len(top_features)), 
                  top_features['feature'])
        plt.xlabel('Feature Importance (Built-in)')
        plt.title('Top 20 Most Important Features - Random Forest Built-in Importance')
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å†…ç½®ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        return importances

    def feature_importance_permutation(self, n_repeats=10, filename="feature_importance_permutation.png"):
        """
        è®¡ç®—å¹¶å±•ç¤ºæ’åˆ—é‡è¦æ€§ï¼ˆPermutation Importanceï¼‰
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è®¡ç®—æ’åˆ—é‡è¦æ€§...")
        print(f"é‡å¤æ¬¡æ•°: {n_repeats}")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # æŒ‰é‡è¦æ€§æ’åº
        importances = importances.sort_values('importance', ascending=False)
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        plt.figure(figsize=(12, 8))
        top_features = importances.head(20)
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        bars = plt.barh(range(len(top_features)), 
                       top_features['importance'],
                       color='lightcoral',
                       edgecolor='black',
                       alpha=0.8)
        
        # æ·»åŠ è¯¯å·®æ¡
        plt.errorbar(top_features['importance'], 
                    range(len(top_features)),
                    xerr=top_features['std'], 
                    fmt='none', 
                    color='black', 
                    capsize=5)
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Permutation Importance (Mean Â± Std)')
        plt.title('Top 20 Most Important Features - Permutation Importance')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ’åˆ—é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\næ’åˆ—é‡è¦æ€§ç»Ÿè®¡:")
        print("-" * 50)
        print(f"å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        for i, (_, row) in enumerate(importances.head(10).iterrows(), 1):
            print(f"{i:2d}. {row['feature']:25s} {row['importance']:8.4f} Â± {row['std']:6.4f}")
        
        return importances

    def clean_feature_name(self, feature_name):
        """
        æ¸…ç†ç‰¹å¾åç§°ï¼Œå°†Logå˜æ¢çš„å˜é‡åè½¬æ¢ä¸ºåŸå˜é‡å
        """
        if feature_name.startswith('Log1p_'):
            return feature_name.replace('Log1p_', '')
        else:
            return feature_name

    def feature_importance_combined_analysis(self, n_features=20, filename="feature_importance_combined.png", use_builtin=False):
        """
        ä¿®å¤ç‰ˆï¼šç»“åˆæ’åˆ—é‡è¦æ€§å’Œåä¾èµ–åˆ†æçš„ç»¼åˆç‰¹å¾é‡è¦æ€§å›¾
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è¿›è¡Œç»¼åˆç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        if use_builtin:
            print("ä½¿ç”¨éšæœºæ£®æ—å†…ç½®é‡è¦æ€§...")
            # ä½¿ç”¨å†…ç½®é‡è¦æ€§
            importances = pd.DataFrame({
                'feature': self.analysis_vars,
                'importance': self.best_model.feature_importances_,
                'std': np.zeros(len(self.analysis_vars))  # å†…ç½®é‡è¦æ€§æ²¡æœ‰æ ‡å‡†å·®
            })
        else:
            print("ä½¿ç”¨æ’åˆ—é‡è¦æ€§...")
            # è®¡ç®—æ’åˆ—é‡è¦æ€§ - ç¡®ä¿å‚æ•°ä¸€è‡´
            r = permutation_importance(
                self.best_model, 
                self.X, 
                self.y, 
                n_repeats=10, 
                random_state=self.random_state,
                scoring='neg_mean_squared_error'
            )
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importances = pd.DataFrame({
                'feature': self.analysis_vars,
                'importance': r.importances_mean,
                'std': r.importances_std
            })
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # æ¸…ç†ç‰¹å¾åç§°ï¼ˆå»é™¤Log1p_å‰ç¼€ï¼‰
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        
        # ä¿®æ­£åçš„ç‰¹å¾åˆ†ç±»å­—å…¸
        feature_categories = {
            # åœ°å½¢åœ°è²Œç‰¹å¾ (Physiography)
            'Elevation': 'Physiography',
            'slp_dg_uav': 'Physiography',
            'ele_mt_uav': 'Physiography',
            
            # æ°´æ–‡ç‰¹å¾ (Hydrology)
            'Depth_avg': 'Hydrology',
            'Vol_total': 'Hydrology',
            'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology',
            'Wshd_area': 'Hydrology',
            'run_mm_vyr': 'Hydrology',
            'dis_m3_pyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology',
            'Tyear_mean': 'Hydrology',
            'Res_time': 'Hydrology',
            'lkv_mc_usu': 'Hydrology',
            
            # æ°”å€™ç‰¹å¾ (Climate)
            'pre_mm_uyr': 'Climate',
            'pre_mm_lyr': 'Climate',
            'tmp_dc_lyr': 'Climate',
            'ice_days': 'Climate',
            'ari_ix_lav': 'Climate',
            
            # äººä¸ºç‰¹å¾ (Anthropogenic)
            'Population_Density': 'Anthropogenic',
            'ppd_pk_vav': 'Anthropogenic',
            'hft_ix_v09': 'Anthropogenic',
            'urb_pc_vse': 'Anthropogenic',
            
            # åœŸåœ°è¦†ç›– (Landcover)
            'for_pc_vse': 'Landcover',
            'crp_pc_vse': 'Landcover',
            
            # åœŸå£¤ä¸åœ°è´¨ç‰¹å¾ (Soils & Geology)
            'soc_th_vav': 'Soils & Geology',
            'ero_kh_vav': 'Soils & Geology',
            'gwt_cm_vav': 'Soils & Geology',
            
            # æ°´è´¨ç‰¹å¾ (Water quality)
            'Chla_pred_RF': 'Water quality',
            'Chla_Preds_Mean': 'Water quality',
            'TN_Load_Per_Volume': 'Water quality',
            'TP_Load_Per_Volume': 'Water quality',
            'TN_Inputs_Mean': 'Water quality',
            'TP_Inputs_Mean': 'Water quality',
            'TN_Preds_Mean': 'Water quality',
            'TP_Preds_Mean': 'Water quality'
        }
                
        # æ·»åŠ ç±»åˆ«ä¿¡æ¯ï¼ˆåŸºäºæ¸…ç†åçš„ç‰¹å¾åï¼‰
        importances['category'] = importances['clean_feature'].map(
            lambda x: feature_categories.get(x, 'Other')
        )
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©é¡¶éƒ¨ç‰¹å¾
        importances = importances.sort_values('importance', ascending=True)
        top_importances = importances.tail(n_features)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',      # ç»¿è‰²
            'Hydrology': '#7FB3D5',    # è“è‰²
            'Anthropogenic': '#F1948A', # çº¢è‰²
            'Landcover': '#F4D03F',    # é»„è‰²
            'Physiography': '#BFC9CA', # ç°è‰²
            'Soils & Geology': '#E59866', # æ£•è‰²
            'Water quality': '#DDA0DD', # æ·¡ç´«è‰²
            'Other': '#D5D8DC'         # æµ…ç°è‰²
        }
    
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
        bars = ax.barh(range(len(top_importances)), 
                       top_importances['importance'],
                       color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
                       alpha=0.8,
                       edgecolor='black',
                       linewidth=0.5)
        
        print("æ­£åœ¨è®¡ç®—åä¾èµ–æ›²çº¿...")
        
        # ä¸ºæ¯ä¸ªç‰¹å¾è®¡ç®—å¹¶ç»˜åˆ¶åä¾èµ–æ›²çº¿
        for idx, (_, row) in enumerate(top_importances.iterrows()):
            feature = row['feature']  # ä½¿ç”¨åŸå§‹ç‰¹å¾åï¼ˆåŒ…å«Log1p_ï¼‰
            importance = row['importance']
            
            try:
                # ç¡®ä¿ç‰¹å¾åœ¨æ•°æ®ä¸­å­˜åœ¨
                if feature not in self.X.columns:
                    print(f"è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨æ•°æ®ä¸­ï¼Œè·³è¿‡åä¾èµ–è®¡ç®—")
                    continue
                
                # è·å–ç‰¹å¾æ•°æ®ï¼Œç¡®ä¿æ²¡æœ‰æ— æ•ˆå€¼
                feature_data = self.X[feature].values
                
                # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                if np.isnan(feature_data).all() or np.isinf(feature_data).any():
                    print(f"è­¦å‘Š: ç‰¹å¾ {feature} åŒ…å«æ— æ•ˆæ•°æ®ï¼Œè·³è¿‡åä¾èµ–è®¡ç®—")
                    continue
                
                # ä½¿ç”¨sklearnçš„partial_dependence
                try:
                    feature_idx = list(self.X.columns).index(feature)
                    pdp_result = partial_dependence(
                        self.best_model, 
                        self.X, 
                        [feature_idx], 
                        grid_resolution=30,  # å‡å°‘ç½‘æ ¼åˆ†è¾¨ç‡
                        kind='average'
                    )
                    
                    # å®‰å…¨åœ°æå–ç»“æœ
                    if len(pdp_result) >= 2 and len(pdp_result[0]) > 0 and len(pdp_result[1]) > 0:
                        pdp_values = pdp_result[0][0]
                        feature_values = pdp_result[1][0]
                        
                        # æ£€æŸ¥ç»“æœæœ‰æ•ˆæ€§
                        if len(pdp_values) > 1 and len(feature_values) > 1:
                            # ç¡®ä¿æ²¡æœ‰æ— æ•ˆå€¼
                            valid_mask = ~(np.isnan(pdp_values) | np.isinf(pdp_values))
                            if np.sum(valid_mask) > 1:
                                pdp_values = pdp_values[valid_mask]
                                feature_values = feature_values[valid_mask]
                                
                                # æ ‡å‡†åŒ–å¹¶ç¼©æ”¾åä¾èµ–æ›²çº¿
                                if len(np.unique(pdp_values)) > 1:  # ç¡®ä¿æœ‰å˜åŒ–
                                    # æ ‡å‡†åŒ–åˆ° [0, 1]
                                    pdp_norm = (pdp_values - np.min(pdp_values)) / (np.max(pdp_values) - np.min(pdp_values))
                                    # ç¼©æ”¾åˆ°æ¡å½¢å›¾å®½åº¦çš„70%
                                    pdp_scaled = pdp_norm * importance * 0.7
                                    
                                    # è·å–é¢œè‰²å¹¶è°ƒæš—
                                    category = row['category']
                                    base_color = category_colors.get(category, '#D5D8DC')
                                    from matplotlib.colors import to_rgb
                                    rgb = to_rgb(base_color)
                                    darker_color = tuple(c * 0.5 for c in rgb)
                                    
                                    # ç»˜åˆ¶åä¾èµ–æ›²çº¿
                                    ax.plot(pdp_scaled, [idx] * len(pdp_scaled), 
                                           color=darker_color, 
                                           linewidth=2.0, 
                                           alpha=0.9,
                                           zorder=10)
                                    
                except Exception as pdp_error:
                    print(f"è®¡ç®—ç‰¹å¾ {feature} çš„åä¾èµ–æ—¶å‡ºé”™: {pdp_error}")
                    continue
                    
            except Exception as e:
                print(f"å¤„ç†ç‰¹å¾ {feature} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(top_importances['importance'], range(len(top_importances)),
                    xerr=top_importances['std'], fmt='none', color='black', 
                    capsize=3, alpha=0.7, zorder=5)
        
        # è‡ªå®šä¹‰å›¾å½¢ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        ax.set_yticks(range(len(top_importances)))
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('Permutation Importance', fontsize=12)
        ax.set_title('Main Drivers of N2O Concentrations in Lakes\n(Permutation Importance with Partial Dependence)', 
                     fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # å›¾ä¾‹ä½ç½®é€‰é¡¹ - æ‚¨å¯ä»¥é€‰æ‹©å…¶ä¸­ä¸€ä¸ª
        unique_categories = top_importances['category'].unique()
        legend_elements = [plt.Rectangle((0,0), 1, 1, facecolor=category_colors.get(cat, '#D5D8DC'), 
                                       label=cat, edgecolor='black', alpha=0.8) 
                          for cat in sorted(unique_categories)]
        
        # é€‰é¡¹1: å›¾ä¾‹åœ¨å³ä¾§æ¡†å†… (å³ä¸Šè§’)
        # ax.legend(handles=legend_elements, 
        #          title='Category',
        #          loc='upper right',
        #          fontsize=9,
        #          title_fontsize=10)
        
        # é€‰é¡¹2: å›¾ä¾‹åœ¨å³ä¾§æ¡†å†… (å³ä¸‹è§’)
        # ax.legend(handles=legend_elements, 
        #          title='Category',
        #          loc='lower right',
        #          fontsize=9,
        #          title_fontsize=10)
        
        # é€‰é¡¹3: å›¾ä¾‹åœ¨å³ä¾§æ¡†å†… (ä¸­é—´å³ä¾§)
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=9,
                 title_fontsize=10)
        
        # é€‰é¡¹4: å›¾ä¾‹åœ¨å›¾å¤–å³ä¾§ (å¦‚æœæ‚¨æƒ³è¦å›¾å¤–)
        # ax.legend(handles=legend_elements, 
        #          title='Category',
        #          loc='center left', 
        #          bbox_to_anchor=(1.02, 0.5),
        #          fontsize=10,
        #          title_fontsize=11)
        
        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ç»¼åˆç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        print("\nç»¼åˆç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ:")
        print("-" * 60)
        print(f"å‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾åŠå…¶ç±»åˆ«:")
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:30s} {row['category']:15s} {row['importance']:8.4f} Â± {row['std']:6.4f}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = top_importances.groupby('category').agg({
            'importance': ['count', 'mean', 'sum']
        }).round(4)
        print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        print(category_stats)
        
        return top_importances
    
    def partial_dependence_analysis_fixed(self, feature_names, n_points=50, filename="partial_dependence.png"):
        """
        ä¿®å¤çš„åä¾èµ–åˆ†æå‡½æ•°
        """
        if self.best_model is None or self.X is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        n_features = len(feature_names)
        if n_features == 0:
            print("æ²¡æœ‰æä¾›ç‰¹å¾åç§°")
            return
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(n_features, 1, figsize=(12, n_features*3))
        if n_features == 1:
            axes = [axes]
        
        print(f"æ­£åœ¨ä¸º {n_features} ä¸ªç‰¹å¾è®¡ç®—åä¾èµ–...")
        
        for idx, feature in enumerate(feature_names):
            if feature not in self.X.columns:
                print(f"è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨æ•°æ®ä¸­")
                continue
                
            try:
                feature_idx = list(self.X.columns).index(feature)
                
                # è®¡ç®—åä¾èµ–
                pdp_result = partial_dependence(
                    self.best_model, 
                    self.X, 
                    [feature_idx], 
                    grid_resolution=n_points,
                    kind='average'
                )
                
                # è·å–ç»“æœ
                pdp_values = pdp_result[0][0]
                feature_values = pdp_result[1][0]
                
                # ä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åä½œä¸ºæ ‡é¢˜
                clean_name = self.clean_feature_name(feature)
                
                # ç»˜åˆ¶åä¾èµ–å›¾
                axes[idx].plot(feature_values, pdp_values, linewidth=2, color='blue')
                axes[idx].set_xlabel(clean_name, fontsize=10)
                axes[idx].set_ylabel('Partial dependence', fontsize=10)
                axes[idx].set_title(f'Partial Dependence Plot for {clean_name}', fontsize=12)
                axes[idx].grid(True, alpha=0.3)
                
                print(f"âœ“ å®Œæˆç‰¹å¾: {clean_name}")
                
            except Exception as e:
                print(f"âœ— è®¡ç®— {feature} çš„åä¾èµ–æ—¶å‡ºé”™: {e}")
                # åœ¨å‡ºé”™çš„å­å›¾ä¸Šæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                clean_name = self.clean_feature_name(feature)
                axes[idx].text(0.5, 0.5, f'Error calculating PDP for {clean_name}', 
                              ha='center', va='center', transform=axes[idx].transAxes)
                axes[idx].set_title(f'Error: {clean_name}')
        
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"åä¾èµ–å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()

    def diagnose_importance_difference(self):
        """
        è¯Šæ–­æ’åˆ—é‡è¦æ€§å’Œå†…ç½®é‡è¦æ€§çš„å·®å¼‚
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è¯Šæ–­ä¸¤ç§é‡è¦æ€§æ–¹æ³•çš„å·®å¼‚...")
        
        # 1. å†…ç½®é‡è¦æ€§
        builtin_imp = self.best_model.feature_importances_
        
        # 2. æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=10, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        permutation_imp = r.importances_mean
        
        # åˆ›å»ºæ¯”è¾ƒDataFrame
        comparison_df = pd.DataFrame({
            'feature': self.analysis_vars,
            'builtin': builtin_imp,
            'permutation': permutation_imp
        })
        
        # æ¸…ç†ç‰¹å¾å
        comparison_df['clean_feature'] = comparison_df['feature'].apply(self.clean_feature_name)
        
        # æ’åºæ˜¾ç¤º
        comparison_df = comparison_df.sort_values('permutation', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆå‰15ä¸ªï¼‰:")
        print("-" * 80)
        print(f"{'Feature':<25} {'Builtin':<12} {'Permutation':<12} {'Ratio':<8}")
        print("-" * 80)
        
        for _, row in comparison_df.head(15).iterrows():
            ratio = row['permutation'] / row['builtin'] if row['builtin'] > 0 else 0
            print(f"{row['clean_feature']:<25} {row['builtin']:<12.6f} {row['permutation']:<12.6f} {ratio:<8.2f}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        correlation = np.corrcoef(builtin_imp, permutation_imp)[0, 1]
        print(f"\nç›¸å…³ç³»æ•°: {correlation:.4f}")
        print(f"å†…ç½®é‡è¦æ€§æ€»å’Œ: {np.sum(builtin_imp):.6f}")
        print(f"æ’åˆ—é‡è¦æ€§æ€»å’Œ: {np.sum(permutation_imp):.6f}")
        
        return comparison_df

    def compare_importance_methods(self, filename="importance_comparison.png"):
        """
        æ¯”è¾ƒä¸åŒé‡è¦æ€§æ–¹æ³•çš„ç»“æœ
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨æ¯”è¾ƒä¸åŒçš„ç‰¹å¾é‡è¦æ€§æ–¹æ³•...")
        
        # 1. å†…ç½®é‡è¦æ€§
        builtin_importance = pd.DataFrame({
            'feature': self.analysis_vars,
            'builtin_importance': self.best_model.feature_importances_
        })
        
        # 2. æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=5, 
            random_state=self.random_state
        )
        
        permutation_importance_df = pd.DataFrame({
            'feature': self.analysis_vars,
            'permutation_importance': r.importances_mean
        })
        
        # åˆå¹¶æ•°æ®
        comparison_df = builtin_importance.merge(permutation_importance_df, on='feature')
        
        # æ·»åŠ æ¸…ç†åçš„ç‰¹å¾å
        comparison_df['clean_feature'] = comparison_df['feature'].apply(self.clean_feature_name)
        
        # æ ‡å‡†åŒ–é‡è¦æ€§å€¼ï¼ˆ0-1èŒƒå›´ï¼‰
        comparison_df['builtin_norm'] = (
            comparison_df['builtin_importance'] / comparison_df['builtin_importance'].max()
        )
        comparison_df['permutation_norm'] = (
            comparison_df['permutation_importance'] / comparison_df['permutation_importance'].max()
        )
        
        # é€‰æ‹©å‰15ä¸ªç‰¹å¾ï¼ˆåŸºäºæ’åˆ—é‡è¦æ€§ï¼‰
        top_features = comparison_df.nlargest(15, 'permutation_importance')
        
        # åˆ›å»ºæ¯”è¾ƒå›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # æ•£ç‚¹å›¾æ¯”è¾ƒ
        ax1.scatter(top_features['builtin_norm'], 
                   top_features['permutation_norm'], 
                   alpha=0.7, s=100, color='blue')
        
        # æ·»åŠ ç‰¹å¾åç§°æ ‡ç­¾ï¼ˆä½¿ç”¨æ¸…ç†åçš„åç§°ï¼‰
        for _, row in top_features.iterrows():
            ax1.annotate(row['clean_feature'], 
                        (row['builtin_norm'], row['permutation_norm']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, alpha=0.8)
        
        # æ·»åŠ å¯¹è§’çº¿
        max_val = max(top_features[['builtin_norm', 'permutation_norm']].max())
        ax1.plot([0, max_val], [0, max_val], 'r--', alpha=0.5)
        
        ax1.set_xlabel('Built-in Importance (Normalized)')
        ax1.set_ylabel('Permutation Importance (Normalized)')
        ax1.set_title('Comparison of Feature Importance Methods')
        ax1.grid(True, alpha=0.3)
        
        # æ¡å½¢å›¾æ¯”è¾ƒï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        x = np.arange(len(top_features))
        width = 0.35
        
        bars1 = ax2.bar(x - width/2, top_features['builtin_norm'], 
                       width, label='Built-in Importance', alpha=0.8, color='lightblue')
        bars2 = ax2.bar(x + width/2, top_features['permutation_norm'], 
                       width, label='Permutation Importance', alpha=0.8, color='lightcoral')
        
        ax2.set_xlabel('Features')
        ax2.set_ylabel('Normalized Importance')
        ax2.set_title('Top 15 Features - Method Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(top_features['clean_feature'], rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"é‡è¦æ€§æ–¹æ³•æ¯”è¾ƒå›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(comparison_df['builtin_importance'], 
                                 comparison_df['permutation_importance'])[0, 1]
        print(f"\nä¸¤ç§é‡è¦æ€§æ–¹æ³•çš„ç›¸å…³ç³»æ•°: {correlation:.4f}")
        
        return comparison_df
        """
        æ¯”è¾ƒä¸åŒé‡è¦æ€§æ–¹æ³•çš„ç»“æœ
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨æ¯”è¾ƒä¸åŒçš„ç‰¹å¾é‡è¦æ€§æ–¹æ³•...")
        
        # 1. å†…ç½®é‡è¦æ€§
        builtin_importance = pd.DataFrame({
            'feature': self.analysis_vars,
            'builtin_importance': self.best_model.feature_importances_
        })
        
        # 2. æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=5, 
            random_state=self.random_state
        )
        
        permutation_importance_df = pd.DataFrame({
            'feature': self.analysis_vars,
            'permutation_importance': r.importances_mean
        })
        
        # åˆå¹¶æ•°æ®
        comparison_df = builtin_importance.merge(permutation_importance_df, on='feature')
        
        # æ·»åŠ æ¸…ç†åçš„ç‰¹å¾å
        comparison_df['clean_feature'] = comparison_df['feature'].apply(self.clean_feature_name)
        
        # æ ‡å‡†åŒ–é‡è¦æ€§å€¼ï¼ˆ0-1èŒƒå›´ï¼‰
        comparison_df['builtin_norm'] = (
            comparison_df['builtin_importance'] / comparison_df['builtin_importance'].max()
        )
        comparison_df['permutation_norm'] = (
            comparison_df['permutation_importance'] / comparison_df['permutation_importance'].max()
        )
        
        # é€‰æ‹©å‰15ä¸ªç‰¹å¾ï¼ˆåŸºäºæ’åˆ—é‡è¦æ€§ï¼‰
        top_features = comparison_df.nlargest(15, 'permutation_importance')
        
        # åˆ›å»ºæ¯”è¾ƒå›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # æ•£ç‚¹å›¾æ¯”è¾ƒ
        ax1.scatter(top_features['builtin_norm'], 
                   top_features['permutation_norm'], 
                   alpha=0.7, s=100, color='blue')
        
        # æ·»åŠ ç‰¹å¾åç§°æ ‡ç­¾ï¼ˆä½¿ç”¨æ¸…ç†åçš„åç§°ï¼‰
        for _, row in top_features.iterrows():
            ax1.annotate(row['clean_feature'], 
                        (row['builtin_norm'], row['permutation_norm']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, alpha=0.8)
        
        # æ·»åŠ å¯¹è§’çº¿
        max_val = max(top_features[['builtin_norm', 'permutation_norm']].max())
        ax1.plot([0, max_val], [0, max_val], 'r--', alpha=0.5)
        
        ax1.set_xlabel('Built-in Importance (Normalized)')
        ax1.set_ylabel('Permutation Importance (Normalized)')
        ax1.set_title('Comparison of Feature Importance Methods')
        ax1.grid(True, alpha=0.3)
        
        # æ¡å½¢å›¾æ¯”è¾ƒï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        x = np.arange(len(top_features))
        width = 0.35
        
        bars1 = ax2.bar(x - width/2, top_features['builtin_norm'], 
                       width, label='Built-in Importance', alpha=0.8, color='lightblue')
        bars2 = ax2.bar(x + width/2, top_features['permutation_norm'], 
                       width, label='Permutation Importance', alpha=0.8, color='lightcoral')
        
        ax2.set_xlabel('Features')
        ax2.set_ylabel('Normalized Importance')
        ax2.set_title('Top 15 Features - Method Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(top_features['clean_feature'], rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"é‡è¦æ€§æ–¹æ³•æ¯”è¾ƒå›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(comparison_df['builtin_importance'], 
                                 comparison_df['permutation_importance'])[0, 1]
        print(f"\nä¸¤ç§é‡è¦æ€§æ–¹æ³•çš„ç›¸å…³ç³»æ•°: {correlation:.4f}")
        
        return comparison_df

    def save_model(self, filepath):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'analysis_vars': self.analysis_vars,
            'variables': self.variables,
            'variables_removed': self.variables_removed,
            'log_transform_vars': self.log_transform_vars
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹ä¿å­˜è‡³: {filepath}")

    def load_model(self, filepath):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.best_model = model_data['best_model']
        self.scaler = model_data['scaler']
        self.best_params = model_data['best_params']
        self.analysis_vars = model_data['analysis_vars']
        self.variables = model_data['variables']
        self.variables_removed = model_data['variables_removed']
        self.log_transform_vars = model_data['log_transform_vars']
        
        print(f"æ¨¡å‹ä» {filepath} åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹å‚æ•°: {self.best_params}")


def main_enhanced_feature_importance_analysis():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¸¦åä¾èµ–æ›²çº¿ï¼‰"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æç³»ç»Ÿ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = EnhancedN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    training_data_path = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(training_data_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ {training_data_path}")
        return
    
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
    print(f"æ•°æ®å½¢çŠ¶: X = {X_scaled.shape}, y = {y.shape}")
    
    print("\n2. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    predictor.train_improved_model_with_repeated_cv(X_scaled, y)
    
    # ç®€å•çš„æ€§èƒ½è¯„ä¼°
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y, test_size=0.3, random_state=predictor.random_state
    )
    results = predictor.evaluate_model(X_train, X_val, y_train, y_val)
    print(f"\næ¨¡å‹æ€§èƒ½:")
    print(f"- è®­ç»ƒé›† RÂ²: {results['train_r2']:.4f}")
    print(f"- éªŒè¯é›† RÂ²: {results['val_r2']:.4f}")
    print(f"- OOB Score: {results['oob_score']:.4f}")
    
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # 3.0 è¯Šæ–­ä¸¤ç§é‡è¦æ€§æ–¹æ³•çš„å·®å¼‚
    print("\n3.0 è¯Šæ–­é‡è¦æ€§æ–¹æ³•å·®å¼‚...")
    predictor.diagnose_importance_difference()
    
    # 3.1 æ’åˆ—é‡è¦æ€§  
    print("\n3.1 æ’åˆ—é‡è¦æ€§åˆ†æ...")
    permutation_importance = predictor.feature_importance_permutation(n_repeats=10)
    
    # 3.2 å¢å¼ºç‰ˆç»¼åˆåˆ†æï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
    print("\n3.2 å¢å¼ºç‰ˆç»¼åˆç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¸¦åä¾èµ–æ›²çº¿ï¼‰...")
    
    # è®©ç”¨æˆ·é€‰æ‹©ä½¿ç”¨å“ªç§é‡è¦æ€§è®¡ç®—æ–¹æ³•
    importance_method = input("é€‰æ‹©é‡è¦æ€§è®¡ç®—æ–¹æ³• (1=æ’åˆ—é‡è¦æ€§, 2=å†…ç½®é‡è¦æ€§): ")
    use_builtin = importance_method == '2'
    
    if use_builtin:
        print("ä½¿ç”¨éšæœºæ£®æ—å†…ç½®é‡è¦æ€§è¿›è¡Œåˆ†æ...")
    else:
        print("ä½¿ç”¨æ’åˆ—é‡è¦æ€§è¿›è¡Œåˆ†æ...")
    
    combined_importance = predictor.feature_importance_combined_analysis(
        n_features=20, 
        use_builtin=use_builtin
    )
    
    # 3.3 å¯é€‰ï¼šå•ç‹¬çš„åä¾èµ–åˆ†æï¼ˆå‰5ä¸ªé‡è¦ç‰¹å¾ï¼‰
    if input("\næ˜¯å¦ç”Ÿæˆå•ç‹¬çš„åä¾èµ–å›¾ï¼Ÿ(y/n): ").lower() == 'y':
        print("\n3.3 å•ç‹¬åä¾èµ–åˆ†æ...")
        top_5_features = permutation_importance.head(5)['feature'].tolist()
        predictor.partial_dependence_analysis_fixed(top_5_features)
    
    # 4. ä¿å­˜æ¨¡å‹
    print("\n4. ä¿å­˜æ¨¡å‹...")
    model_save_path = "n2o_model_enhanced.pkl"
    predictor.save_model(model_save_path)
    
    print("\n" + "="*60)
    print("å¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆï¼")
    print("="*60)
    print("\nç”Ÿæˆçš„å…³é”®æ–‡ä»¶:")
    print("- feature_importance_permutation.png: æ’åˆ—é‡è¦æ€§")
    print("- feature_importance_combined.png: ğŸŒŸ å¢å¼ºç‰ˆç»¼åˆåˆ†æï¼ˆå¸¦åä¾èµ–æ›²çº¿ï¼‰")
    print("- partial_dependence.png: å•ç‹¬åä¾èµ–åˆ†æï¼ˆå¯é€‰ï¼‰")
    print(f"- {model_save_path}: è®­ç»ƒå¥½çš„æ¨¡å‹")
    
    # è¾“å‡ºå…³é”®å‘ç°æ‘˜è¦
    print("\nğŸ” å…³é”®å‘ç°æ‘˜è¦:")
    print("-" * 40)
    top_5_features = combined_importance.tail(5)
    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
        print(f"{i}. {row['clean_feature']} ({row['category']}) - é‡è¦æ€§: {row['importance']:.4f}")
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("- æ¡å½¢å›¾æ˜¾ç¤ºç‰¹å¾çš„æ’åˆ—é‡è¦æ€§")
    print("- æ¡å½¢å›¾å†…çš„æ·±è‰²æ›²çº¿æ˜¾ç¤ºåä¾èµ–å…³ç³»")
    print("- é¢œè‰²åˆ†ç±»ï¼šç»¿è‰²=æ°”å€™ï¼Œè“è‰²=æ°´æ–‡ï¼Œçº¢è‰²=äººç±»æ´»åŠ¨ç­‰")
    print("- Logå˜æ¢çš„å˜é‡å·²æ˜¾ç¤ºä¸ºåŸå˜é‡å")
    
    return predictor


if __name__ == "__main__":
    # è¿è¡Œå¢å¼ºç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ
    predictor = main_enhanced_feature_importance_analysis()



ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆå‰15ä¸ªï¼‰:
--------------------------------------------------------------------------------
Feature                   Builtin      Permutation  Ratio   
--------------------------------------------------------------------------------
Elevation                 0.150945     0.193582     1.28    
Population_Density        0.116880     0.122124     1.04    
run_mm_vyr                0.094807     0.116849     1.23    
crp_pc_vse                0.077401     0.110513     1.43    
ari_ix_lav                0.105159     0.088271     0.84    
pre_mm_uyr                0.083352     0.071588     0.86    
ero_kh_vav                0.045132     0.037016     0.82    
Lake_area                 0.043348     0.035364     0.82    
soc_th_vav                0.041077     0.029639     0.72    
Vol_total                 0.028664     0.015679     0.55    
hft_ix_v09                0.023935     0.014329     0.60    
gwt_cm_vav                0.024065     0.014140     0.59    
Tyear_mean_open           0.025074     0.012854     0.51    
Depth_avg                 0.023898     0.012711     0.53    
for_pc_vse                0.019613     0.011302     0.58   

ğŸ” å…³é”®å‘ç°æ‘˜è¦:
----------------------------------------
1. ari_ix_lav (Climate) - é‡è¦æ€§: 0.0883
2. crp_pc_vse (Landcover) - é‡è¦æ€§: 0.1105
3. run_mm_vyr (Hydrology) - é‡è¦æ€§: 0.1168
4. Population_Density (Anthropogenic) - é‡è¦æ€§: 0.1221
5. Elevation (Physiography) - é‡è¦æ€§: 0.1936

æŒ‰ç±»åˆ«ç»Ÿè®¡:
                importance                
                     count    mean     sum
category                                  
Anthropogenic            2  0.0682  0.1365
Climate                  2  0.0799  0.1599
Hydrology                7  0.0296  0.2072
Landcover                2  0.0609  0.1218
Physiography             2  0.1017  0.2033
Soils & Geology          3  0.0269  0.0808
Water quality            2  0.0065  0.0131

#%% æ’åˆ—é‡è¦æ€§ 0813


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
import warnings
import pickle
from datetime import datetime
warnings.filterwarnings('ignore')

class EnhancedN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        self.X = None  # ä¿å­˜è®­ç»ƒæ•°æ®ç”¨äºé‡è¦æ€§åˆ†æ
        self.y = None  # ä¿å­˜ç›®æ ‡å˜é‡ç”¨äºé‡è¦æ€§åˆ†æ
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # ä½¿ç”¨RobustScalerè¿›è¡Œç¼©æ”¾
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        return X_scaled, y

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """ä½¿ç”¨é¢„è®¾æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹"""
        
        # ä¿å­˜æ•°æ®ç”¨äºåç»­åˆ†æ
        self.X = X
        self.y = y
        
        # ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°
        best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        print(f"ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹:")
        print(f"å‚æ•°: {best_params}")
        
        # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
        rf_reg = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **best_params
        )
        
        print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        rf_reg.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = rf_reg
        self.best_params = best_params
        
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"OOB Score: {rf_reg.oob_score_:.4f}")
        
        return self.best_model

    def evaluate_model(self, X_train, X_val, y_train, y_val):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…å«è¯¦ç»†çš„æ€§èƒ½åˆ†æ"""
        k_folds = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        cv_scores = cross_val_score(self.best_model, X_train, y_train, cv=k_folds, scoring='r2')
        
        # å¯¹æ•°ç©ºé—´çš„é¢„æµ‹
        y_train_pred = self.best_model.predict(X_train)
        y_val_pred = self.best_model.predict(X_val)
        
        # å¯¹æ•°ç©ºé—´çš„R2
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        
        # åŸå§‹å°ºåº¦çš„RMSEè®¡ç®—
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
        
        # æ·»åŠ OOBåˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        oob_score = getattr(self.best_model, 'oob_score_', None)
        
        return {
            'cv_scores': cv_scores,
            'train_r2': train_r2,
            'val_r2': val_r2,
            'train_rmse': train_rmse,
            'val_rmse': val_rmse,
            'oob_score': oob_score,
            'y_val_true': y_val,
            'y_val_pred': y_val_pred
        }

    def feature_importance_builtin(self, filename="feature_importance_builtin.png"):
        """
        è®¡ç®—å¹¶å±•ç¤ºéšæœºæ£®æ—å†…ç½®ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºåŸºå°¼ä¸çº¯åº¦ï¼‰
        """
        if self.best_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
            
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': self.best_model.feature_importances_
        })
        importances = importances.sort_values('importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        plt.style.use('default')
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_features = importances.head(20)
        plt.barh(np.arange(len(top_features)), 
                top_features['importance'],
                align='center',
                color='lightblue',
                edgecolor='black')
        plt.yticks(np.arange(len(top_features)), 
                  top_features['feature'])
        plt.xlabel('Feature Importance (Built-in)')
        plt.title('Top 20 Most Important Features - Random Forest Built-in Importance')
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å†…ç½®ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        return importances

    def feature_importance_permutation(self, n_repeats=10, filename="feature_importance_permutation.png"):
        """
        è®¡ç®—å¹¶å±•ç¤ºæ’åˆ—é‡è¦æ€§ï¼ˆPermutation Importanceï¼‰
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è®¡ç®—æ’åˆ—é‡è¦æ€§...")
        print(f"é‡å¤æ¬¡æ•°: {n_repeats}")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # æŒ‰é‡è¦æ€§æ’åº
        importances = importances.sort_values('importance', ascending=False)
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        plt.figure(figsize=(12, 8))
        top_features = importances.head(20)
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        bars = plt.barh(range(len(top_features)), 
                       top_features['importance'],
                       color='lightcoral',
                       edgecolor='black',
                       alpha=0.8)
        
        # æ·»åŠ è¯¯å·®æ¡
        plt.errorbar(top_features['importance'], 
                    range(len(top_features)),
                    xerr=top_features['std'], 
                    fmt='none', 
                    color='black', 
                    capsize=5)
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Permutation Importance (Mean Â± Std)')
        plt.title('Top 20 Most Important Features - Permutation Importance')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"æ’åˆ—é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\næ’åˆ—é‡è¦æ€§ç»Ÿè®¡:")
        print("-" * 50)
        print(f"å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
        for i, (_, row) in enumerate(importances.head(10).iterrows(), 1):
            print(f"{i:2d}. {row['feature']:25s} {row['importance']:8.4f} Â± {row['std']:6.4f}")
        
        return importances

    def clean_feature_name(self, feature_name):
        """
        æ¸…ç†ç‰¹å¾åç§°ï¼Œå°†Logå˜æ¢çš„å˜é‡åè½¬æ¢ä¸ºåŸå˜é‡å
        """
        if feature_name.startswith('Log1p_'):
            return feature_name.replace('Log1p_', '')
        else:
            return feature_name

    def feature_importance_permutation_with_categories(self, n_features=20, filename="feature_importance_categorized.png"):
        """
        å¸¦ç±»åˆ«åˆ†ç±»çš„æ’åˆ—é‡è¦æ€§åˆ†æ
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è¿›è¡Œå¸¦ç±»åˆ«çš„æ’åˆ—é‡è¦æ€§åˆ†æ...")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.best_model, 
            self.X, 
            self.y, 
            n_repeats=10, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # æ¸…ç†ç‰¹å¾åç§°ï¼ˆå»é™¤Log1p_å‰ç¼€ï¼‰
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        
        # ç‰¹å¾åˆ†ç±»å­—å…¸
        feature_categories = {
            # åœ°å½¢åœ°è²Œç‰¹å¾ (Physiography)
            'Elevation': 'Physiography',
            'slp_dg_uav': 'Physiography',
            'ele_mt_uav': 'Physiography',
            
            # æ°´æ–‡ç‰¹å¾ (Hydrology)
            'Depth_avg': 'Hydrology',
            'Vol_total': 'Hydrology',
            'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology',
            'Wshd_area': 'Hydrology',
            'run_mm_vyr': 'Hydrology',
            'dis_m3_pyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology',
            'Tyear_mean': 'Hydrology',
            'Res_time': 'Hydrology',
            'lkv_mc_usu': 'Hydrology',
            
            # æ°”å€™ç‰¹å¾ (Climate)
            'pre_mm_uyr': 'Climate',
            'pre_mm_lyr': 'Climate',
            'tmp_dc_lyr': 'Climate',
            'ice_days': 'Climate',
            'ari_ix_lav': 'Climate',
            
            # äººä¸ºç‰¹å¾ (Anthropogenic)
            'Population_Density': 'Anthropogenic',
            'ppd_pk_vav': 'Anthropogenic',
            'hft_ix_v09': 'Anthropogenic',
            'urb_pc_vse': 'Anthropogenic',
            
            # åœŸåœ°è¦†ç›– (Landcover)
            'for_pc_vse': 'Landcover',
            'crp_pc_vse': 'Landcover',
            
            # åœŸå£¤ä¸åœ°è´¨ç‰¹å¾ (Soils & Geology)
            'soc_th_vav': 'Soils & Geology',
            'ero_kh_vav': 'Soils & Geology',
            'gwt_cm_vav': 'Soils & Geology',
            
            # æ°´è´¨ç‰¹å¾ (Water quality)
            'Chla_pred_RF': 'Water quality',
            'Chla_Preds_Mean': 'Water quality',
            'TN_Load_Per_Volume': 'Water quality',
            'TP_Load_Per_Volume': 'Water quality',
            'TN_Inputs_Mean': 'Water quality',
            'TP_Inputs_Mean': 'Water quality',
            'TN_Preds_Mean': 'Water quality',
            'TP_Preds_Mean': 'Water quality'
        }
                
        # æ·»åŠ ç±»åˆ«ä¿¡æ¯ï¼ˆåŸºäºæ¸…ç†åçš„ç‰¹å¾åï¼‰
        importances['category'] = importances['clean_feature'].map(
            lambda x: feature_categories.get(x, 'Other')
        )
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©é¡¶éƒ¨ç‰¹å¾
        importances = importances.sort_values('importance', ascending=True)
        top_importances = importances.tail(n_features)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',      # ç»¿è‰²
            'Hydrology': '#7FB3D5',    # è“è‰²
            'Anthropogenic': '#F1948A', # çº¢è‰²
            'Landcover': '#F4D03F',    # é»„è‰²
            'Physiography': '#BFC9CA', # ç°è‰²
            'Soils & Geology': '#E59866', # æ£•è‰²
            'Water quality': '#DDA0DD', # æ·¡ç´«è‰²
            'Other': '#D5D8DC'         # æµ…ç°è‰²
        }
    
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
        bars = ax.barh(range(len(top_importances)), 
                       top_importances['importance'],
                       color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
                       alpha=0.8,
                       edgecolor='black',
                       linewidth=0.5)
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(top_importances['importance'], range(len(top_importances)),
                    xerr=top_importances['std'], fmt='none', color='black', 
                    capsize=3, alpha=0.7, zorder=5)
        
        # è‡ªå®šä¹‰å›¾å½¢ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        ax.set_yticks(range(len(top_importances)))
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('Permutation Importance', fontsize=12)
        ax.set_title('Main Drivers of N2O Concentrations in Lakes\n(Permutation Importance)', 
                     fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # å›¾ä¾‹
        unique_categories = top_importances['category'].unique()
        legend_elements = [plt.Rectangle((0,0), 1, 1, facecolor=category_colors.get(cat, '#D5D8DC'), 
                                       label=cat, edgecolor='black', alpha=0.8) 
                          for cat in sorted(unique_categories)]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=9,
                 title_fontsize=10)
        
        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"åˆ†ç±»ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        print("\nåˆ†ç±»ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ:")
        print("-" * 60)
        print(f"å‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾åŠå…¶ç±»åˆ«:")
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:30s} {row['category']:15s} {row['importance']:8.4f} Â± {row['std']:6.4f}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = top_importances.groupby('category').agg({
            'importance': ['count', 'mean', 'sum']
        }).round(4)
        print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        print(category_stats)
        
        return top_importances

    def save_model(self, filepath):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'analysis_vars': self.analysis_vars,
            'variables': self.variables,
            'variables_removed': self.variables_removed,
            'log_transform_vars': self.log_transform_vars
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹ä¿å­˜è‡³: {filepath}")

    def load_model(self, filepath):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.best_model = model_data['best_model']
        self.scaler = model_data['scaler']
        self.best_params = model_data['best_params']
        self.analysis_vars = model_data['analysis_vars']
        self.variables = model_data['variables']
        self.variables_removed = model_data['variables_removed']
        self.log_transform_vars = model_data['log_transform_vars']
        
        print(f"æ¨¡å‹ä» {filepath} åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹å‚æ•°: {self.best_params}")


def main_simplified_feature_importance_analysis():
    """ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä»…æ’åˆ—é‡è¦æ€§ï¼‰"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - ç®€åŒ–ç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æç³»ç»Ÿ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = EnhancedN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    training_data_path = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(training_data_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ {training_data_path}")
        return
    
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
    print(f"æ•°æ®å½¢çŠ¶: X = {X_scaled.shape}, y = {y.shape}")
    
    print("\n2. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    predictor.train_improved_model_with_repeated_cv(X_scaled, y)
    
    # ç®€å•çš„æ€§èƒ½è¯„ä¼°
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y, test_size=0.3, random_state=predictor.random_state
    )
    results = predictor.evaluate_model(X_train, X_val, y_train, y_val)
    print(f"\næ¨¡å‹æ€§èƒ½:")
    print(f"- è®­ç»ƒé›† RÂ²: {results['train_r2']:.4f}")
    print(f"- éªŒè¯é›† RÂ²: {results['val_r2']:.4f}")
    print(f"- OOB Score: {results['oob_score']:.4f}")
    
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # 3.1 åŸºæœ¬æ’åˆ—é‡è¦æ€§  
    print("\n3.1 æ’åˆ—é‡è¦æ€§åˆ†æ...")
    permutation_importance = predictor.feature_importance_permutation(n_repeats=10)
    
    # 3.2 å¸¦ç±»åˆ«åˆ†ç±»çš„æ’åˆ—é‡è¦æ€§
    print("\n3.2 å¸¦ç±»åˆ«åˆ†ç±»çš„æ’åˆ—é‡è¦æ€§åˆ†æ...")
    categorized_importance = predictor.feature_importance_permutation_with_categories(n_features=20)
    
    # 4. ä¿å­˜æ¨¡å‹
    print("\n4. ä¿å­˜æ¨¡å‹...")
    model_save_path = "n2o_model_simplified.pkl"
    predictor.save_model(model_save_path)
    
    print("\n" + "="*60)
    print("ç®€åŒ–ç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆï¼")
    print("="*60)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("- feature_importance_permutation.png: åŸºæœ¬æ’åˆ—é‡è¦æ€§")
    print("- feature_importance_categorized.png: å¸¦ç±»åˆ«åˆ†ç±»çš„æ’åˆ—é‡è¦æ€§")
    print(f"- {model_save_path}: è®­ç»ƒå¥½çš„æ¨¡å‹")
    
    # è¾“å‡ºå…³é”®å‘ç°æ‘˜è¦
    print("\nğŸ” å…³é”®å‘ç°æ‘˜è¦:")
    print("-" * 40)
    top_5_features = categorized_importance.tail(5)
    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
        print(f"{i}. {row['clean_feature']} ({row['category']}) - é‡è¦æ€§: {row['importance']:.4f}")
    
    print("\nğŸ’¡ è¯´æ˜:")
    print("- æ’åˆ—é‡è¦æ€§åæ˜ ç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹æ€§èƒ½çš„å®é™…è´¡çŒ®")
    print("- é¢œè‰²åˆ†ç±»ï¼šç»¿è‰²=æ°”å€™ï¼Œè“è‰²=æ°´æ–‡ï¼Œçº¢è‰²=äººç±»æ´»åŠ¨ç­‰")
    print("- Logå˜æ¢çš„å˜é‡å·²æ˜¾ç¤ºä¸ºåŸå˜é‡å")
    
    return predictor


if __name__ == "__main__":
    # è¿è¡Œç®€åŒ–ç‰ˆç‰¹å¾é‡è¦æ€§åˆ†æ
    predictor = main_simplified_feature_importance_analysis()


#%% æ’åˆ—é‡è¦æ€§åˆ†æ çƒ­å›¾ å‡ºå›¾ 0815


import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        # ç‰¹å¾ç±»åˆ«æ˜ å°„
        self.feature_categories = {
            'Elevation': 'Physiography', 'slp_dg_uav': 'Physiography',
            'Depth_avg': 'Hydrology', 'Vol_total': 'Hydrology', 'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology', 'Wshd_area': 'Hydrology', 'run_mm_vyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology', 'Res_time': 'Hydrology',
            'pre_mm_uyr': 'Climate', 'ice_days': 'Climate', 'ari_ix_lav': 'Climate',
            'Population_Density': 'Anthropogenic', 'hft_ix_v09': 'Anthropogenic', 'urb_pc_vse': 'Anthropogenic',
            'for_pc_vse': 'Landcover', 'crp_pc_vse': 'Landcover',
            'soc_th_vav': 'Soils & Geology', 'ero_kh_vav': 'Soils & Geology', 'gwt_cm_vav': 'Soils & Geology',
            'Chla_pred_RF': 'Water quality', 'TN_Load_Per_Volume': 'Water quality', 'TP_Load_Per_Volume': 'Water quality'
        }
        
        self.model = None
        self.analysis_vars = None
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y
    
    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
    
    def clean_feature_name(self, feature_name):
        """æ¸…ç†ç‰¹å¾åç§°"""
        return feature_name.replace('Log1p_', '') if feature_name.startswith('Log1p_') else feature_name
    
    
    def plot_permutation_importance(self, X, y, n_features=20, n_repeats=10):
        """è®¡ç®—å¹¶ç»˜åˆ¶æ’åˆ—é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            
        print(f"è®¡ç®—æ’åˆ—é‡è¦æ€§ (é‡å¤{n_repeats}æ¬¡)...")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.model, X, y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # å°†é‡è¦æ€§è½¬æ¢ä¸ºç™¾åˆ†æ¯” (MSEå¢åŠ çš„ç™¾åˆ†æ¯”)
        importances['importance_pct'] = importances['importance'] * 100
        importances['std_pct'] = importances['std'] * 100
        
        # æ¸…ç†ç‰¹å¾åç§°å¹¶æ·»åŠ ç±»åˆ«
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        importances['category'] = importances['clean_feature'].map(
            lambda x: self.feature_categories.get(x, 'Other')
        )
        
        # é€‰æ‹©å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶æŒ‰é‡è¦æ€§é™åºæ’åˆ—
        top_importances = importances.nlargest(n_features, 'importance').reset_index(drop=True)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',
            'Hydrology': '#7FB3D5', 
            'Anthropogenic': '#F1948A',
            'Landcover': '#F4D03F',
            'Physiography': '#BFC9CA',
            'Soils & Geology': '#E59866',
            'Water quality': '#DDA0DD',
            'Other': '#D5D8DC'
        }
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # åè½¬yè½´é¡ºåºï¼Œè®©æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨
        y_positions = range(len(top_importances))
        y_positions = [len(top_importances) - 1 - i for i in y_positions]
        
        bars = ax.barh(
            y_positions, 
            top_importances['importance_pct'],
            color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
            alpha=0.8,
            edgecolor='black',
            linewidth=0.5
        )
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(
            top_importances['importance_pct'], 
            y_positions,
            xerr=top_importances['std_pct'], 
            fmt='none', 
            color='black', 
            capsize=3, 
            alpha=0.7
        )
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_yticks(y_positions)
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('Increase in MSE (%)', fontsize=12)
        ax.set_title('N2O Concentration Key Driving Factors\n(Permutation Importance Analysis)', fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # è®¾ç½®xè½´èŒƒå›´ï¼Œç•™äº›ç©ºé—´
        x_max = top_importances['importance_pct'].max() + top_importances['std_pct'].max()
        ax.set_xlim(0, x_max * 1.1)
        
        # å›¾ä¾‹
        unique_categories = sorted(top_importances['category'].unique())
        legend_elements = [
            plt.Rectangle((0,0), 1, 1, 
                         facecolor=category_colors.get(cat, '#D5D8DC'), 
                         label=cat, 
                         edgecolor='black', 
                         alpha=0.8) 
            for cat in unique_categories
        ]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=12,
                 title_fontsize=14)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "feature_importance_permutation_fixed.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"å›¾ç‰‡ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nå‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾:")
        print("-" * 70)
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:25s} {row['category']:15s} "
                  f"{row['importance_pct']:8.2f}% Â± {row['std_pct']:6.2f}%")
        
        return top_importances


    def plot_correlation_heatmap(self, X, y, importance_results=None):
        """ç»˜åˆ¶ç¯å¢ƒå› å­ä¸N2Oçš„ç›¸å…³ç³»æ•°çƒ­å›¾ï¼ŒæŒ‰é‡è¦æ€§æ’åº"""
        
        # å¦‚æœæ²¡æœ‰æä¾›é‡è¦æ€§ç»“æœï¼Œå…ˆè®¡ç®—
        if importance_results is None:
            print("å…ˆè®¡ç®—ç‰¹å¾é‡è¦æ€§...")
            importance_results = self.plot_permutation_importance(X, y, n_features=20)
        
        # è·å–æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾åˆ—è¡¨
        ordered_features = importance_results['feature'].tolist()
        
        # è®¡ç®—ç›¸å…³ç³»æ•°å’Œæ˜¾è‘—æ€§
        correlations = []
        p_values = []
        clean_names = []
        
        print("è®¡ç®—ç›¸å…³ç³»æ•°å’Œæ˜¾è‘—æ€§...")
        
        for feature in ordered_features:
            if feature in X.columns:
                # è®¡ç®—pearsonç›¸å…³ç³»æ•°
                corr, p_val = pearsonr(X[feature], y)
                correlations.append(corr)
                p_values.append(p_val)
                
                # è·å–æ¸…ç†åçš„ç‰¹å¾å
                clean_name = self.clean_feature_name(feature)
                clean_names.append(clean_name)
        
        # åˆ›å»ºæ•°æ®æ¡†
        corr_data = pd.DataFrame({
            'feature': clean_names,
            'correlation': correlations,
            'p_value': p_values
        })
        
        # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        def get_significance_mark(p_val):
            if p_val < 0.001:
                return '***'
            elif p_val < 0.01:
                return '**'
            elif p_val < 0.05:
                return '*'
            else:
                return ''
        
        corr_data['significance'] = corr_data['p_value'].apply(get_significance_mark)
        
        # åˆ›å»ºçƒ­å›¾æ•°æ®çŸ©é˜µï¼ˆåªæœ‰ä¸€åˆ—ï¼‰
        corr_matrix = corr_data[['correlation']].T
        corr_matrix.columns = corr_data['feature']
        
        # ç»˜åˆ¶çƒ­å›¾
        fig, ax = plt.subplots(figsize=(3, 12))
        
        # ä½¿ç”¨RdBu_ré…è‰²æ–¹æ¡ˆï¼ˆçº¢è‰²è¡¨ç¤ºæ­£ç›¸å…³ï¼Œè“è‰²è¡¨ç¤ºè´Ÿç›¸å…³ï¼‰
        sns.heatmap(corr_matrix, 
                    annot=False,  # ä¸æ˜¾ç¤ºæ•°å€¼ï¼Œæˆ‘ä»¬è¦è‡ªå®šä¹‰æ ‡æ³¨
                    cmap='RdBu_r', 
                    center=0,
                    vmin=-1, 
                    vmax=1,
                    cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8},
                    linewidths=0.5,
                    linecolor='white',
                    ax=ax)
        
        # æ·»åŠ ç›¸å…³ç³»æ•°æ•°å€¼å’Œæ˜¾è‘—æ€§æ ‡è®°
        for i, (corr, sig) in enumerate(zip(corr_data['correlation'], corr_data['significance'])):
            # æ ¹æ®ç›¸å…³ç³»æ•°çš„ç»å¯¹å€¼å†³å®šæ–‡å­—é¢œè‰²
            text_color = 'white' if abs(corr) > 0.5 else 'black'
            
            # æ·»åŠ ç›¸å…³ç³»æ•°å€¼
            ax.text(0.5, i + 0.3, f'{corr:.3f}', 
                    ha='center', va='center', 
                    fontsize=8, color=text_color, weight='bold')
            
            # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
            if sig:
                ax.text(0.5, i + 0.7, sig, 
                        ha='center', va='center', 
                        fontsize=10, color=text_color, weight='bold')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title('Correlation between Environmental Factors and N2O\n(Ordered by Feature Importance)', 
                    fontsize=12, pad=20)
        ax.set_xlabel('')
        ax.set_ylabel('')
        
        # è®¾ç½®yè½´æ ‡ç­¾
        ax.set_yticklabels(['N2O'], rotation=0, fontsize=10)
        
        # æ—‹è½¬xè½´æ ‡ç­¾
        plt.xticks(rotation=45, ha='right', fontsize=9)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "correlation_heatmap_with_significance.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"ç›¸å…³ç³»æ•°çƒ­å›¾ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç›¸å…³ç³»æ•°ç»Ÿè®¡
        print(f"\nç›¸å…³ç³»æ•°ç»Ÿè®¡:")
        print("-" * 80)
        print(f"{'ç‰¹å¾åç§°':<25} {'ç›¸å…³ç³»æ•°':<12} {'På€¼':<12} {'æ˜¾è‘—æ€§':<8}")
        print("-" * 80)
        
        for _, row in corr_data.iterrows():
            print(f"{row['feature']:<25} {row['correlation']:>8.4f}    {row['p_value']:>8.4e}   {row['significance']:>6s}")
        
        # ç»Ÿè®¡æ˜¾è‘—ç›¸å…³çš„ç‰¹å¾æ•°é‡
        significant_features = corr_data[corr_data['p_value'] < 0.05]
        print(f"\næ˜¾è‘—ç›¸å…³çš„ç‰¹å¾æ•°é‡ (p < 0.05): {len(significant_features)}/{len(corr_data)}")
        
        return corr_data

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - ç‰¹å¾é‡è¦æ€§åˆ†æä¸ç›¸å…³æ€§åˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    importance_results = predictor.plot_permutation_importance(X, y, n_features=20)
    
    # ç›¸å…³ç³»æ•°çƒ­å›¾åˆ†æ
    print("\n4. ç›¸å…³ç³»æ•°åˆ†æ...")
    correlation_results = predictor.plot_correlation_heatmap(X, y, importance_results)
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor, importance_results, correlation_results

if __name__ == "__main__":
    predictor, importance_results, correlation_results = main()


#%% åº”ç”¨register_cmapä¿®å¤ 

# æœ€ç®€å•ç›´æ¥çš„ä¿®å¤æ–¹æ¡ˆ
import matplotlib.cm as mpl_cm
import matplotlib as mpl

# ç›´æ¥æŒ‰ç…§ç½‘ä¸Šçš„ä¿®å¤æ–¹æ¡ˆè¿›è¡Œmonkey patch
if not hasattr(mpl_cm, 'register_cmap'):
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£å‡½æ•°
    def register_cmap(name, cmap):
        # ä½¿ç”¨ç°ä»£matplotlibçš„æ–¹å¼
        if hasattr(mpl, 'colormaps'):
            mpl.colormaps.register(cmap, name=name)
        else:
            # å¦‚æœä»¥ä¸Šéƒ½ä¸è¡Œï¼Œå°±å¿½ç•¥æ³¨å†Œï¼ˆå¾ˆå¤šæ—¶å€™ä¸å½±å“ä½¿ç”¨ï¼‰
            pass
    
    mpl_cm.register_cmap = register_cmap
    print("å·²åº”ç”¨register_cmapä¿®å¤")

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        # ç‰¹å¾ç±»åˆ«æ˜ å°„
        self.feature_categories = {
            'Elevation': 'Physiography', 'slp_dg_uav': 'Physiography',
            'Depth_avg': 'Hydrology', 'Vol_total': 'Hydrology', 'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology', 'Wshd_area': 'Hydrology', 'run_mm_vyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology', 'Res_time': 'Hydrology',
            'pre_mm_uyr': 'Climate', 'ice_days': 'Climate', 'ari_ix_lav': 'Climate',
            'Population_Density': 'Anthropogenic', 'hft_ix_v09': 'Anthropogenic', 'urb_pc_vse': 'Anthropogenic',
            'for_pc_vse': 'Landcover', 'crp_pc_vse': 'Landcover',
            'soc_th_vav': 'Soils & Geology', 'ero_kh_vav': 'Soils & Geology', 'gwt_cm_vav': 'Soils & Geology',
            'Chla_pred_RF': 'Water quality', 'TN_Load_Per_Volume': 'Water quality', 'TP_Load_Per_Volume': 'Water quality'
        }
        
        # åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        self.custom_colors = ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', 
                             '#FF9800', '#FB8C00', '#F57C00', '#EF6C00', '#E65100',
                             '#C2185B', '#7B1FA2', '#4A148C']
        
        # åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        self.custom_cmap = mcolors.LinearSegmentedColormap.from_list(
            'custom_orange_purple', self.custom_colors, N=256)
        
        self.model = None
        self.analysis_vars = None
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y
    
    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
    
    def clean_feature_name(self, feature_name):
        """æ¸…ç†ç‰¹å¾åç§°"""
        return feature_name.replace('Log1p_', '') if feature_name.startswith('Log1p_') else feature_name
    
    def get_category_colors(self):
        """è·å–ç±»åˆ«é¢œè‰²æ˜ å°„ï¼Œä½¿ç”¨è‡ªå®šä¹‰é¢œè‰²"""
        categories = list(set(self.feature_categories.values()))
        n_categories = len(categories)
        
        # ä»è‡ªå®šä¹‰é¢œè‰²åˆ—è¡¨ä¸­é€‰æ‹©é¢œè‰²
        if n_categories <= len(self.custom_colors):
            selected_colors = self.custom_colors[:n_categories]
        else:
            # å¦‚æœç±»åˆ«æ•°é‡è¶…è¿‡è‡ªå®šä¹‰é¢œè‰²æ•°é‡ï¼Œä½¿ç”¨colormapç”Ÿæˆæ›´å¤šé¢œè‰²
            selected_colors = [self.custom_cmap(i / (n_categories - 1)) for i in range(n_categories)]
        
        return dict(zip(categories, selected_colors))
    
    def plot_permutation_importance(self, X, y, n_features=20, n_repeats=10):
        """è®¡ç®—å¹¶ç»˜åˆ¶æ’åˆ—é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            
        print(f"è®¡ç®—æ’åˆ—é‡è¦æ€§ (é‡å¤{n_repeats}æ¬¡)...")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.model, X, y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # å°†é‡è¦æ€§è½¬æ¢ä¸ºç™¾åˆ†æ¯” (MSEå¢åŠ çš„ç™¾åˆ†æ¯”)
        importances['importance_pct'] = importances['importance'] * 100
        importances['std_pct'] = importances['std'] * 100
        
        # æ¸…ç†ç‰¹å¾åç§°å¹¶æ·»åŠ ç±»åˆ«
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        importances['category'] = importances['clean_feature'].map(
            lambda x: self.feature_categories.get(x, 'Other')
        )
        
        # é€‰æ‹©å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶æŒ‰é‡è¦æ€§é™åºæ’åˆ—
        top_importances = importances.nlargest(n_features, 'importance').reset_index(drop=True)
        
        # ä½¿ç”¨è‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        category_colors = self.get_category_colors()
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # åè½¬yè½´é¡ºåºï¼Œè®©æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨
        y_positions = range(len(top_importances))
        y_positions = [len(top_importances) - 1 - i for i in y_positions]
        
        bars = ax.barh(
            y_positions, 
            top_importances['importance_pct'],
            color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
            alpha=0.8,
            edgecolor='black',
            linewidth=0.5
        )
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(
            top_importances['importance_pct'], 
            y_positions,
            xerr=top_importances['std_pct'], 
            fmt='none', 
            color='black', 
            capsize=3, 
            alpha=0.7
        )
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_yticks(y_positions)
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('Increase in MSE (%)', fontsize=12)
        ax.set_title('N2O Concentration Key Driving Factors\n(Permutation Importance Analysis)', fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # è®¾ç½®xè½´èŒƒå›´ï¼Œç•™äº›ç©ºé—´
        x_max = top_importances['importance_pct'].max() + top_importances['std_pct'].max()
        ax.set_xlim(0, x_max * 1.1)
        
        # å›¾ä¾‹
        unique_categories = sorted(top_importances['category'].unique())
        legend_elements = [
            plt.Rectangle((0,0), 1, 1, 
                         facecolor=category_colors.get(cat, '#D5D8DC'), 
                         label=cat, 
                         edgecolor='black', 
                         alpha=0.8) 
            for cat in unique_categories
        ]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=9)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "feature_importance_permutation_fixed.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"å›¾ç‰‡ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nå‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾:")
        print("-" * 70)
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:25s} {row['category']:15s} "
                  f"{row['importance_pct']:8.2f}% Â± {row['std_pct']:6.2f}%")
        
        return top_importances

    def plot_correlation_heatmap(self, X, y, importance_results=None):
        """ç»˜åˆ¶ç¯å¢ƒå› å­ä¸N2Oçš„ç›¸å…³ç³»æ•°çƒ­å›¾ï¼ŒæŒ‰é‡è¦æ€§æ’åº"""
        
        # å¦‚æœæ²¡æœ‰æä¾›é‡è¦æ€§ç»“æœï¼Œå…ˆè®¡ç®—
        if importance_results is None:
            print("å…ˆè®¡ç®—ç‰¹å¾é‡è¦æ€§...")
            importance_results = self.plot_permutation_importance(X, y, n_features=20)
        
        # è·å–æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾åˆ—è¡¨
        ordered_features = importance_results['feature'].tolist()
        
        # è®¡ç®—ç›¸å…³ç³»æ•°å’Œæ˜¾è‘—æ€§
        correlations = []
        p_values = []
        clean_names = []
        
        print("è®¡ç®—ç›¸å…³ç³»æ•°å’Œæ˜¾è‘—æ€§...")
        
        for feature in ordered_features:
            if feature in X.columns:
                # è®¡ç®—pearsonç›¸å…³ç³»æ•°
                corr, p_val = pearsonr(X[feature], y)
                correlations.append(corr)
                p_values.append(p_val)
                
                # è·å–æ¸…ç†åçš„ç‰¹å¾å
                clean_name = self.clean_feature_name(feature)
                clean_names.append(clean_name)
        
        # åˆ›å»ºæ•°æ®æ¡†
        corr_data = pd.DataFrame({
            'feature': clean_names,
            'correlation': correlations,
            'p_value': p_values
        })
        
        # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        def get_significance_mark(p_val):
            if p_val < 0.001:
                return '***'
            elif p_val < 0.01:
                return '**'
            elif p_val < 0.05:
                return '*'
            else:
                return ''
        
        corr_data['significance'] = corr_data['p_value'].apply(get_significance_mark)
        
        # åˆ›å»ºçƒ­å›¾æ•°æ®çŸ©é˜µï¼ˆåªæœ‰ä¸€åˆ—ï¼‰
        corr_matrix = corr_data[['correlation']].T
        corr_matrix.columns = corr_data['feature']
        
        # ç»˜åˆ¶çƒ­å›¾
        fig, ax = plt.subplots(figsize=(3, 12))
        
        # ä½¿ç”¨è‡ªå®šä¹‰é¢œè‰²æ˜ å°„æˆ–RdBu_r
        try:
            # åˆ›å»ºä¸€ä¸ªä»¥0ä¸ºä¸­å¿ƒçš„è‡ªå®šä¹‰é¢œè‰²æ˜ å°„
            custom_diverging_colors = ['#4A148C', '#7B1FA2', '#C2185B', '#E65100', '#FFF3E0', 
                                     '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', '#FF9800']
            custom_diverging_cmap = mcolors.LinearSegmentedColormap.from_list(
                'custom_diverging', custom_diverging_colors, N=256)
            
            sns.heatmap(corr_matrix, 
                        annot=False,  # ä¸æ˜¾ç¤ºæ•°å€¼ï¼Œæˆ‘ä»¬è¦è‡ªå®šä¹‰æ ‡æ³¨
                        cmap=custom_diverging_cmap, 
                        center=0,
                        vmin=-1, 
                        vmax=1,
                        cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8},
                        linewidths=0.5,
                        linecolor='white',
                        ax=ax)
        except:
            # å¦‚æœè‡ªå®šä¹‰é¢œè‰²æ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„RdBu_r
            sns.heatmap(corr_matrix, 
                        annot=False,
                        cmap='RdBu_r', 
                        center=0,
                        vmin=-1, 
                        vmax=1,
                        cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8},
                        linewidths=0.5,
                        linecolor='white',
                        ax=ax)
        
        # æ·»åŠ ç›¸å…³ç³»æ•°æ•°å€¼å’Œæ˜¾è‘—æ€§æ ‡è®°
        for i, (corr, sig) in enumerate(zip(corr_data['correlation'], corr_data['significance'])):
            # æ ¹æ®ç›¸å…³ç³»æ•°çš„ç»å¯¹å€¼å†³å®šæ–‡å­—é¢œè‰²
            text_color = 'white' if abs(corr) > 0.5 else 'black'
            
            # æ·»åŠ ç›¸å…³ç³»æ•°å€¼
            ax.text(0.5, i + 0.3, f'{corr:.3f}', 
                    ha='center', va='center', 
                    fontsize=8, color=text_color, weight='bold')
            
            # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
            if sig:
                ax.text(0.5, i + 0.7, sig, 
                        ha='center', va='center', 
                        fontsize=10, color=text_color, weight='bold')
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title('Correlation between Environmental Factors and N2O\n(Ordered by Feature Importance)', 
                    fontsize=12, pad=20)
        ax.set_xlabel('')
        ax.set_ylabel('')
        
        # è®¾ç½®yè½´æ ‡ç­¾
        ax.set_yticklabels(['N2O'], rotation=0, fontsize=10)
        
        # æ—‹è½¬xè½´æ ‡ç­¾
        plt.xticks(rotation=45, ha='right', fontsize=9)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "correlation_heatmap_with_significance.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"ç›¸å…³ç³»æ•°çƒ­å›¾ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç›¸å…³ç³»æ•°ç»Ÿè®¡
        print(f"\nç›¸å…³ç³»æ•°ç»Ÿè®¡:")
        print("-" * 80)
        print(f"{'ç‰¹å¾åç§°':<25} {'ç›¸å…³ç³»æ•°':<12} {'På€¼':<12} {'æ˜¾è‘—æ€§':<8}")
        print("-" * 80)
        
        for _, row in corr_data.iterrows():
            print(f"{row['feature']:<25} {row['correlation']:>8.4f}    {row['p_value']:>8.4e}   {row['significance']:>6s}")
        
        # ç»Ÿè®¡æ˜¾è‘—ç›¸å…³çš„ç‰¹å¾æ•°é‡
        significant_features = corr_data[corr_data['p_value'] < 0.05]
        print(f"\næ˜¾è‘—ç›¸å…³çš„ç‰¹å¾æ•°é‡ (p < 0.05): {len(significant_features)}/{len(corr_data)}")
        
        return corr_data

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - ç‰¹å¾é‡è¦æ€§åˆ†æä¸ç›¸å…³æ€§åˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    importance_results = predictor.plot_permutation_importance(X, y, n_features=20)
    
    # ç›¸å…³ç³»æ•°çƒ­å›¾åˆ†æ
    print("\n4. ç›¸å…³ç³»æ•°åˆ†æ...")
    correlation_results = predictor.plot_correlation_heatmap(X, y, importance_results)
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor, importance_results, correlation_results

if __name__ == "__main__":
    predictor, importance_results, correlation_results = main()


#%% æ’åˆ—é‡è¦æ€§å’Œçƒ­å›¾ å‡ºå›¾ è§£å†³éšæœºæ£®æ—Xç¼ºå¤±å€¼çš„é—®é¢˜ 0815

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        # ç‰¹å¾ç±»åˆ«æ˜ å°„
        self.feature_categories = {
            'Elevation': 'Physiography', 'slp_dg_uav': 'Physiography',
            'Depth_avg': 'Hydrology', 'Vol_total': 'Hydrology', 'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology', 'Wshd_area': 'Hydrology', 'run_mm_vyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology', 'Res_time': 'Hydrology',
            'pre_mm_uyr': 'Climate', 'ice_days': 'Climate', 'ari_ix_lav': 'Climate',
            'Population_Density': 'Anthropogenic', 'hft_ix_v09': 'Anthropogenic', 'urb_pc_vse': 'Anthropogenic',
            'for_pc_vse': 'Landcover', 'crp_pc_vse': 'Landcover',
            'soc_th_vav': 'Soils & Geology', 'ero_kh_vav': 'Soils & Geology', 'gwt_cm_vav': 'Soils & Geology',
            'Chla_pred_RF': 'Water quality', 'TN_Load_Per_Volume': 'Water quality', 'TP_Load_Per_Volume': 'Water quality'
        }
        
        self.model = None
        self.analysis_vars = None
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_counts = X.isnull().sum()
        missing_vars = missing_counts[missing_counts > 0]
        if len(missing_vars) > 0:
            print("åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:")
            for var, count in missing_vars.items():
                print(f"  {var}: {count} ({count/len(X)*100:.1f}%)")
        else:
            print("  æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        before_drop = len(X)
        complete_cases = X.notna().all(axis=1) & y.notna()
        X = X[complete_cases]
        y = y[complete_cases]
        after_drop = len(X)
        
        if before_drop != after_drop:
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
        else:
            print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ•°æ®
        if len(X) == 0:
            raise ValueError("åˆ é™¤ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
    
    def clean_feature_name(self, feature_name):
        """æ¸…ç†ç‰¹å¾åç§°"""
        return feature_name.replace('Log1p_', '') if feature_name.startswith('Log1p_') else feature_name
    
    
    def plot_permutation_importance(self, X, y, n_features=20, n_repeats=10):
        """è®¡ç®—å¹¶ç»˜åˆ¶æ’åˆ—é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            
        print(f"è®¡ç®—æ’åˆ—é‡è¦æ€§ (é‡å¤{n_repeats}æ¬¡)...")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.model, X, y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # å°†é‡è¦æ€§è½¬æ¢ä¸ºç™¾åˆ†æ¯” (MSEå¢åŠ çš„ç™¾åˆ†æ¯”)
        importances['importance_pct'] = importances['importance'] * 100
        importances['std_pct'] = importances['std'] * 100
        
        # æ¸…ç†ç‰¹å¾åç§°å¹¶æ·»åŠ ç±»åˆ«
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        importances['category'] = importances['clean_feature'].map(
            lambda x: self.feature_categories.get(x, 'Other')
        )
        
        # é€‰æ‹©å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶æŒ‰é‡è¦æ€§é™åºæ’åˆ—
        top_importances = importances.nlargest(n_features, 'importance').reset_index(drop=True)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',
            'Hydrology': '#7FB3D5', 
            'Anthropogenic': '#F1948A',
            'Landcover': '#F4D03F',
            'Physiography': '#BFC9CA',
            'Soils & Geology': '#E59866',
            'Water quality': '#DDA0DD',
            'Other': '#D5D8DC'
        }
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(8, 10))
        
        # åè½¬yè½´é¡ºåºï¼Œè®©æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨
        y_positions = range(len(top_importances))
        y_positions = [len(top_importances) - 1 - i for i in y_positions]
        
        bars = ax.barh(
            y_positions, 
            top_importances['importance_pct'],
            color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
            alpha=0.8,
            edgecolor='black',
            linewidth=0.5
        )
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(
            top_importances['importance_pct'], 
            y_positions,
            xerr=top_importances['std_pct'], 
            fmt='none', 
            color='black', 
            capsize=3, 
            alpha=0.7
        )
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_yticks(y_positions)
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('Increase in MSE (%)', fontsize=12)
        ax.set_title('N2O flux Key Driving Factors\n(Permutation Importance Analysis)', fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # è®¾ç½®xè½´èŒƒå›´ï¼Œç•™äº›ç©ºé—´
        x_max = top_importances['importance_pct'].max() + top_importances['std_pct'].max()
        ax.set_xlim(0, x_max * 1.1)
        
        # å›¾ä¾‹
        unique_categories = sorted(top_importances['category'].unique())
        legend_elements = [
            plt.Rectangle((0,0), 1, 1, 
                         facecolor=category_colors.get(cat, '#D5D8DC'), 
                         label=cat, 
                         edgecolor='black', 
                         alpha=0.8) 
            for cat in unique_categories
        ]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=9)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "feature_importance_permutation_fixed3.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"å›¾ç‰‡ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nå‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾:")
        print("-" * 70)
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:25s} {row['category']:15s} "
                  f"{row['importance_pct']:8.2f}% Â± {row['std_pct']:6.2f}%")
        
        return top_importances

    # ä¿®æ”¹åçš„æ’åˆ—é‡è¦æ€§ç»˜å›¾å‡½æ•°ï¼ˆåœ¨SimpleN2OPredictorç±»ä¸­ï¼‰
    def plot_permutation_importance_modified(self, X, y, n_features=20, n_repeats=10):
        """ä¿®æ”¹ç‰ˆæ’åˆ—é‡è¦æ€§å›¾ï¼šä¸æ˜¾ç¤ºå·¦ä¾§çš„ç¯å¢ƒå› å­åç§°"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
            
        print(f"è®¡ç®—æ’åˆ—é‡è¦æ€§ (é‡å¤{n_repeats}æ¬¡)...")
        
        # è®¡ç®—æ’åˆ—é‡è¦æ€§
        r = permutation_importance(
            self.model, X, y, 
            n_repeats=n_repeats, 
            random_state=self.random_state,
            scoring='neg_mean_squared_error'
        )
        
        # åˆ›å»ºé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': r.importances_mean,
            'std': r.importances_std
        })
        
        # å°†é‡è¦æ€§è½¬æ¢ä¸ºç™¾åˆ†æ¯” (MSEå¢åŠ çš„ç™¾åˆ†æ¯”)
        importances['importance_pct'] = importances['importance'] * 100
        importances['std_pct'] = importances['std'] * 100
        
        # æ¸…ç†ç‰¹å¾åç§°å¹¶æ·»åŠ ç±»åˆ«
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        importances['category'] = importances['clean_feature'].map(
            lambda x: self.feature_categories.get(x, 'Other')
        )
        
        # é€‰æ‹©å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œå¹¶æŒ‰é‡è¦æ€§é™åºæ’åˆ—
        top_importances = importances.nlargest(n_features, 'importance').reset_index(drop=True)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',
            'Hydrology': '#7FB3D5', 
            'Anthropogenic': '#F1948A',
            'Landcover': '#F4D03F',
            'Physiography': '#BFC9CA',
            'Soils & Geology': '#E59866',
            'Water quality': '#DDA0DD',
            'Other': '#D5D8DC'
        }
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(8, 10))
        
        # åè½¬yè½´é¡ºåºï¼Œè®©æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨
        y_positions = range(len(top_importances))
        y_positions = [len(top_importances) - 1 - i for i in y_positions]
        
        bars = ax.barh(
            y_positions, 
            top_importances['importance_pct'],
            color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
            alpha=0.8,
            edgecolor='black',
            linewidth=0.5
        )
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(
            top_importances['importance_pct'], 
            y_positions,
            xerr=top_importances['std_pct'], 
            fmt='none', 
            color='black', 
            capsize=3, 
            alpha=0.7
        )
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_yticks(y_positions)
        
        # ä¿®æ”¹ï¼šä¸æ˜¾ç¤ºå·¦ä¾§çš„ç¯å¢ƒå› å­åç§°
        ax.set_yticklabels([])  # è®¾ç½®ä¸ºç©ºåˆ—è¡¨ï¼Œä¸æ˜¾ç¤ºyè½´æ ‡ç­¾
        # æˆ–è€…ä½ ä¹Ÿå¯ä»¥ç”¨è¿™ç§æ–¹å¼ï¼š
        # ax.set_yticklabels([''] * len(top_importances))  # è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
        
        ax.set_xlabel('Increase in MSE (%)', fontsize=12)
        # ax.set_title('N2O flux Key Driving Factors\n(Permutation Importance Analysis)', fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # è®¾ç½®xè½´èŒƒå›´ï¼Œç•™äº›ç©ºé—´
        x_max = top_importances['importance_pct'].max() + top_importances['std_pct'].max()
        ax.set_xlim(0, x_max * 1.1)
        
        # å›¾ä¾‹
        unique_categories = sorted(top_importances['category'].unique())
        legend_elements = [
            plt.Rectangle((0,0), 1, 1, 
                         facecolor=category_colors.get(cat, '#D5D8DC'), 
                         label=cat, 
                         edgecolor='black', 
                         alpha=0.8) 
            for cat in unique_categories
        ]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=12,
                 title_fontsize=14)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        filename = "feature_importance_permutation_modified.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"ä¿®æ”¹åçš„å›¾ç‰‡ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nå‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾:")
        print("-" * 70)
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:25s} {row['category']:15s} "
                  f"{row['importance_pct']:8.2f}% Â± {row['std_pct']:6.2f}%")
        
        return top_importances

# ä¿®æ”¹åçš„ç›¸å…³æ€§çƒ­å›¾å‡½æ•°
def plot_correlation_heatmap_by_importance_modified(X, y, importance_results=None, n_features=20, feature_categories=None):
    """
    ä¿®æ”¹ç‰ˆçš„ç›¸å…³ç³»æ•°çƒ­å›¾ï¼š
    1. å»æ‰xè½´N2Oæ ‡ç­¾
    2. colorbaræ”¾åˆ°åº•éƒ¨
    3. å¯é€‰æ‹©æ˜¯å¦æ˜¾ç¤ºçƒ­å›¾å†…çš„æ•°å€¼
    """
    
    # å¦‚æœæ²¡æœ‰æä¾›é‡è¦æ€§ç»“æœï¼Œéœ€è¦å…ˆè®¡ç®—æˆ–æ‰‹åŠ¨æ’åº
    if importance_results is None:
        print("è­¦å‘Š: æœªæä¾›ç‰¹å¾é‡è¦æ€§ç»“æœï¼Œå°†æŒ‰ç…§ç‰¹å¾åœ¨æ•°æ®ä¸­çš„é¡ºåºæ˜¾ç¤º")
        ordered_features = X.columns.tolist()[:n_features]
    else:
        # è·å–æŒ‰é‡è¦æ€§æ’åºçš„å‰n_featuresä¸ªç‰¹å¾
        ordered_features = importance_results['feature'].head(n_features).tolist()
    
    # è®¡ç®—ç›¸å…³ç³»æ•°å’Œæ˜¾è‘—æ€§
    correlations = []
    p_values = []
    feature_names = []
    
    print(f"è®¡ç®—å‰{len(ordered_features)}ä¸ªé‡è¦ç‰¹å¾ä¸N2Oçš„ç›¸å…³ç³»æ•°...")
    
    for feature in ordered_features:
        if feature in X.columns:
            # è®¡ç®—pearsonç›¸å…³ç³»æ•°
            corr, p_val = pearsonr(X[feature], y)
            correlations.append(corr)
            p_values.append(p_val)
            
            # æ¸…ç†ç‰¹å¾åç§°ï¼ˆç§»é™¤Log1p_å‰ç¼€ï¼‰
            clean_name = feature.replace('Log1p_', '') if feature.startswith('Log1p_') else feature
            feature_names.append(clean_name)
    
    # åˆ›å»ºç›¸å…³æ€§æ•°æ®æ¡†
    corr_data = pd.DataFrame({
        'feature': feature_names,
        'original_feature': ordered_features[:len(feature_names)],
        'correlation': correlations,
        'p_value': p_values
    })
    
    # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
    def get_significance_mark(p_val):
        if p_val < 0.001:
            return '***'
        elif p_val < 0.01:
            return '**'
        elif p_val < 0.05:
            return '*'
        else:
            return ''
    
    corr_data['significance'] = corr_data['p_value'].apply(get_significance_mark)
    
    # æŒ‰é‡è¦æ€§é¡ºåºæ’åˆ—ï¼ˆä¿æŒåŸé¡ºåºï¼‰
    corr_data = corr_data.reset_index(drop=True)
    
    # åˆ›å»ºç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆçºµå‘å¸ƒå±€ï¼Œç‰¹å¾åœ¨yè½´ï¼ŒN2Oåœ¨xè½´ï¼‰
    corr_matrix = pd.DataFrame(
        corr_data['correlation'].values.reshape(-1, 1), 
        index=corr_data['feature'],
        columns=['N2O']
    )
    
    # è®¾ç½®å›¾å½¢å¤§å°ï¼ˆçºµå‘å¸ƒå±€ï¼‰
    fig, ax = plt.subplots(figsize=(4, max(8, len(feature_names) * 0.6)))
    
    # ç»˜åˆ¶çƒ­å›¾ - ä½¿ç”¨æ–¹æ³•4çš„æ”¹è¿›å‚æ•°
    heatmap = sns.heatmap(corr_matrix, 
                annot=False,  
                cmap='RdBu_r',  
                center=0,
                vmin=-1, 
                vmax=1,
                cbar_kws={
                    'label': 'Pearson Correlation Coefficient',
                    'orientation': 'horizontal',
                    'pad': 0.01,     # å¢åŠ ä¸å›¾çš„è·ç¦»
                    'aspect': 15,    # å¢åŠ é•¿å®½æ¯”ä½¿colorbaræ›´ç»†é•¿
                    'shrink': 1.5,   # ç¼©å°colorbar
                },
                linewidths=0.5,
                linecolor='white',
                square=False,  
                ax=ax)
    
    # å¯é€‰ï¼šè¿›ä¸€æ­¥å¾®è°ƒcolorbarä½ç½®
    cbar = heatmap.collections[0].colorbar
    cbar_pos = cbar.ax.get_position()
    
    new_pos = [
        cbar_pos.x0  - 0.3,    # æ°´å¹³ä½ç½®
        cbar_pos.y0,         # ä¸Šä¸‹ä½ç½®
        cbar_pos.width,     # å®½åº¦
        cbar_pos.height     # é«˜åº¦
    ]
    cbar.ax.set_position(new_pos)

    
    # æ‰‹åŠ¨æ·»åŠ ç›¸å…³ç³»æ•°æ•°å€¼å’Œæ˜¾è‘—æ€§æ ‡è®°ï¼ˆåœ¨åŒä¸€ä¸ªæ¡†å†…ï¼‰
    for i, (corr, sig) in enumerate(zip(corr_data['correlation'], corr_data['significance'])):
        # æ ¹æ®ç›¸å…³ç³»æ•°çš„ç»å¯¹å€¼å†³å®šæ–‡å­—é¢œè‰²ï¼ˆæ·±è‰²èƒŒæ™¯ç”¨ç™½è‰²æ–‡å­—ï¼Œæµ…è‰²èƒŒæ™¯ç”¨é»‘è‰²æ–‡å­—ï¼‰
        text_color = 'white' if abs(corr) > 0.5 else 'black'
        
        # åœ¨æ¡†å†…æ˜¾ç¤ºç›¸å…³ç³»æ•°å€¼å’Œæ˜¾è‘—æ€§æ ‡è®°
        if sig:
            # å¦‚æœæœ‰æ˜¾è‘—æ€§æ ‡è®°ï¼Œåˆ†ä¸¤è¡Œæ˜¾ç¤º
            ax.text(0.5, i + 0.35, f'{corr:.3f}', 
                    ha='center', va='center', 
                    fontsize=9, color=text_color, weight='bold')
            ax.text(0.5, i + 0.65, sig, 
                    ha='center', va='center', 
                    fontsize=11, color=text_color, weight='bold')
        else:
            # å¦‚æœæ²¡æœ‰æ˜¾è‘—æ€§æ ‡è®°ï¼Œå±…ä¸­æ˜¾ç¤ºç›¸å…³ç³»æ•°
            ax.text(0.5, i + 0.5, f'{corr:.3f}', 
                    ha='center', va='center', 
                    fontsize=10, color=text_color, weight='bold')
    
   
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    # ax.set_title('Environmental Factors vs N2O Correlation\n(Ordered by Feature Importance)', 
    #             fontsize=14, pad=20, weight='bold')
    ax.set_xlabel('', fontsize=12)  # å»æ‰xè½´æ ‡ç­¾
    # ax.set_ylabel('Environmental Factors', fontsize=12, weight='bold')
    
    # ä¿®æ”¹ï¼šå»æ‰xè½´çš„N2Oæ ‡ç­¾
    ax.set_xticklabels([])  # è®¾ç½®ä¸ºç©ºåˆ—è¡¨
    
    # è®¾ç½®yè½´æ ‡ç­¾ï¼ˆç‰¹å¾åç§°ï¼‰
    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=0)
    
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    filename = "correlation_heatmap_modified2.png"
    try:
        plt.savefig(filename, dpi=600, bbox_inches='tight', facecolor='white')
        print(f"ä¿®æ”¹åçš„ç›¸å…³ç³»æ•°çƒ­å›¾å·²ä¿å­˜è‡³: {filename}")
    except Exception as e:
        print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
    
    plt.show()
    
    return corr_data




def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - ç‰¹å¾é‡è¦æ€§åˆ†æä¸ç›¸å…³æ€§åˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # æ’åˆ—é‡è¦æ€§åˆ†æï¼ˆä¸æ˜¾ç¤ºyè½´æ ‡ç­¾ï¼‰
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    importance_results = predictor.plot_permutation_importance_modified(X, y, n_features=20)
    
    # ç›¸å…³ç³»æ•°çƒ­å›¾åˆ†æ
    print("\n4. ç›¸å…³ç³»æ•°åˆ†æ...")
    # ç›¸å…³æ€§çƒ­å›¾ï¼ˆcolorbaråœ¨åº•éƒ¨ï¼Œæ— xè½´æ ‡ç­¾ï¼Œæ— å†…éƒ¨æ•°å€¼ï¼‰
    correlation_results = plot_correlation_heatmap_by_importance_modified(
        X, y, 
        importance_results=importance_results, 
        n_features=20
    )
        
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor, importance_results, correlation_results

if __name__ == "__main__":
    predictor, importance_results, correlation_results = main()



#%% LIMEå±€éƒ¨å¯è§£é‡Šæ€§åˆ†æ 251017


import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings('ignore')

# LIMEç›¸å…³åº“
from lime import lime_tabular
import joblib

# åœ°å›¾ç›¸å…³åº“
import cartopy.crs as ccrs
import cartopy.feature as cfeature

# è®¾ç½®å­—ä½“
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class N2OPredictor_LIME:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        # ç‰¹å¾ç±»åˆ«æ˜ å°„
        self.feature_categories = {
            'Elevation': 'Physiography', 'slp_dg_uav': 'Physiography',
            'Depth_avg': 'Hydrology', 'Vol_total': 'Hydrology', 'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology', 'Wshd_area': 'Hydrology', 'run_mm_vyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology', 'Res_time': 'Hydrology',
            'pre_mm_uyr': 'Climate', 'ice_days': 'Climate', 'ari_ix_lav': 'Climate',
            'Population_Density': 'Anthropogenic', 'hft_ix_v09': 'Anthropogenic', 'urb_pc_vse': 'Anthropogenic',
            'for_pc_vse': 'Landcover', 'crp_pc_vse': 'Landcover',
            'soc_th_vav': 'Soils & Geology', 'ero_kh_vav': 'Soils & Geology', 'gwt_cm_vav': 'Soils & Geology',
            'Chla_pred_RF': 'Water quality', 'TN_Load_Per_Volume': 'Water quality', 'TP_Load_Per_Volume': 'Water quality'
        }
        
        self.model = None
        self.analysis_vars = None
        self.X_train = None
        self.lime_explainer = None
        
    def load_and_preprocess_data(self, filepath):
        """æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤åæ•°æ®é‡: {len(data_filtered)}")
        
        # ä¿å­˜åŸå§‹åœ°ç†ä¿¡æ¯ï¼ˆç”¨äºLIMEç©ºé—´åˆ†æï¼‰
        if 'lat' in data_filtered.columns and 'lon' in data_filtered.columns:
            geo_info = data_filtered[['lat', 'lon']].copy()
        else:
            geo_info = None
            print("è­¦å‘Š: æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°ç»çº¬åº¦ä¿¡æ¯")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']

        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_counts = X.isnull().sum()
        missing_vars = missing_counts[missing_counts > 0]
        if len(missing_vars) > 0:
            print("åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:")
            for var, count in missing_vars.items():
                print(f"  {var}: {count} ({count/len(X)*100:.1f}%)")
        else:
            print("  æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        before_drop = len(X)
        complete_cases = X.notna().all(axis=1) & y.notna()
        X = X[complete_cases]
        y = y[complete_cases]
        after_drop = len(X)
        
        if before_drop != after_drop:
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
        else:
            print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼,æœ€ç»ˆæ•°æ®é‡: {after_drop}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ•°æ®
        if len(X) == 0:
            raise ValueError("åˆ é™¤ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®!è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y, geo_info
    
    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        self.X_train = X  # ä¿å­˜è®­ç»ƒæ•°æ®ç”¨äºLIME
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
    
    def clean_feature_name(self, feature_name):
        """æ¸…ç†ç‰¹å¾åç§°"""
        return feature_name.replace('Log1p_', '') if feature_name.startswith('Log1p_') else feature_name
    
    def save_model(self, filepath='N2O_RF_model.joblib'):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'analysis_vars': self.analysis_vars,
            'feature_categories': self.feature_categories
        }
        
        joblib.dump(model_data, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
     
    def save_lime_results(self, lime_df, filepath='LIME_results.csv'):
        """
        ä¿å­˜LIMEåˆ†æç»“æœåˆ°CSVæ–‡ä»¶
        
        å‚æ•°:
        - lime_df: LIMEç»“æœDataFrame
        - filepath: ä¿å­˜è·¯å¾„
        """
        if lime_df is None or len(lime_df) == 0:
            print("è­¦å‘Š: LIMEç»“æœä¸ºç©º,æ— æ³•ä¿å­˜")
            return
        
        try:
            lime_df.to_csv(filepath, index=False)
            print(f"LIMEç»“æœå·²ä¿å­˜è‡³: {filepath}")
            print(f"  - æ ·æœ¬æ•°: {len(lime_df)}")
            print(f"  - åˆ—æ•°: {len(lime_df.columns)}")
        except Exception as e:
            print(f"ä¿å­˜LIMEç»“æœå¤±è´¥: {e}")
         
    
    def load_model(self, filepath='N2O_RF_model.joblib'):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.analysis_vars = model_data['analysis_vars']
        self.feature_categories = model_data['feature_categories']
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")
    
    def perform_lime_analysis(self, X, y, n_samples=None, num_features=10, num_samples_lime=5000):
        """
        å¯¹æ•°æ®é›†è¿›è¡ŒLIMEåˆ†æ
        
        å‚æ•°:
        - X: ç‰¹å¾æ•°æ® (DataFrame)
        - y: ç›®æ ‡å˜é‡
        - n_samples: è¦åˆ†æçš„æ¹–æ³Šæ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)
        - num_features: æ¯ä¸ªæ ·æœ¬æå–çš„topç‰¹å¾æ•°é‡
        - num_samples_lime: LIMEé‡‡æ ·æ¬¡æ•°
        """
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        if self.X_train is None:
            raise ValueError("éœ€è¦è®­ç»ƒæ•°æ®æ¥åˆ›å»ºLIMEè§£é‡Šå™¨!")
        
        # ç¡®å®šåˆ†ææ ·æœ¬æ•°
        if n_samples is None or n_samples > len(X):
            n_samples = len(X)
        
        print(f"\nå¼€å§‹LIMEåˆ†æ (å…±{n_samples}ä¸ªæ¹–æ³Š)...")
        print(f"æ¯ä¸ªæ¹–æ³Šåˆ†æå‰{num_features}ä¸ªç‰¹å¾,é‡‡æ ·{num_samples_lime}æ¬¡")
        
        # **å…³é”®ä¿®å¤:æ£€æŸ¥å¹¶ç§»é™¤æ ‡å‡†å·®ä¸º0çš„ç‰¹å¾**
        feature_stds = self.X_train.std()
        valid_features = feature_stds[feature_stds > 1e-10].index.tolist()
        invalid_features = feature_stds[feature_stds <= 1e-10].index.tolist()
        
        if invalid_features:
            print(f"\nè­¦å‘Š: ä»¥ä¸‹ç‰¹å¾æ ‡å‡†å·®ä¸º0,å°†è¢«ç§»é™¤:")
            for feat in invalid_features:
                clean_name = self.clean_feature_name(feat)
                print(f"  - {clean_name}")
            
            # è¿‡æ»¤è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
            X_train_filtered = self.X_train[valid_features]
            X_filtered = X[valid_features]
            feature_names_clean = [self.clean_feature_name(f) for f in valid_features]
        else:
            X_train_filtered = self.X_train
            X_filtered = X
            feature_names_clean = [self.clean_feature_name(f) for f in self.analysis_vars]
        
        print(f"\nä½¿ç”¨ {len(valid_features)} ä¸ªæœ‰æ•ˆç‰¹å¾è¿›è¡ŒLIMEåˆ†æ")
        
        # åˆ›å»ºLIMEè§£é‡Šå™¨
        try:
            self.lime_explainer = lime_tabular.LimeTabularExplainer(
                X_train_filtered.values,
                feature_names=feature_names_clean,
                mode='regression',
                random_state=self.random_state,
                discretize_continuous=False
            )
        except Exception as e:
            print(f"åˆ›å»ºLIMEè§£é‡Šå™¨å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨ç¦»æ•£åŒ–æ¨¡å¼...")
            self.lime_explainer = lime_tabular.LimeTabularExplainer(
                X_train_filtered.values,
                feature_names=feature_names_clean,
                mode='regression',
                random_state=self.random_state,
                discretize_continuous=True
            )
        
        # éšæœºé‡‡æ ·
        sample_indices = np.random.choice(X_filtered.index, size=n_samples, replace=False)
        X_sample = X_filtered.loc[sample_indices]
        
        # å­˜å‚¨LIMEç»“æœ
        lime_results = []
        failed_samples = 0
        
        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒLIMEè§£é‡Š
        for i, idx in enumerate(sample_indices):
            if (i + 1) % 500 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i+1}/{n_samples} (å¤±è´¥: {failed_samples})")
            
            try:
                # è·å–æ ·æœ¬
                instance = X_sample.loc[idx].values
                
                # ç”ŸæˆLIMEè§£é‡Š
                exp = self.lime_explainer.explain_instance(
                    instance,
                    lambda x: self.model.predict(
                        pd.DataFrame(x, columns=X_train_filtered.columns)
                    ),
                    num_features=num_features,
                    num_samples=num_samples_lime
                )
                
                # æå–topç‰¹å¾
                top_features = exp.as_list()[:num_features]
                
                # è§£æç‰¹å¾åå’Œæƒé‡
                feature_data = {'sample_idx': idx}
                for j, (feat_str, weight) in enumerate(top_features, 1):
                    # æå–ç‰¹å¾å(å»æ‰æ¯”è¾ƒç¬¦å·)
                    feat_name = feat_str.split()[0]
                    feature_data[f'var{j}'] = feat_name
                    feature_data[f'weight{j}'] = weight
                
                lime_results.append(feature_data)
                
            except Exception as e:
                failed_samples += 1
                if failed_samples <= 5:
                    print(f"    æ ·æœ¬ {idx} å¤±è´¥: {str(e)[:50]}")
        
        # è½¬æ¢ä¸ºDataFrame
        lime_df = pd.DataFrame(lime_results)
        
        print(f"\nLIMEåˆ†æå®Œæˆ!")
        print(f"æˆåŠŸåˆ†æ: {len(lime_df)}/{n_samples} ä¸ªæ ·æœ¬")
        print(f"å¤±è´¥æ ·æœ¬: {failed_samples}")
        
        if len(lime_df) == 0:
            raise ValueError("æ‰€æœ‰æ ·æœ¬éƒ½å¤±è´¥äº†!è¯·æ£€æŸ¥æ•°æ®å’Œæ¨¡å‹")
        
        return lime_df

        
    def plot_lime_histogram(self, lime_df, top_n=3, save_path='LIME_histogram.png'):
        """
        ç»˜åˆ¶LIMEç»“æœçš„ç›´æ–¹å›¾ - ä¿®æ”¹ç‰ˆ
        ç»Ÿè®¡å‰Nä¸ªä¸»å¯¼å› ç´ çš„ç‰¹å¾ç±»åˆ«é¢‘ç‡
        
        å‚æ•°:
        - lime_df: LIMEåˆ†æç»“æœ
        - top_n: ç»Ÿè®¡å‰å‡ ä¸ªä¸»å¯¼å› ç´  (é»˜è®¤3)
        - save_path: ä¿å­˜è·¯å¾„
        """
        print(f"\nç»˜åˆ¶LIMEç‰¹å¾é¢‘ç‡ç›´æ–¹å›¾ (å‰{top_n}ä¸ªä¸»å¯¼å› ç´ )...")
        
        # å‡†å¤‡æ•°æ®ç»“æ„
        rank_names = [f'Rank {i+1}' for i in range(top_n)]
        category_colors = {
            'Climate': '#98D8A0',
            'Hydrology': '#7FB3D5',
            'Anthropogenic': '#F1948A',
            'Landcover': '#F4D03F',
            'Physiography': '#BFC9CA',
            'Soils & Geology': '#E59866',
            'Water quality': '#DDA0DD',
            'Other': '#D5D8DC'
        }
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        rank_stats = {}  # {rank: {category: count}}
        rank_feature_stats = {}  # {rank: {feature: count}}  # æ–°å¢ï¼šç‰¹å¾å˜é‡ç»Ÿè®¡
        
        for rank in range(1, top_n + 1):
            var_col = f'var{rank}'
            if var_col not in lime_df.columns:
                print(f"è­¦å‘Š: åˆ— {var_col} ä¸å­˜åœ¨")
                continue
            
            category_count = {}
            feature_count = {}  # æ–°å¢ï¼šç‰¹å¾è®¡æ•°
            
            for var in lime_df[var_col]:
                if pd.notna(var):
                    # ç»Ÿè®¡ç±»åˆ«
                    category = self.feature_categories.get(var, 'Other')
                    category_count[category] = category_count.get(category, 0) + 1
                    
                    # ç»Ÿè®¡ç‰¹å¾å˜é‡
                    feature_count[var] = feature_count.get(var, 0) + 1
            
            rank_stats[rank] = category_count
            rank_feature_stats[rank] = feature_count
        
        # ========== æ‰“å°è¯¦ç»†ç»Ÿè®¡ ==========
        total_samples = len(lime_df)
        
        # 1. æ‰“å°ç±»åˆ«é¢‘ç‡ç»Ÿè®¡
        print("\n" + "="*70)
        print("ç‰¹å¾ç±»åˆ«é¢‘ç‡ç»Ÿè®¡ (æŒ‰ä¸»å¯¼å› ç´ æ’å)")
        print("="*70)
        
        for rank in range(1, top_n + 1):
            print(f"\nã€ç¬¬ {rank} ä¸»å¯¼å› ç´ ã€‘")
            print("-" * 70)
            
            if rank not in rank_stats:
                print("  (æ— æ•°æ®)")
                continue
            
            category_count = rank_stats[rank]
            # æŒ‰é¢‘ç‡é™åºæ’åº
            sorted_categories = sorted(category_count.items(), 
                                      key=lambda x: x[1], reverse=True)
            
            for category, count in sorted_categories:
                percentage = count / total_samples * 100
                print(f"  {category:20s}: {percentage:6.2f}% ({count:4d} / {total_samples})")
        
        print("="*70)
        
        # 2. æ‰“å°ç‰¹å¾å˜é‡é¢‘ç‡ç»Ÿè®¡ (æ–°å¢éƒ¨åˆ†)
        print("\n" + "="*70)
        print("ç‰¹å¾å˜é‡é¢‘ç‡ç»Ÿè®¡ (æŒ‰ä¸»å¯¼å› ç´ æ’å)")
        print("="*70)
        
        for rank in range(1, top_n + 1):
            print(f"\nã€ç¬¬ {rank} ä¸»å¯¼å› ç´ ã€‘")
            print("-" * 70)
            
            if rank not in rank_feature_stats:
                print("  (æ— æ•°æ®)")
                continue
            
            feature_count = rank_feature_stats[rank]
            # æŒ‰é¢‘ç‡é™åºæ’åº
            sorted_features = sorted(feature_count.items(), 
                                    key=lambda x: x[1], reverse=True)
            
            # æ‰“å°æ‰€æœ‰ç‰¹å¾åŠå…¶é¢‘ç‡
            for i, (feature, count) in enumerate(sorted_features, 1):
                percentage = count / total_samples * 100
                category = self.feature_categories.get(feature, 'Other')
                print(f"  {i:2d}. {feature:25s} [{category:20s}]: {percentage:6.2f}% ({count:4d} / {total_samples})")
        
        print("="*70)
        
        # 3. æ‰“å°è·¨æ’åçš„ç‰¹å¾ç»Ÿè®¡ (æ–°å¢éƒ¨åˆ†)
        print("\n" + "="*70)
        print(f"ç‰¹å¾å˜é‡ç»¼åˆç»Ÿè®¡ (å‰ {top_n} ä¸ªä¸»å¯¼å› ç´ )")
        print("="*70)
        
        # åˆå¹¶æ‰€æœ‰æ’åçš„ç‰¹å¾ç»Ÿè®¡
        all_features_count = {}
        for rank in rank_feature_stats.values():
            for feature, count in rank.items():
                all_features_count[feature] = all_features_count.get(feature, 0) + count
        
        # æŒ‰æ€»é¢‘ç‡é™åºæ’åº
        sorted_all_features = sorted(all_features_count.items(), 
                                    key=lambda x: x[1], reverse=True)
        
        print(f"\næ€»è®¡å‡ºç°çš„ä¸åŒç‰¹å¾æ•°: {len(sorted_all_features)}")
        print("\nç‰¹å¾å‡ºç°é¢‘ç‡æ’å (è·¨æ‰€æœ‰æ’å):")
        print("-" * 70)
        
        for i, (feature, count) in enumerate(sorted_all_features, 1):
            percentage = count / (total_samples * top_n) * 100
            category = self.feature_categories.get(feature, 'Other')
            
            # ç»Ÿè®¡è¯¥ç‰¹å¾åœ¨å“ªäº›æ’åä¸­å‡ºç°
            ranks_appeared = []
            for rank in range(1, top_n + 1):
                if rank in rank_feature_stats and feature in rank_feature_stats[rank]:
                    rank_count = rank_feature_stats[rank][feature]
                    ranks_appeared.append(f"Rank{rank}:{rank_count}")
            
            ranks_str = ", ".join(ranks_appeared)
            print(f"  {i:2d}. {feature:25s} [{category:20s}]: {percentage:6.2f}% ({count:4d}) - [{ranks_str}]")
        
        print("="*70)
        
        # ========== ç»˜åˆ¶åˆ†ç»„æŸ±çŠ¶å›¾ ==========
        categories = list(category_colors.keys())
        n_categories = len(categories)
        n_ranks = len(rank_stats)
        
        fig, ax = plt.subplots(figsize=(14, 7))
        
        # è®¾ç½®æŸ±å­å®½åº¦å’Œä½ç½®
        bar_width = 0.25
        x_pos = np.arange(n_categories)
        
        # ä¸ºæ¯ä¸ªæ’åç»˜åˆ¶æŸ±å­
        for i, rank in enumerate(sorted(rank_stats.keys())):
            frequencies = []
            for category in categories:
                count = rank_stats[rank].get(category, 0)
                freq = count / total_samples
                frequencies.append(freq)
            
            offset = (i - (n_ranks - 1) / 2) * bar_width
            bars = ax.bar(x_pos + offset, frequencies, bar_width, 
                         label=f'Rank {rank}',
                         alpha=0.8, edgecolor='black', linewidth=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, freq in zip(bars, frequencies):
                if freq > 0.01:  # åªæ˜¾ç¤º>1%çš„æ ‡ç­¾
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{freq:.1%}',
                           ha='center', va='bottom', fontsize=8, rotation=0)
        
        ax.set_xticks(x_pos)
        ax.set_xticklabels(categories, rotation=45, ha='right', fontsize=11)
        ax.set_ylabel('Percentage', fontsize=12)
        ax.set_title(f'Feature Category Distribution by Importance Rank (Top {top_n})\n(LIME Analysis)', 
                    fontsize=14, pad=20)
        ax.legend(title='Importance Rank', fontsize=10, loc='upper right')
        ax.set_ylim(0, max([max(rank_stats[rank].values()) / total_samples 
                           for rank in rank_stats]) * 1.15)
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nç›´æ–¹å›¾å·²ä¿å­˜è‡³: {save_path}")
        plt.show()
        
        # è¿”å›ç»Ÿè®¡ç»“æœ
        return {
            'category_stats': rank_stats,
            'feature_stats': rank_feature_stats,
            'overall_feature_stats': all_features_count
        }

        
    def plot_lime_spatial(self, lime_df, geo_info, save_path='LIME_spatial_map.png'):
        """
        ç»˜åˆ¶LIMEç»“æœçš„ç©ºé—´åˆ†å¸ƒå›¾ - ä¸‰ä¸ªå­å›¾ç‰ˆæœ¬
        åˆ†åˆ«å±•ç¤ºç¬¬ä¸€ã€ç¬¬äºŒã€ç¬¬ä¸‰ä¸»å¯¼å› ç´ çš„ç©ºé—´åˆ†å¸ƒ
        æ¯ä¸ªç‚¹çš„é¢œè‰²ä»£è¡¨å…·ä½“çš„ç‰¹å¾å˜é‡
        
        å‚æ•°:
        - lime_df: LIMEåˆ†æç»“æœ
        - geo_info: åœ°ç†ä¿¡æ¯ (DataFrame with lat, lon)
        - save_path: ä¿å­˜è·¯å¾„
        """
        if geo_info is None:
            print("è­¦å‘Š: æ²¡æœ‰åœ°ç†ä¿¡æ¯,æ— æ³•ç»˜åˆ¶ç©ºé—´å›¾")
            return
        
        print("\nç»˜åˆ¶LIMEç©ºé—´åˆ†å¸ƒå›¾ (å‰ä¸‰ä¸ªä¸»å¯¼å› ç´ )...")
        
        # åˆå¹¶åœ°ç†ä¿¡æ¯
        lime_spatial = lime_df.copy()
        lime_spatial = lime_spatial.join(geo_info, on='sample_idx')
        
        # ç§»é™¤ç¼ºå¤±åœ°ç†ä¿¡æ¯çš„æ ·æœ¬
        lime_spatial = lime_spatial.dropna(subset=['lat', 'lon'])
        
        if len(lime_spatial) == 0:
            print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„åœ°ç†åæ ‡æ•°æ®")
            return
        
        # é¢„å®šä¹‰é¢œè‰² - æŒ‡å®šç‰¹å®šå˜é‡çš„é¢œè‰²
        specified_colors = {
            'Lake_area': '#F8B88B',      # æµ…æ©™
            'crp_pc_vse': '#DDA0DD',     # æ·¡ç´«è‰²
            'ari_ix_lav': '#F4D03F'      # é»„è‰²
        }
        
        # å…¶ä»–é¢œè‰²æ± 
        other_colors = [
            '#98D8A0',  # ç»¿è‰²
            '#7FB3D5',  # è“è‰²
            '#F1948A',  # çº¢è‰²
            '#BFC9CA',  # ç°è‰²
            '#E59866',  # æ£•è‰²
            '#AED6F1',  # æµ…è“
            '#C39BD3',  # æµ…ç´«
            '#82E0AA',  # æµ…ç»¿
            '#F7DC6F',  # æµ…é‡‘
            '#D7DBDD',  # é“¶ç°
            '#FAD7A0',  # æµ…æ¡ƒ
            '#ABEBC6',  # è–„è·ç»¿
            '#F5B7B1',  # æµ…ç²‰
            '#D2B4DE',  # è–°è¡£è‰
            '#A9CCE3',  # å¤©è“
            '#A3E4D7'   # æ°´ç»¿
        ]
        
        # åˆ›å»ºä¸‰ä¸ªå­å›¾
        fig = plt.figure(figsize=(20, 18))
        projection = ccrs.Robinson(central_longitude=0)
        
        # å­å›¾æ ‡é¢˜ - ç§»é™¤å›¾æ³¨
        var_columns = ['var1', 'var2', 'var3']
        
        for idx, var_col in enumerate(var_columns, 1):
            ax = fig.add_subplot(3, 1, idx, projection=projection)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¯¥åˆ—
            if var_col not in lime_spatial.columns:
                print(f"è­¦å‘Š: åˆ— {var_col} ä¸å­˜åœ¨,è·³è¿‡")
                continue
            
            # è·å–è¯¥åˆ—ä¸­æ‰€æœ‰å”¯ä¸€çš„ç‰¹å¾å˜é‡
            unique_features = lime_spatial[var_col].dropna().unique()
            unique_features = sorted(unique_features)  # æ’åºä¿è¯ä¸€è‡´æ€§
            
            # ä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…é¢œè‰²
            feature_colors = {}
            other_color_idx = 0
            
            for feature in unique_features:
                if feature in specified_colors:
                    # ä½¿ç”¨æŒ‡å®šé¢œè‰²
                    feature_colors[feature] = specified_colors[feature]
                else:
                    # ä½¿ç”¨å…¶ä»–é¢œè‰²æ± 
                    feature_colors[feature] = other_colors[other_color_idx % len(other_colors)]
                    other_color_idx += 1
            
            # æ·»åŠ åœ°å›¾ç‰¹å¾
            ax.set_global()
            ax.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='gray')
            ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray', linestyle=':')
            ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
            ax.add_feature(cfeature.OCEAN, facecolor='lightblue')
            
            # ç»˜åˆ¶æ•°æ®ç‚¹ - æŒ‰ç‰¹å¾å˜é‡åˆ†ç»„
            for feature, color in feature_colors.items():
                mask = lime_spatial[var_col] == feature
                if mask.any():
                    ax.scatter(
                        lime_spatial.loc[mask, 'lon'],
                        lime_spatial.loc[mask, 'lat'],
                        c=color,
                        label=feature,
                        alpha=0.75,
                        s=40,
                        edgecolors='black',
                        linewidth=0.5,
                        transform=ccrs.PlateCarree(),
                        zorder=5
                    )
            
            # ä¸æ·»åŠ å­å›¾æ ‡é¢˜ï¼ˆå·²ç§»é™¤å›¾æ³¨ï¼‰
            # âœ… åœ¨å·¦ä¸Šè§’æ·»åŠ åŠ ç²—æ ‡ç­¾ (a), (b), (c)
            ax.text(
                0.02, 0.97,                   # åæ ‡ï¼ˆç›¸å¯¹äºå›¾çš„å·¦ä¸Šè§’ï¼‰
                chr(96 + idx),                # å°å†™å­—æ¯ï¼ša,b,c...
                transform=ax.transAxes,       # ä½¿ç”¨åæ ‡è½´æ¯”ä¾‹
                fontsize=24, 
                fontweight='bold',
                va='top', ha='left'
            )            
  
            # æ·»åŠ å›¾ä¾‹ - æ¯ä¸ªå­å›¾éƒ½æœ‰è‡ªå·±çš„å›¾ä¾‹
            legend = ax.legend(
                title='Feature Variable',
                bbox_to_anchor=(1.02, 1),
                loc='upper left',
                fontsize=12,
                title_fontsize=14,
                frameon=True,
                fancybox=True,
                shadow=True,
                ncol=1,
                markerscale=1.5 
            )
        
        # æ€»æ ‡é¢˜
        fig.suptitle(
            'The spatial variation of the first, second and third predictors\ncontrolling lake Nâ‚‚O emissions derived from the LIME analysis',
            fontsize=16,
            weight='bold',
            y=0.995
        )
        
        plt.tight_layout(rect=[0, 0, 0.92, 0.99])
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç©ºé—´åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")
        plt.show()
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡ - æŒ‰ç‰¹å¾å˜é‡å’Œç±»åˆ«
        print("\n" + "="*80)
        print("ç©ºé—´åˆ†å¸ƒç»Ÿè®¡ (ç‰¹å¾å˜é‡ + ç±»åˆ«)")
        print("="*80)
        
        for i, var_col in enumerate(var_columns, 1):
            if var_col not in lime_spatial.columns:
                continue
            
            print(f"\nã€ç¬¬ {i} ä¸»å¯¼å› ç´ ã€‘")
            print("-" * 80)
            
            # ç»Ÿè®¡ç‰¹å¾å˜é‡é¢‘ç‡
            feature_counts = lime_spatial[var_col].value_counts()
            
            for feature in feature_counts.index:
                count = feature_counts[feature]
                percentage = count / len(lime_spatial) * 100
                category = self.feature_categories.get(feature, 'Other')
                
                print(f"  {feature:25s} [{category:20s}]: {count:4d} ({percentage:5.1f}%)")
            
            # æŒ‰ç±»åˆ«æ±‡æ€»
            print(f"\n  ã€ç±»åˆ«æ±‡æ€»ã€‘")
            lime_spatial[f'category_{var_col}'] = lime_spatial[var_col].map(
                lambda x: self.feature_categories.get(x, 'Other') if pd.notna(x) else 'Other'
            )
            category_counts = lime_spatial[f'category_{var_col}'].value_counts()
            
            for category in sorted(category_counts.index):
                count = category_counts[category]
                percentage = count / len(lime_spatial) * 100
                print(f"    {category:20s}: {count:4d} ({percentage:5.1f}%)")
        
        print("="*80)
        
        return lime_spatial
    


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - LIMEåˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = N2OPredictor_LIME()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y, geo_info = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ä¿å­˜æ¨¡å‹(å¯é€‰)
    print("\n3. ä¿å­˜æ¨¡å‹...")
    predictor.save_model('N2O_RF_model.joblib')
    
    # LIMEåˆ†æ
    print("\n4. æ‰§è¡ŒLIMEåˆ†æ...")
    lime_results = predictor.perform_lime_analysis(
        X, y,
        n_samples=None,  # åˆ†ææ‰€æœ‰æ ·æœ¬
        num_features=5   # æå–top5ç‰¹å¾
    )
    
    # ä¿å­˜LIMEç»“æœ
    print("\n5. ä¿å­˜LIMEç»“æœ...")
    predictor.save_lime_results(lime_results, 'LIME_results.csv')
    
    # ç»˜åˆ¶LIMEç‰¹å¾é¢‘ç‡ç›´æ–¹å›¾ (å‰3ä¸ªä¸»å¯¼å› ç´ )
    print("\n6. ç»˜åˆ¶ç‰¹å¾ç±»åˆ«åˆ†å¸ƒå›¾...")
    feature_dist = predictor.plot_lime_histogram(lime_results, top_n=3)
    
    # ç»˜åˆ¶ç©ºé—´åˆ†å¸ƒå›¾(å¦‚æœæœ‰åœ°ç†ä¿¡æ¯)
    if geo_info is not None:
        print("\n7. ç»˜åˆ¶ç©ºé—´åˆ†å¸ƒå›¾...")
        lime_spatial = predictor.plot_lime_spatial(lime_results, geo_info)
        # ä¿å­˜ç©ºé—´æ•°æ®
        predictor.save_lime_results(lime_spatial, 'LIME_spatial_results.csv')
    
    print("\n" + "="*60)
    print("LIMEåˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor, lime_results

if __name__ == "__main__":
    predictor, lime_results = main()
    
    

#%% LIME åˆ†æç»“æœ 251017

N2Oé¢„æµ‹æ¨¡å‹ - LIMEåˆ†æ
============================================================

1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...
åŸå§‹æ•°æ®é‡: 5016
è¿‡æ»¤åæ•°æ®é‡: 2998
ç¼ºå¤±å€¼ç»Ÿè®¡:
åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:
  soc_th_vav: 11 (0.4%)
  Chla_pred_RF: 2 (0.1%)
  Tyear_mean_open: 118 (3.9%)
  Log1p_Res_time: 3 (0.1%)
  Log1p_ice_days: 118 (3.9%)
  Log1p_TN_Load_Per_Volume: 1 (0.0%)
  Log1p_TP_Load_Per_Volume: 1 (0.0%)
åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: 2865 (åˆ é™¤äº†133è¡Œ)
æ•°æ®å½¢çŠ¶: X=(2865, 24), y=(2865,)

2. è®­ç»ƒæ¨¡å‹...
ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {'max_depth': None, 'max_features': 15, 'min_samples_leaf': 6, 'min_samples_split': 15, 'n_estimators': 1200}
æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: 0.6123

3. ä¿å­˜æ¨¡å‹...
æ¨¡å‹å·²ä¿å­˜è‡³: N2O_RF_model.joblib

4. æ‰§è¡ŒLIMEåˆ†æ...

å¼€å§‹LIMEåˆ†æ (å…±2865ä¸ªæ¹–æ³Š)...
æ¯ä¸ªæ¹–æ³Šåˆ†æå‰5ä¸ªç‰¹å¾,é‡‡æ ·5000æ¬¡

ä½¿ç”¨ 24 ä¸ªæœ‰æ•ˆç‰¹å¾è¿›è¡ŒLIMEåˆ†æ
  å¤„ç†è¿›åº¦: 500/2865 (å¤±è´¥: 0)
  å¤„ç†è¿›åº¦: 1000/2865 (å¤±è´¥: 0)
  å¤„ç†è¿›åº¦: 1500/2865 (å¤±è´¥: 0)
  å¤„ç†è¿›åº¦: 2000/2865 (å¤±è´¥: 0)
  å¤„ç†è¿›åº¦: 2500/2865 (å¤±è´¥: 0)

LIMEåˆ†æå®Œæˆ!
æˆåŠŸåˆ†æ: 2865/2865 ä¸ªæ ·æœ¬
å¤±è´¥æ ·æœ¬: 0

5. ä¿å­˜LIMEç»“æœ...
LIMEç»“æœå·²ä¿å­˜è‡³: LIME_results.csv
  - æ ·æœ¬æ•°: 2865
  - åˆ—æ•°: 11

6. ç»˜åˆ¶ç‰¹å¾ç±»åˆ«åˆ†å¸ƒå›¾...

ç»˜åˆ¶LIMEç‰¹å¾é¢‘ç‡ç›´æ–¹å›¾ (å‰3ä¸ªä¸»å¯¼å› ç´ )...

======================================================================
ç‰¹å¾ç±»åˆ«é¢‘ç‡ç»Ÿè®¡ (æŒ‰ä¸»å¯¼å› ç´ æ’å)
======================================================================

ã€ç¬¬ 1 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
  Hydrology           :  99.72% (2857 / 2865)
  Landcover           :   0.28% (   8 / 2865)

ã€ç¬¬ 2 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
  Landcover           :  89.39% (2561 / 2865)
  Climate             :   8.45% ( 242 / 2865)
  Physiography        :   1.40% (  40 / 2865)
  Hydrology           :   0.66% (  19 / 2865)
  Soils & Geology     :   0.10% (   3 / 2865)

ã€ç¬¬ 3 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
  Climate             :  76.30% (2186 / 2865)
  Hydrology           :   9.63% ( 276 / 2865)
  Landcover           :   6.35% ( 182 / 2865)
  Physiography        :   6.21% ( 178 / 2865)
  Soils & Geology     :   1.36% (  39 / 2865)
  Anthropogenic       :   0.14% (   4 / 2865)
======================================================================

======================================================================
ç‰¹å¾å˜é‡é¢‘ç‡ç»Ÿè®¡ (æŒ‰ä¸»å¯¼å› ç´ æ’å)
======================================================================

ã€ç¬¬ 1 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
   1. Lake_area                 [Hydrology           ]:  99.65% (2855 / 2865)
   2. crp_pc_vse                [Landcover           ]:   0.28% (   8 / 2865)
   3. Depth_avg                 [Hydrology           ]:   0.07% (   2 / 2865)

ã€ç¬¬ 2 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
   1. crp_pc_vse                [Landcover           ]:  89.39% (2561 / 2865)
   2. ari_ix_lav                [Climate             ]:   8.45% ( 242 / 2865)
   3. Elevation                 [Physiography        ]:   1.40% (  40 / 2865)
   4. Vol_total                 [Hydrology           ]:   0.52% (  15 / 2865)
   5. Lake_area                 [Hydrology           ]:   0.14% (   4 / 2865)
   6. soc_th_vav                [Soils & Geology     ]:   0.07% (   2 / 2865)
   7. gwt_cm_vav                [Soils & Geology     ]:   0.03% (   1 / 2865)

ã€ç¬¬ 3 ä¸»å¯¼å› ç´ ã€‘
----------------------------------------------------------------------
   1. ari_ix_lav                [Climate             ]:  76.30% (2186 / 2865)
   2. Vol_total                 [Hydrology           ]:   9.53% ( 273 / 2865)
   3. crp_pc_vse                [Landcover           ]:   6.35% ( 182 / 2865)
   4. Elevation                 [Physiography        ]:   6.21% ( 178 / 2865)
   5. soc_th_vav                [Soils & Geology     ]:   1.26% (  36 / 2865)
   6. Population_Density        [Anthropogenic       ]:   0.10% (   3 / 2865)
   7. ero_kh_vav                [Soils & Geology     ]:   0.10% (   3 / 2865)
   8. Lake_area                 [Hydrology           ]:   0.07% (   2 / 2865)
   9. hft_ix_v09                [Anthropogenic       ]:   0.03% (   1 / 2865)
  10. run_mm_vyr                [Hydrology           ]:   0.03% (   1 / 2865)
======================================================================

======================================================================
ç‰¹å¾å˜é‡ç»¼åˆç»Ÿè®¡ (å‰ 3 ä¸ªä¸»å¯¼å› ç´ )
======================================================================

æ€»è®¡å‡ºç°çš„ä¸åŒç‰¹å¾æ•°: 12

ç‰¹å¾å‡ºç°é¢‘ç‡æ’å (è·¨æ‰€æœ‰æ’å):
----------------------------------------------------------------------
   1. Lake_area                 [Hydrology           ]:  33.29% (2861) - [Rank1:2855, Rank2:4, Rank3:2]
   2. crp_pc_vse                [Landcover           ]:  32.01% (2751) - [Rank1:8, Rank2:2561, Rank3:182]
   3. ari_ix_lav                [Climate             ]:  28.25% (2428) - [Rank2:242, Rank3:2186]
   4. Vol_total                 [Hydrology           ]:   3.35% ( 288) - [Rank2:15, Rank3:273]
   5. Elevation                 [Physiography        ]:   2.54% ( 218) - [Rank2:40, Rank3:178]
   6. soc_th_vav                [Soils & Geology     ]:   0.44% (  38) - [Rank2:2, Rank3:36]
   7. Population_Density        [Anthropogenic       ]:   0.03% (   3) - [Rank3:3]
   8. ero_kh_vav                [Soils & Geology     ]:   0.03% (   3) - [Rank3:3]
   9. Depth_avg                 [Hydrology           ]:   0.02% (   2) - [Rank1:2]
  10. gwt_cm_vav                [Soils & Geology     ]:   0.01% (   1) - [Rank2:1]
  11. hft_ix_v09                [Anthropogenic       ]:   0.01% (   1) - [Rank3:1]
  12. run_mm_vyr                [Hydrology           ]:   0.01% (   1) - [Rank3:1]
======================================================================

ç›´æ–¹å›¾å·²ä¿å­˜è‡³: LIME_histogram.png

7. ç»˜åˆ¶ç©ºé—´åˆ†å¸ƒå›¾...

ç»˜åˆ¶LIMEç©ºé—´åˆ†å¸ƒå›¾ (å‰ä¸‰ä¸ªä¸»å¯¼å› ç´ )...
ç©ºé—´åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: LIME_spatial_map.png

================================================================================
ç©ºé—´åˆ†å¸ƒç»Ÿè®¡ (ç‰¹å¾å˜é‡ + ç±»åˆ«)
================================================================================

ã€ç¬¬ 1 ä¸»å¯¼å› ç´ ã€‘
--------------------------------------------------------------------------------
  Lake_area                 [Hydrology           ]: 2855 ( 99.7%)
  crp_pc_vse                [Landcover           ]:    8 (  0.3%)
  Depth_avg                 [Hydrology           ]:    2 (  0.1%)

  ã€ç±»åˆ«æ±‡æ€»ã€‘
    Hydrology           : 2857 ( 99.7%)
    Landcover           :    8 (  0.3%)

ã€ç¬¬ 2 ä¸»å¯¼å› ç´ ã€‘
--------------------------------------------------------------------------------
  crp_pc_vse                [Landcover           ]: 2561 ( 89.4%)
  ari_ix_lav                [Climate             ]:  242 (  8.4%)
  Elevation                 [Physiography        ]:   40 (  1.4%)
  Vol_total                 [Hydrology           ]:   15 (  0.5%)
  Lake_area                 [Hydrology           ]:    4 (  0.1%)
  soc_th_vav                [Soils & Geology     ]:    2 (  0.1%)
  gwt_cm_vav                [Soils & Geology     ]:    1 (  0.0%)

  ã€ç±»åˆ«æ±‡æ€»ã€‘
    Climate             :  242 (  8.4%)
    Hydrology           :   19 (  0.7%)
    Landcover           : 2561 ( 89.4%)
    Physiography        :   40 (  1.4%)
    Soils & Geology     :    3 (  0.1%)

ã€ç¬¬ 3 ä¸»å¯¼å› ç´ ã€‘
--------------------------------------------------------------------------------
  ari_ix_lav                [Climate             ]: 2186 ( 76.3%)
  Vol_total                 [Hydrology           ]:  273 (  9.5%)
  crp_pc_vse                [Landcover           ]:  182 (  6.4%)
  Elevation                 [Physiography        ]:  178 (  6.2%)
  soc_th_vav                [Soils & Geology     ]:   36 (  1.3%)
  Population_Density        [Anthropogenic       ]:    3 (  0.1%)
  ero_kh_vav                [Soils & Geology     ]:    3 (  0.1%)
  Lake_area                 [Hydrology           ]:    2 (  0.1%)
  hft_ix_v09                [Anthropogenic       ]:    1 (  0.0%)
  run_mm_vyr                [Hydrology           ]:    1 (  0.0%)

  ã€ç±»åˆ«æ±‡æ€»ã€‘
    Anthropogenic       :    4 (  0.1%)
    Climate             : 2186 ( 76.3%)
    Hydrology           :  276 (  9.6%)
    Landcover           :  182 (  6.4%)
    Physiography        :  178 (  6.2%)
    Soils & Geology     :   39 (  1.4%)
================================================================================
LIMEç»“æœå·²ä¿å­˜è‡³: LIME_spatial_results.csv
  - æ ·æœ¬æ•°: 2865
  - åˆ—æ•°: 16

============================================================
LIMEåˆ†æå®Œæˆ!


#%% è¾¹é™…æ•ˆåº”å›¾ï¼ˆPDPï¼‰ç»˜åˆ¶ 251018


import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        self.model = None
        self.analysis_vars = None
        self.X_original = None  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè¾¹é™…å›¾
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_counts = X.isnull().sum()
        missing_vars = missing_counts[missing_counts > 0]
        if len(missing_vars) > 0:
            print("åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:")
            for var, count in missing_vars.items():
                print(f"  {var}: {count} ({count/len(X)*100:.1f}%)")
        else:
            print("  æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        before_drop = len(X)
        complete_cases = X.notna().all(axis=1) & y.notna()
        X = X[complete_cases]
        y = y[complete_cases]
        after_drop = len(X)
        
        if before_drop != after_drop:
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
        else:
            print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ•°æ®
        if len(X) == 0:
            raise ValueError("åˆ é™¤ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        
        # ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºè¾¹é™…å›¾ï¼‰
        self.X_original = X.copy()
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
    
    def plot_marginal_effects(self, X, y, features_to_plot=None):
        """
        ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾(Partial Dependence Plot)
        
        Parameters:
        -----------
        X : DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        features_to_plot : list
            è¦ç»˜åˆ¶çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆä½¿ç”¨å˜æ¢åçš„åç§°ï¼‰
        """
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        if features_to_plot is None:
            features_to_plot = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                              'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
        
        # å®šä¹‰å“ªäº›å˜é‡éœ€è¦å¯¹æ•°å°ºåº¦Xè½´æ˜¾ç¤º
        log_scale_features = ['Log1p_Lake_area', 'Log1p_Population_Density']
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        valid_features = [f for f in features_to_plot if f in X.columns]
        if len(valid_features) != len(features_to_plot):
            missing = set(features_to_plot) - set(valid_features)
            print(f"è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾ä¸å­˜åœ¨: {missing}")
        
        if len(valid_features) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾å¯ä»¥ç»˜åˆ¶!")
        
        print(f"\nç»˜åˆ¶ {len(valid_features)} ä¸ªç‰¹å¾çš„è¾¹é™…æ•ˆåº”å›¾...")
        
        # åˆ›å»ºå­å›¾å¸ƒå±€ (2è¡Œ3åˆ—)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        # å®šä¹‰ç‰¹å¾çš„æ˜¾ç¤ºåç§°æ˜ å°„
        display_names = {
            'Log1p_Lake_area': 'Lake Area (kmÂ²)',
            'crp_pc_vse': 'Cropland Extent(%)',
            'ari_ix_lav': 'Global Aridity Index (*100)',
            'Elevation': 'Elevation (m)',
            'Log1p_Population_Density': 'Population Density (people/kmÂ²)',
            'run_mm_vyr': 'Land Surface Runoff (mm/yr)'
        }
        
        for idx, feature in enumerate(valid_features):
            ax = axes[idx]
            
            # è·å–ç‰¹å¾åœ¨Xä¸­çš„ç´¢å¼•
            feature_idx = X.columns.get_loc(feature)
            
            # è®¡ç®—partial dependence
            pd_result = partial_dependence(
                self.model, 
                X, 
                features=[feature_idx],
                grid_resolution=100
            )
            
            # è·å–åŸå§‹å°ºåº¦çš„ç‰¹å¾å€¼ï¼ˆç”¨äºxè½´æ˜¾ç¤ºï¼‰
            # åæ ‡å‡†åŒ–
            feature_values_scaled = pd_result['grid_values'][0]
            
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°ç»„ç”¨äºåæ ‡å‡†åŒ–
            temp_array = np.zeros((len(feature_values_scaled), X.shape[1]))
            temp_array[:, feature_idx] = feature_values_scaled
            temp_df = pd.DataFrame(temp_array, columns=X.columns)
            
            # åæ ‡å‡†åŒ–
            feature_values_original = self.scaler.inverse_transform(temp_df)[:, feature_idx]
            
            # å¦‚æœæ˜¯å¯¹æ•°å˜æ¢çš„å˜é‡ï¼Œéœ€è¦åå˜æ¢
            if feature.startswith('Log1p_'):
                feature_values_original = np.expm1(feature_values_original)
            
            # å°†é¢„æµ‹å€¼ä»log10åè½¬æ¢ä¸ºåŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿éè´Ÿ
            pd_values = pd_result['average'][0]
            pd_values_original = np.maximum(10**pd_values - 1e-10, 0)  # ç¡®ä¿éè´Ÿ
            
            # ç»˜å›¾
            ax.plot(feature_values_original, pd_values_original, 
                   linewidth=2.5, color='#2E86AB', alpha=0.8)
            
            # è®¡ç®—é¢„æµ‹çš„æ ‡å‡†å·®ï¼ˆä½¿ç”¨æ‰€æœ‰æ ‘çš„é¢„æµ‹ï¼‰
            # å¯¹äºæ¯ä¸ªç½‘æ ¼ç‚¹ï¼Œè®¡ç®—æ ‡å‡†å·®
            grid_predictions = []
            for val_scaled in feature_values_scaled:
                X_temp = X.copy()
                X_temp.iloc[:, feature_idx] = val_scaled
                tree_preds = np.array([tree.predict(X_temp) for tree in self.model.estimators_])
                grid_predictions.append(tree_preds.mean(axis=0))
            
            grid_predictions = np.array(grid_predictions)
            std_pred = np.std(grid_predictions, axis=1)
            
            # è½¬æ¢æ ‡å‡†å·®åˆ°åŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿ç½®ä¿¡åŒºé—´éè´Ÿ
            upper_bound = np.maximum(10**(pd_values + std_pred) - 1e-10, 0)
            lower_bound = np.maximum(10**(pd_values - std_pred) - 1e-10, 0)
            
            ax.fill_between(feature_values_original, 
                          lower_bound,
                          upper_bound,
                          alpha=0.2, color='#2E86AB')
            
            # æ·»åŠ æ•°æ®åˆ†å¸ƒï¼ˆåœ°æ¯¯å›¾ï¼‰
            original_feature_name = feature.replace('Log1p_', '') if feature.startswith('Log1p_') else feature
            if self.X_original is not None and original_feature_name in self.X_original.columns:
                data_points = self.X_original[original_feature_name].values
            else:
                # å¦‚æœæ— æ³•è·å–åŸå§‹æ•°æ®ï¼Œä½¿ç”¨åæ ‡å‡†åŒ–çš„æ•°æ®
                data_points = self.scaler.inverse_transform(X)[:, feature_idx]
                if feature.startswith('Log1p_'):
                    data_points = np.expm1(data_points)
            
            # ç»˜åˆ¶åœ°æ¯¯å›¾
            y_min, y_max = ax.get_ylim()
            rug_height = (y_max - y_min) * 0.02
            
            # è¿‡æ»¤æ‰è¿‡å°çš„å€¼ä»¥é¿å…å¯¹æ•°å°ºåº¦é—®é¢˜
            if feature in log_scale_features:
                data_points_filtered = data_points[data_points > 0.01]
            else:
                data_points_filtered = data_points
            
            ax.plot(data_points_filtered, 
                   np.ones_like(data_points_filtered) * y_min + rug_height,
                   '|', color='gray', alpha=0.3, markersize=2)
            
            # è®¾ç½®Xè½´ä¸ºå¯¹æ•°å°ºåº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if feature in log_scale_features:
                ax.set_xscale('log')
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            display_name = display_names.get(feature, feature)
            ax.set_xlabel(display_name, fontsize=11, fontweight='bold')
            ax.set_ylabel('Nâ‚‚O Flux (mg N mâ»Â² dâ»Â¹)', fontsize=11)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(valid_features), len(axes)):
            axes[idx].set_visible(False)
        
        # æ·»åŠ æ€»æ ‡é¢˜
        fig.suptitle('Marginal Effects of Environmental Factors on Nâ‚‚O Flux', 
                    fontsize=14, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        
        # ä¿å­˜å›¾ç‰‡
        filename = "marginal_effects_plot.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"è¾¹é™…æ•ˆåº”å›¾å·²ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        return fig


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - è¾¹é™…æ•ˆåº”åˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾
    print("\n3. è¾¹é™…æ•ˆåº”åˆ†æ...")
    features_to_analyze = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                          'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
    
    fig = predictor.plot_marginal_effects(X, y, features_to_plot=features_to_analyze)
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor

if __name__ == "__main__":
    predictor = main()

#%% PNASçš„Marginal Analysis 251022

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        self.model = None
        self.analysis_vars = None
        self.X_original = None  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè¾¹é™…å›¾
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_counts = X.isnull().sum()
        missing_vars = missing_counts[missing_counts > 0]
        if len(missing_vars) > 0:
            print("åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:")
            for var, count in missing_vars.items():
                print(f"  {var}: {count} ({count/len(X)*100:.1f}%)")
        else:
            print("  æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        before_drop = len(X)
        complete_cases = X.notna().all(axis=1) & y.notna()
        X = X[complete_cases]
        y = y[complete_cases]
        after_drop = len(X)
        
        if before_drop != after_drop:
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
        else:
            print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ•°æ®
        if len(X) == 0:
            raise ValueError("åˆ é™¤ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        
        # ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºè¾¹é™…å›¾ï¼‰
        self.X_original = X.copy()
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
        
    def plot_marginal_effects_pnas(self, X, y, features_to_plot=None, n_grid_points=100):
        """
        ä½¿ç”¨PNASæ–¹æ³•ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾
        
        PNASæ–¹æ³•æè¿°ï¼š
        "For each environmental driver, we reran the calibrated random forest model 
        by equally sampling within its range while keeping other drivers as constant 
        at their averaged values."
        
        Parameters:
        -----------
        X : DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        features_to_plot : list
            è¦ç»˜åˆ¶çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆä½¿ç”¨å˜æ¢åçš„åç§°ï¼‰
        n_grid_points : int
            åœ¨ç‰¹å¾èŒƒå›´å†…å‡åŒ€é‡‡æ ·çš„ç‚¹æ•°ï¼ˆé»˜è®¤100ï¼‰
        """
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        if features_to_plot is None:
            features_to_plot = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                              'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
        
        # å®šä¹‰å“ªäº›å˜é‡éœ€è¦å¯¹æ•°å°ºåº¦Xè½´æ˜¾ç¤º
        log_scale_features = ['Log1p_Lake_area', 'Log1p_Population_Density']
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        valid_features = [f for f in features_to_plot if f in X.columns]
        if len(valid_features) != len(features_to_plot):
            missing = set(features_to_plot) - set(valid_features)
            print(f"è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾ä¸å­˜åœ¨: {missing}")
        
        if len(valid_features) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾å¯ä»¥ç»˜åˆ¶!")
        
        print(f"\nä½¿ç”¨PNASæ–¹æ³•ç»˜åˆ¶ {len(valid_features)} ä¸ªç‰¹å¾çš„è¾¹é™…æ•ˆåº”å›¾...")
        print(f"  - åœ¨æ¯ä¸ªç‰¹å¾èŒƒå›´å†…å‡åŒ€é‡‡æ · {n_grid_points} ä¸ªç‚¹")
        print(f"  - å…¶ä»–ç‰¹å¾ä¿æŒä¸ºå¹³å‡å€¼\n")
        
        # åˆ›å»ºå­å›¾å¸ƒå±€ (2è¡Œ3åˆ—)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        # å®šä¹‰ç‰¹å¾çš„æ˜¾ç¤ºåç§°æ˜ å°„
        display_names = {
            'Log1p_Lake_area': 'Lake Area (kmÂ²)',
            'crp_pc_vse': 'Cropland (%)',
            'ari_ix_lav': 'Aridity Index',
            'Elevation': 'Elevation (m)',
            'Log1p_Population_Density': 'Population Density (people/kmÂ²)',
            'run_mm_vyr': 'Runoff (mm/yr)'
        }
        
        # è®¡ç®—æ‰€æœ‰ç‰¹å¾çš„å¹³å‡å€¼ï¼ˆä½œä¸ºåŸºçº¿ï¼‰
        X_mean = X.mean()
        
        for idx, feature in enumerate(valid_features):
            ax = axes[idx]
            
            # è·å–ç‰¹å¾åœ¨Xä¸­çš„ç´¢å¼•
            feature_idx = X.columns.get_loc(feature)
            
            # === PNAS Marginal Plotæ–¹æ³• ===
            # æ­¥éª¤1: åœ¨ç›®æ ‡ç‰¹å¾èŒƒå›´å†…å‡åŒ€é‡‡æ ·
            feature_min = X[feature].min()
            feature_max = X[feature].max()
            feature_values_scaled = np.linspace(feature_min, feature_max, n_grid_points)
            
            # æ­¥éª¤2: åˆ›å»ºé¢„æµ‹æ•°æ® - å…¶ä»–ç‰¹å¾ä¿æŒä¸ºå¹³å‡å€¼
            X_marginal = pd.DataFrame(
                np.tile(X_mean.values, (n_grid_points, 1)),
                columns=X.columns
            )
            
            # æ­¥éª¤3: åªè®©ç›®æ ‡ç‰¹å¾å˜åŒ–
            X_marginal[feature] = feature_values_scaled
            
            # æ­¥éª¤4: ä½¿ç”¨æ¨¡å‹é¢„æµ‹
            y_pred_mean = self.model.predict(X_marginal)
            
            # æ­¥éª¤5: è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆä½¿ç”¨éšæœºæ£®æ—çš„æ ‘é¢„æµ‹æ ‡å‡†å·®ï¼‰
            tree_predictions = np.array([tree.predict(X_marginal) for tree in self.model.estimators_])
            y_pred_std = np.std(tree_predictions, axis=0)
            
            # === æ•°æ®è½¬æ¢å’Œç»˜å›¾ ===
            # åæ ‡å‡†åŒ–åˆ°åŸå§‹å°ºåº¦
            temp_array = np.zeros((len(feature_values_scaled), X.shape[1]))
            temp_array[:, feature_idx] = feature_values_scaled
            temp_df = pd.DataFrame(temp_array, columns=X.columns)
            
            # åæ ‡å‡†åŒ–
            feature_values_original = self.scaler.inverse_transform(temp_df)[:, feature_idx]
            
            # å¦‚æœæ˜¯å¯¹æ•°å˜æ¢çš„å˜é‡ï¼Œéœ€è¦åå˜æ¢
            if feature.startswith('Log1p_'):
                feature_values_original = np.expm1(feature_values_original)
            
            # å°†é¢„æµ‹å€¼ä»log10åè½¬æ¢ä¸ºåŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿éè´Ÿ
            pd_values_original = np.maximum(10**y_pred_mean - 1e-10, 0)
            
            # è½¬æ¢æ ‡å‡†å·®åˆ°åŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿ç½®ä¿¡åŒºé—´éè´Ÿ
            upper_bound = np.maximum(10**(y_pred_mean + y_pred_std) - 1e-10, 0)
            lower_bound = np.maximum(10**(y_pred_mean - y_pred_std) - 1e-10, 0)
            
            # ç»˜åˆ¶ä¸»æ›²çº¿
            ax.plot(feature_values_original, pd_values_original, 
                   linewidth=2.5, color='#2E86AB', alpha=0.8, label='Marginal Effect')
            
            # æ·»åŠ ä¸ç¡®å®šæ€§åŒºé—´
            ax.fill_between(feature_values_original, 
                          lower_bound,
                          upper_bound,
                          alpha=0.2, color='#2E86AB', label='Â±1 SD')
            
            # æ·»åŠ æ•°æ®åˆ†å¸ƒï¼ˆåœ°æ¯¯å›¾ï¼‰- æ˜¾ç¤ºå®é™…æ•°æ®ç‚¹çš„åˆ†å¸ƒ
            original_feature_name = feature.replace('Log1p_', '') if feature.startswith('Log1p_') else feature
            if self.X_original is not None and original_feature_name in self.X_original.columns:
                data_points = self.X_original[original_feature_name].values
            else:
                # å¦‚æœæ— æ³•è·å–åŸå§‹æ•°æ®ï¼Œä½¿ç”¨åæ ‡å‡†åŒ–çš„æ•°æ®
                data_points = self.scaler.inverse_transform(X)[:, feature_idx]
                if feature.startswith('Log1p_'):
                    data_points = np.expm1(data_points)
            
            # ç»˜åˆ¶åœ°æ¯¯å›¾
            y_min, y_max = ax.get_ylim()
            rug_height = (y_max - y_min) * 0.02
            
            # è¿‡æ»¤æ‰è¿‡å°çš„å€¼ä»¥é¿å…å¯¹æ•°å°ºåº¦é—®é¢˜
            if feature in log_scale_features:
                data_points_filtered = data_points[data_points > 0.01]
            else:
                data_points_filtered = data_points
            
            # é‡‡æ ·åœ°æ¯¯å›¾ç‚¹ï¼ˆå¦‚æœæ•°æ®ç‚¹å¤ªå¤šï¼‰
            if len(data_points_filtered) > 1000:
                sample_indices = np.random.choice(len(data_points_filtered), 1000, replace=False)
                data_points_filtered = data_points_filtered[sample_indices]
            
            ax.plot(data_points_filtered, 
                   np.ones_like(data_points_filtered) * y_min + rug_height,
                   '|', color='gray', alpha=0.3, markersize=2)
            
            # è®¾ç½®Xè½´ä¸ºå¯¹æ•°å°ºåº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if feature in log_scale_features:
                ax.set_xscale('log')
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            display_name = display_names.get(feature, feature)
            ax.set_xlabel(display_name, fontsize=11, fontweight='bold')
            ax.set_ylabel('Nâ‚‚O Flux (Î¼g N mâ»Â² dâ»Â¹)', fontsize=11)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
            # åªåœ¨ç¬¬ä¸€ä¸ªå­å›¾æ·»åŠ å›¾ä¾‹
            if idx == 0:
                ax.legend(loc='best', frameon=False, fontsize=9)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(valid_features), len(axes)):
            axes[idx].set_visible(False)
        
        # æ·»åŠ æ€»æ ‡é¢˜
        fig.suptitle('Marginal Effects of Environmental Factors on Nâ‚‚O Flux\n(PNAS Method)', 
                    fontsize=14, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        
        # ä¿å­˜å›¾ç‰‡
        output_dir = '/mnt/user-data/outputs'
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, "marginal_effects_pnas.png")
        
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"è¾¹é™…æ•ˆåº”å›¾å·²ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        return fig
    
    def compare_marginal_methods(self, X, y, features_to_plot=None, n_grid_points=100):
        """
        å¯¹æ¯”PNASæ–¹æ³•å’Œsklearnçš„PDPæ–¹æ³•
        
        Parameters:
        -----------
        X : DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        features_to_plot : list
            è¦ç»˜åˆ¶çš„ç‰¹å¾ï¼ˆæœ€å¤š4ä¸ªç”¨äºå¯¹æ¯”ï¼‰
        n_grid_points : int
            é‡‡æ ·ç‚¹æ•°
        """
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        if features_to_plot is None:
            # é€‰æ‹©4ä¸ªä»£è¡¨æ€§ç‰¹å¾è¿›è¡Œå¯¹æ¯”
            features_to_plot = ['Log1p_Lake_area', 'crp_pc_vse', 
                              'Elevation', 'Log1p_Population_Density']
        
        # é™åˆ¶æœ€å¤š4ä¸ªç‰¹å¾
        features_to_plot = features_to_plot[:4]
        
        print(f"\nå¯¹æ¯”PNASæ–¹æ³•å’Œsklearn PDPæ–¹æ³•...")
        
        # å®šä¹‰æ˜¾ç¤ºåç§°
        display_names = {
            'Log1p_Lake_area': 'Lake Area',
            'crp_pc_vse': 'Cropland',
            'ari_ix_lav': 'Aridity Index',
            'Elevation': 'Elevation',
            'Log1p_Population_Density': 'Population Density',
            'run_mm_vyr': 'Runoff'
        }
        
        # å®šä¹‰å¯¹æ•°å°ºåº¦ç‰¹å¾
        log_scale_features = ['Log1p_Lake_area', 'Log1p_Population_Density']
        
        # åˆ›å»ºå­å›¾ (2è¡Œï¼Œæ¯è¡Œæ˜¾ç¤ºä¸€ä¸ªç‰¹å¾çš„ä¸¤ç§æ–¹æ³•)
        fig, axes = plt.subplots(len(features_to_plot), 2, figsize=(12, 3*len(features_to_plot)))
        if len(features_to_plot) == 1:
            axes = axes.reshape(1, -1)
        
        X_mean = X.mean()
        
        for idx, feature in enumerate(features_to_plot):
            feature_idx = X.columns.get_loc(feature)
            
            # === å·¦å›¾: PNASæ–¹æ³• ===
            ax_pnas = axes[idx, 0]
            
            # å‡åŒ€é‡‡æ ·
            feature_min = X[feature].min()
            feature_max = X[feature].max()
            feature_values_scaled = np.linspace(feature_min, feature_max, n_grid_points)
            
            # åˆ›å»ºé¢„æµ‹æ•°æ®
            X_marginal = pd.DataFrame(
                np.tile(X_mean.values, (n_grid_points, 1)),
                columns=X.columns
            )
            X_marginal[feature] = feature_values_scaled
            
            # é¢„æµ‹
            y_pred_pnas = self.model.predict(X_marginal)
            tree_predictions = np.array([tree.predict(X_marginal) for tree in self.model.estimators_])
            y_std_pnas = np.std(tree_predictions, axis=0)
            
            # === å³å›¾: sklearn PDPæ–¹æ³• ===
            ax_pdp = axes[idx, 1]
            
            from sklearn.inspection import partial_dependence
            pd_result = partial_dependence(
                self.model, 
                X, 
                features=[feature_idx],
                grid_resolution=n_grid_points
            )
            
            feature_values_pdp = pd_result['grid_values'][0]
            y_pred_pdp = pd_result['average'][0]
            
            # è®¡ç®—PDPçš„ä¸ç¡®å®šæ€§
            grid_predictions = []
            for val_scaled in feature_values_pdp:
                X_temp = X.copy()
                X_temp.iloc[:, feature_idx] = val_scaled
                tree_preds = np.array([tree.predict(X_temp) for tree in self.model.estimators_])
                grid_predictions.append(tree_preds.mean(axis=0))
            
            grid_predictions = np.array(grid_predictions)
            y_std_pdp = np.std(grid_predictions, axis=1)
            
            # === è½¬æ¢å’Œç»˜å›¾ ===
            for ax, feature_vals, y_pred, y_std, method_name in [
                (ax_pnas, feature_values_scaled, y_pred_pnas, y_std_pnas, 'PNAS Method'),
                (ax_pdp, feature_values_pdp, y_pred_pdp, y_std_pdp, 'sklearn PDP')
            ]:
                # åæ ‡å‡†åŒ–
                temp_array = np.zeros((len(feature_vals), X.shape[1]))
                temp_array[:, feature_idx] = feature_vals
                temp_df = pd.DataFrame(temp_array, columns=X.columns)
                feature_original = self.scaler.inverse_transform(temp_df)[:, feature_idx]
                
                if feature.startswith('Log1p_'):
                    feature_original = np.expm1(feature_original)
                
                # è½¬æ¢é¢„æµ‹å€¼
                y_original = np.maximum(10**y_pred - 1e-10, 0)
                upper = np.maximum(10**(y_pred + y_std) - 1e-10, 0)
                lower = np.maximum(10**(y_pred - y_std) - 1e-10, 0)
                
                # ç»˜å›¾
                ax.plot(feature_original, y_original, linewidth=2.5, color='#2E86AB')
                ax.fill_between(feature_original, lower, upper, alpha=0.2, color='#2E86AB')
                
                # æ·»åŠ åœ°æ¯¯å›¾
                original_feature_name = feature.replace('Log1p_', '') if feature.startswith('Log1p_') else feature
                if self.X_original is not None and original_feature_name in self.X_original.columns:
                    data_points = self.X_original[original_feature_name].values
                    if len(data_points) > 1000:
                        sample_indices = np.random.choice(len(data_points), 1000, replace=False)
                        data_points = data_points[sample_indices]
                    
                    if feature in log_scale_features:
                        data_points = data_points[data_points > 0.01]
                    
                    y_min, y_max = ax.get_ylim()
                    rug_height = (y_max - y_min) * 0.02
                    ax.plot(data_points, np.ones_like(data_points) * y_min + rug_height,
                           '|', color='gray', alpha=0.3, markersize=2)
                
                # è®¾ç½®æ ·å¼
                if feature in log_scale_features:
                    ax.set_xscale('log')
                
                display_name = display_names.get(feature, feature)
                ax.set_title(f'{display_name} - {method_name}', fontsize=11, fontweight='bold')
                ax.set_xlabel(display_name, fontsize=10)
                ax.set_ylabel('Nâ‚‚O Flux (Î¼g N mâ»Â² dâ»Â¹)', fontsize=10)
                ax.grid(True, alpha=0.3, linestyle='--')
                ax.spines['top'].set_visible(False)
                ax.spines['right'].set_visible(False)
        
        fig.suptitle('Comparison: PNAS Marginal Method vs sklearn Partial Dependence', 
                    fontsize=14, fontweight='bold', y=0.995)
        
        plt.tight_layout(rect=[0, 0, 1, 0.99])
        
        # ä¿å­˜
        output_dir = '/mnt/user-data/outputs'
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, "method_comparison.png")
        
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        return fig


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - PNAS Marginal Analysis")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ä½¿ç”¨PNASæ–¹æ³•ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾
    print("\n3. ä½¿ç”¨PNASæ–¹æ³•è¿›è¡Œè¾¹é™…æ•ˆåº”åˆ†æ...")
    features_to_analyze = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                          'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
    
    fig = predictor.plot_marginal_effects_pnas(
        X, y, 
        features_to_plot=features_to_analyze, 
        n_grid_points=100  # åœ¨æ¯ä¸ªç‰¹å¾èŒƒå›´å†…é‡‡æ ·100ä¸ªç‚¹
    )
    
    # å¦‚æœæƒ³å¯¹æ¯”ä¸¤ç§æ–¹æ³•ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š
    print("\n4. å¯¹æ¯”ä¸åŒæ–¹æ³•...")
    fig_comparison = predictor.compare_marginal_methods(X, y, features_to_plot=features_to_analyze[:4])
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor

if __name__ == "__main__":
    predictor = main()


#%% Partial Dependence Plot (PDP) å¯¹æ¯” Marginal Plot (PNASæ–¹æ³•) 251018

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'


class SimpleN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        
        # ç‰¹å¾å®šä¹‰
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # è¦ç§»é™¤çš„å˜é‡
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        
        # éœ€è¦å¯¹æ•°å˜æ¢çš„å˜é‡
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        
        # æœ€ä¼˜å‚æ•°ï¼ˆé¢„è®¾ï¼‰
        self.best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        self.model = None
        self.analysis_vars = None
        self.X_original = None  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè¾¹é™…å›¾
        
    def load_and_preprocess_data(self, filepath):
        """ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†"""
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"åŸå§‹æ•°æ®é‡: {len(data)}")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))
        ].copy()
        print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")
        
        # å¯¹æ•°å˜æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars].replace([np.inf, -np.inf], np.nan)
        y = data_filtered['Log_N2O']
        
        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:")
        missing_counts = X.isnull().sum()
        missing_vars = missing_counts[missing_counts > 0]
        if len(missing_vars) > 0:
            print("åŒ…å«ç¼ºå¤±å€¼çš„å˜é‡:")
            for var, count in missing_vars.items():
                print(f"  {var}: {count} ({count/len(X)*100:.1f}%)")
        else:
            print("  æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
        
        # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        before_drop = len(X)
        complete_cases = X.notna().all(axis=1) & y.notna()
        X = X[complete_cases]
        y = y[complete_cases]
        after_drop = len(X)
        
        if before_drop != after_drop:
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
        else:
            print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ•°æ®
        if len(X) == 0:
            raise ValueError("åˆ é™¤ç¼ºå¤±å€¼åæ²¡æœ‰å‰©ä½™æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        
        # ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºè¾¹é™…å›¾ï¼‰
        self.X_original = X.copy()
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = pd.DataFrame(
            self.scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        return X_scaled, y

    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ä½¿ç”¨é¢„è®¾å‚æ•°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹: {self.best_params}")
        
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **self.best_params
        )
        
        self.model.fit(X, y)
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ! OOB Score: {self.model.oob_score_:.4f}")
        
        return self.model
        
    def plot_marginal_effects(self, X, y, features_to_plot=None, method='pdp'):
        """
        ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾
        
        Parameters:
        -----------
        X : DataFrame
            æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
        y : Series
            ç›®æ ‡å˜é‡
        features_to_plot : list
            è¦ç»˜åˆ¶çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆä½¿ç”¨å˜æ¢åçš„åç§°ï¼‰
        method : str
            'pdp' - Partial Dependence Plot (é»˜è®¤ï¼Œæ›´ç¨³å¥)
            'marginal' - Marginal Plot (PNASæ–¹æ³•ï¼Œæ›´å¿«é€Ÿ)
        """
        if self.model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹!")
        
        if features_to_plot is None:
            features_to_plot = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                              'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
        
        # å®šä¹‰å“ªäº›å˜é‡éœ€è¦å¯¹æ•°å°ºåº¦Xè½´æ˜¾ç¤º
        log_scale_features = ['Log1p_Lake_area', 'Log1p_Population_Density']
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        valid_features = [f for f in features_to_plot if f in X.columns]
        if len(valid_features) != len(features_to_plot):
            missing = set(features_to_plot) - set(valid_features)
            print(f"è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾ä¸å­˜åœ¨: {missing}")
        
        if len(valid_features) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾å¯ä»¥ç»˜åˆ¶!")
        
        print(f"\nç»˜åˆ¶ {len(valid_features)} ä¸ªç‰¹å¾çš„è¾¹é™…æ•ˆåº”å›¾...")
        
        # åˆ›å»ºå­å›¾å¸ƒå±€ (2è¡Œ3åˆ—)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        # å®šä¹‰ç‰¹å¾çš„æ˜¾ç¤ºåç§°æ˜ å°„
        display_names = {
            'Log1p_Lake_area': 'Lake Area (kmÂ²)',
            'crp_pc_vse': 'Cropland (%)',
            'ari_ix_lav': 'Aridity Index',
            'Elevation': 'Elevation (m)',
            'Log1p_Population_Density': 'Population Density (people/kmÂ²)',
            'run_mm_vyr': 'Runoff (mm/yr)'
        }
        
        for idx, feature in enumerate(valid_features):
            ax = axes[idx]
            
            # è·å–ç‰¹å¾åœ¨Xä¸­çš„ç´¢å¼•
            feature_idx = X.columns.get_loc(feature)
            
            if method == 'pdp':
                # === æ–¹æ³•1: Partial Dependence Plot (sklearnæ ‡å‡†æ–¹æ³•) ===
                pd_result = partial_dependence(
                    self.model, 
                    X, 
                    features=[feature_idx],
                    grid_resolution=50
                )
                
                feature_values_scaled = pd_result['grid_values'][0]
                pd_values = pd_result['average'][0]
                
                # è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆä½¿ç”¨æ ‘çš„æ ‡å‡†å·®ï¼‰
                grid_predictions = []
                for val_scaled in feature_values_scaled:
                    X_temp = X.copy()
                    X_temp.iloc[:, feature_idx] = val_scaled
                    tree_preds = np.array([tree.predict(X_temp) for tree in self.model.estimators_])
                    grid_predictions.append(tree_preds.mean(axis=0))
                
                grid_predictions = np.array(grid_predictions)
                std_pred = np.std(grid_predictions, axis=1)
                
            else:  # method == 'marginal'
                # === æ–¹æ³•2: Marginal Plot (PNASæ–¹æ³•) ===
                # åˆ›å»ºåŸºçº¿ï¼šæ‰€æœ‰ç‰¹å¾è®¾ä¸ºä¸­ä½æ•°
                X_baseline = X.copy()
                for col in X.columns:
                    X_baseline[col] = X[col].median()
                
                # åªè®©ç›®æ ‡ç‰¹å¾å˜åŒ–
                X_marginal = X_baseline.copy()
                X_marginal[feature] = X[feature]
                
                # é¢„æµ‹
                y_pred = self.model.predict(X_marginal)
                
                # ä¸ºäº†ç»˜åˆ¶å¹³æ»‘æ›²çº¿ï¼Œå¯¹æ•°æ®æ’åºå¹¶åˆ†ç»„
                sorted_indices = X[feature].argsort()
                feature_values_scaled = X[feature].iloc[sorted_indices].values
                pd_values = y_pred[sorted_indices]
                
                # ä½¿ç”¨æ»‘åŠ¨çª—å£å¹³æ»‘ï¼ˆå¯é€‰ï¼‰
                window_size = max(len(pd_values) // 50, 10)
                pd_values_smooth = pd.Series(pd_values).rolling(window=window_size, center=True).mean().values
                std_pred = pd.Series(pd_values).rolling(window=window_size, center=True).std().values
                
                # å»é™¤NaN
                valid_mask = ~np.isnan(pd_values_smooth)
                feature_values_scaled = feature_values_scaled[valid_mask]
                pd_values = pd_values_smooth[valid_mask]
                std_pred = std_pred[valid_mask]
            
            # === å…±åŒéƒ¨åˆ†ï¼šæ•°æ®è½¬æ¢å’Œç»˜å›¾ ===
            # åæ ‡å‡†åŒ–åˆ°åŸå§‹å°ºåº¦
            temp_array = np.zeros((len(feature_values_scaled), X.shape[1]))
            temp_array[:, feature_idx] = feature_values_scaled
            temp_df = pd.DataFrame(temp_array, columns=X.columns)
            
            # åæ ‡å‡†åŒ–
            feature_values_original = self.scaler.inverse_transform(temp_df)[:, feature_idx]
            
            # å¦‚æœæ˜¯å¯¹æ•°å˜æ¢çš„å˜é‡ï¼Œéœ€è¦åå˜æ¢
            if feature.startswith('Log1p_'):
                feature_values_original = np.expm1(feature_values_original)
            
            # å°†é¢„æµ‹å€¼ä»log10åè½¬æ¢ä¸ºåŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿éè´Ÿ
            pd_values_original = np.maximum(10**pd_values - 1e-10, 0)
            
            # è½¬æ¢æ ‡å‡†å·®åˆ°åŸå§‹å°ºåº¦ï¼Œå¹¶ç¡®ä¿ç½®ä¿¡åŒºé—´éè´Ÿ
            upper_bound = np.maximum(10**(pd_values + std_pred) - 1e-10, 0)
            lower_bound = np.maximum(10**(pd_values - std_pred) - 1e-10, 0)
            
            # ç»˜å›¾
            ax.plot(feature_values_original, pd_values_original, 
                   linewidth=2.5, color='#2E86AB', alpha=0.8)
            
            # æ·»åŠ ä¸ç¡®å®šæ€§åŒºé—´
            ax.fill_between(feature_values_original, 
                          lower_bound,
                          upper_bound,
                          alpha=0.2, color='#2E86AB')
            
            # æ·»åŠ æ•°æ®åˆ†å¸ƒï¼ˆåœ°æ¯¯å›¾ï¼‰
            original_feature_name = feature.replace('Log1p_', '') if feature.startswith('Log1p_') else feature
            if self.X_original is not None and original_feature_name in self.X_original.columns:
                data_points = self.X_original[original_feature_name].values
            else:
                # å¦‚æœæ— æ³•è·å–åŸå§‹æ•°æ®ï¼Œä½¿ç”¨åæ ‡å‡†åŒ–çš„æ•°æ®
                data_points = self.scaler.inverse_transform(X)[:, feature_idx]
                if feature.startswith('Log1p_'):
                    data_points = np.expm1(data_points)
            
            # ç»˜åˆ¶åœ°æ¯¯å›¾
            y_min, y_max = ax.get_ylim()
            rug_height = (y_max - y_min) * 0.02
            
            # è¿‡æ»¤æ‰è¿‡å°çš„å€¼ä»¥é¿å…å¯¹æ•°å°ºåº¦é—®é¢˜
            if feature in log_scale_features:
                data_points_filtered = data_points[data_points > 0.01]
            else:
                data_points_filtered = data_points
            
            ax.plot(data_points_filtered, 
                   np.ones_like(data_points_filtered) * y_min + rug_height,
                   '|', color='gray', alpha=0.3, markersize=2)
            
            # è®¾ç½®Xè½´ä¸ºå¯¹æ•°å°ºåº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if feature in log_scale_features:
                ax.set_xscale('log')
            
            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            display_name = display_names.get(feature, feature)
            ax.set_xlabel(display_name, fontsize=11, fontweight='bold')
            ax.set_ylabel('Nâ‚‚O Flux (Î¼g N mâ»Â² dâ»Â¹)', fontsize=11)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(valid_features), len(axes)):
            axes[idx].set_visible(False)
        
        # æ·»åŠ æ€»æ ‡é¢˜
        method_name = "Partial Dependence Plot" if method == 'pdp' else "Marginal Plot"
        fig.suptitle(f'Marginal Effects of Environmental Factors on Nâ‚‚O Flux\n({method_name})', 
                    fontsize=14, fontweight='bold', y=0.98)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"marginal_effects_{method}.png"
        try:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"è¾¹é™…æ•ˆåº”å›¾å·²ä¿å­˜è‡³: {filename}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡å‡ºé”™: {e}")
        
        plt.show()
        
        return fig
    
        
        def compare_methods(self, X, y, features_to_plot=None):
            """
            å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç»“æœ
            """
            print("\n=== å¯¹æ¯”ä¸¤ç§è¾¹é™…æ•ˆåº”åˆ†ææ–¹æ³• ===\n")
            
            print("æ–¹æ³•1: Partial Dependence Plot (PDP)")
            print("  - å¯¹æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹å–å¹³å‡")
            print("  - æ›´ç¨³å¥ï¼Œè€ƒè™‘äº†å˜é‡é—´çš„è‡ªç„¶ç›¸å…³æ€§")
            print("  - è®¡ç®—è¾ƒæ…¢\n")
            fig1 = self.plot_marginal_effects(X, y, features_to_plot, method='pdp')
            
            print("\næ–¹æ³•2: Marginal Plot (PNASæ–¹æ³•)")
            print("  - å›ºå®šå…¶ä»–å˜é‡ä¸ºä¸­ä½æ•°")
            print("  - åªè¯„ä¼°ä¸€ä¸ª'å…¸å‹'æ¡ä»¶ä¸‹çš„æ•ˆåº”")
            print("  - è®¡ç®—å¿«é€Ÿ\n")
            fig2 = self.plot_marginal_effects(X, y, features_to_plot, method='marginal')
            
            return fig1, fig2


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - è¾¹é™…æ•ˆåº”åˆ†æ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SimpleN2OPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    
    if not os.path.exists(data_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_file}")
        return
    
    # åŠ è½½æ•°æ®
    print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    X, y = predictor.load_and_preprocess_data(data_file)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒæ¨¡å‹...")
    predictor.train_model(X, y)
    
    # ç»˜åˆ¶è¾¹é™…æ•ˆåº”å›¾
    print("\n3. è¾¹é™…æ•ˆåº”åˆ†æ...")
    features_to_analyze = ['Log1p_Lake_area', 'crp_pc_vse', 'ari_ix_lav', 
                          'Elevation', 'Log1p_Population_Density', 'run_mm_vyr']
    
    # é€‰æ‹©æ–¹æ³•ï¼š'pdp' æˆ– 'marginal'
    # æ¨èä½¿ç”¨ 'pdp' æ–¹æ³•ï¼ˆæ›´ç§‘å­¦ç¨³å¥ï¼‰
    fig = predictor.plot_marginal_effects(X, y, features_to_plot=features_to_analyze, method='marginal')
    
    # å¦‚æœæƒ³å¯¹æ¯”ä¸¤ç§æ–¹æ³•ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š
    #fig1, fig2 = predictor.compare_methods(X, y, features_to_plot=features_to_analyze)
    
    print("\n" + "="*60)
    print("åˆ†æå®Œæˆ!")
    print("="*60)
    
    return predictor

if __name__ == "__main__":
    predictor = main()


#%% æ•£ç‚¹å›¾åˆ†æ 0816


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è¯»å–æ•°æ®
data = pd.read_csv("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(data)}")

# æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
required_columns = ['N2O', 'Population_Density', 'Lake_area']
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print(f"è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_columns}")
    print(f"å¯ç”¨åˆ—: {list(data.columns)}")

# è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆä»…å¯¹N2Oè¿›è¡Œå¼‚å¸¸å€¼è¿‡æ»¤ï¼‰
data_filtered = data[
    (data['N2O'] > data['N2O'].quantile(0.01)) & 
    (data['N2O'] < data['N2O'].quantile(0.99)) & 
    (data['Population_Density'] > data['Population_Density'].quantile(0.01)) &
    (data['Population_Density'] < data['Population_Density'].quantile(0.99))
].copy()
print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")

# å¯¹æ•°å˜æ¢æ‰€æœ‰å˜é‡
data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
data_filtered['Log_Population_Density'] = np.log10(data_filtered['Population_Density'] + 1e-10)
data_filtered['Log_Lake_area'] = np.log10(data_filtered['Lake_area'] + 1e-10)

# å‡†å¤‡å˜é‡ï¼ˆä½¿ç”¨å¯¹æ•°å˜æ¢åçš„æ•°æ®ï¼‰
X = data_filtered['Log_Population_Density']
y = data_filtered['Log_N2O']
colors = data_filtered['Log_Lake_area']

# åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
before_drop = len(X)
complete_cases = X.notna() & y.notna() & colors.notna()
X = X[complete_cases]
y = y[complete_cases]
colors = colors[complete_cases]
after_drop = len(X)

if before_drop != after_drop:
    print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
else:
    print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")

# åˆ›å»ºå›¾å½¢
plt.figure(figsize=(10, 8))

# åˆ›å»ºæ•£ç‚¹å›¾ï¼Œé¢œè‰²æ˜ å°„Lake_area
scatter = plt.scatter(X, y, c=colors, cmap='viridis', 
                     alpha=0.7, s=50, edgecolors='white', linewidth=0.5)

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(scatter)
cbar.set_label('Logâ‚â‚€(Lake Area)', fontsize=12, fontweight='bold')

# è®¡ç®—å¹¶ç»˜åˆ¶æ‹Ÿåˆçº¿
if len(X) > 1:
    # æ•°æ®å·²ç»æ˜¯å¯¹æ•°å˜æ¢åçš„ï¼Œç›´æ¥è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)
    
    # ç”Ÿæˆæ‹Ÿåˆçº¿
    x_fit = np.linspace(X.min(), X.max(), 100)
    y_fit = slope * x_fit + intercept
    
    plt.plot(x_fit, y_fit, 'r-', linewidth=2, alpha=0.8, 
             label=f'æ‹Ÿåˆçº¿ (RÂ² = {r_value**2:.3f}, p = {p_value:.3f})')
    
    # æ·»åŠ 95%ç½®ä¿¡åŒºé—´
    from scipy.stats import t
    n = len(X)
    dof = n - 2  # è‡ªç”±åº¦
    t_val = t.ppf(0.975, dof)  # 95%ç½®ä¿¡åŒºé—´çš„tå€¼
    
    # è®¡ç®—æ ‡å‡†è¯¯å·®
    residuals = y - (slope * X + intercept)
    mse = np.sum(residuals**2) / dof
    se = np.sqrt(mse * (1/n + (x_fit - X.mean())**2 / np.sum((X - X.mean())**2)))
    
    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
    ci = t_val * se
    plt.fill_between(x_fit, y_fit - ci, y_fit + ci, alpha=0.2, color='red',
                     label='95% ç½®ä¿¡åŒºé—´')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
plt.xlabel('Logâ‚â‚€(Population Density)', fontsize=14, fontweight='bold')
plt.ylabel('Logâ‚â‚€(Nâ‚‚O) (Î¼mol/mÂ²/yr)', fontsize=14, fontweight='bold')
plt.title('Log-transformed Nâ‚‚O Emissions vs Population Density\n(Color gradient represents Logâ‚â‚€(Lake Area))', 
          fontsize=16, fontweight='bold', pad=20)

# è®¾ç½®åæ ‡è½´
plt.grid(True, alpha=0.3)
plt.tight_layout()

# æ·»åŠ å›¾ä¾‹
if 'label' in locals():
    plt.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)

# å¯é€‰ï¼šå¦‚æœéœ€è¦ï¼Œå¯ä»¥è°ƒæ•´åæ ‡è½´èŒƒå›´
# plt.xlim([X.min() - 0.1, X.max() + 0.1])
# plt.ylim([y.min() - 0.1, y.max() + 0.1])

# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
textstr = f'æ ·æœ¬æ•°: {len(X)}\n' + \
          f'Logâ‚â‚€(Nâ‚‚O) èŒƒå›´: {y.min():.2f} - {y.max():.2f}\n' + \
          f'Logâ‚â‚€(äººå£å¯†åº¦) èŒƒå›´: {X.min():.2f} - {X.max():.2f}'
props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
         verticalalignment='top', bbox=props)

plt.show()

# æ‰“å°ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\n=== å¯¹æ•°å˜æ¢åæ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"Logâ‚â‚€(N2O) æè¿°ç»Ÿè®¡:")
print(y.describe())
print(f"\nLogâ‚â‚€(Population_Density) æè¿°ç»Ÿè®¡:")
print(X.describe())
print(f"\nLogâ‚â‚€(Lake_area) æè¿°ç»Ÿè®¡:")
print(colors.describe())

# æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå¯¹æ¯”
print("\n=== åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"åŸå§‹ N2O æè¿°ç»Ÿè®¡:")
print(data_filtered['N2O'].describe())
print(f"\nåŸå§‹ Population_Density æè¿°ç»Ÿè®¡:")
print(data_filtered['Population_Density'].describe())
print(f"\nåŸå§‹ Lake_area æè¿°ç»Ÿè®¡:")
print(data_filtered['Lake_area'].describe())

# è®¡ç®—ç›¸å…³ç³»æ•°
correlation = np.corrcoef(X, y)[0, 1]
print(f"\nPearsonç›¸å…³ç³»æ•°: {correlation:.4f}")

# Spearmanç›¸å…³ç³»æ•°ï¼ˆå¯¹éçº¿æ€§å…³ç³»æ›´æ•æ„Ÿï¼‰
spearman_corr, spearman_p = stats.spearmanr(X, y)
print(f"Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f} (p-value: {spearman_p:.4f})")

#%% äººå£å¯†åº¦ä»¥åŠTPè´Ÿè· vs N2O


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è¯»å–æ•°æ®
data = pd.read_csv("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(data)}")

# æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
required_columns = ['N2O', 'TP_Load_Per_Volume', 'Lake_area']
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print(f"è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_columns}")
    print(f"å¯ç”¨åˆ—: {list(data.columns)}")

# æ£€æŸ¥TP_Load_Per_Volumeçš„æ•°æ®è´¨é‡
print(f"\nTP_Load_Per_Volume æ•°æ®æ¦‚è§ˆ:")
print(f"æ€»æ•°æ®é‡: {len(data)}")
print(f"éç©ºå€¼æ•°é‡: {data['TP_Load_Per_Volume'].notna().sum()}")
print(f"ç¼ºå¤±å€¼æ•°é‡: {data['TP_Load_Per_Volume'].isna().sum()}")
print(f"é›¶å€¼æ•°é‡: {(data['TP_Load_Per_Volume'] == 0).sum()}")
print(f"è´Ÿå€¼æ•°é‡: {(data['TP_Load_Per_Volume'] < 0).sum()}")

# è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆå¯¹N2Oå’ŒTP_Load_Per_Volumeè¿›è¡Œå¼‚å¸¸å€¼è¿‡æ»¤ï¼‰
# é¦–å…ˆè¿‡æ»¤æ‰è´Ÿå€¼å’Œé›¶å€¼ï¼ˆå› ä¸ºè¦åšå¯¹æ•°å˜æ¢ï¼‰
data_positive = data[
    (data['N2O'] > 0) & 
    (data['TP_Load_Per_Volume'] > 0) &
    (data['Lake_area'] > 0)
].copy()

print(f"è¿‡æ»¤è´Ÿå€¼å’Œé›¶å€¼åæ•°æ®é‡: {len(data_positive)}")

# å†è¿‡æ»¤æç«¯å¼‚å¸¸å€¼
data_filtered = data_positive[
    (data_positive['N2O'] > data_positive['N2O'].quantile(0.01)) & 
    (data_positive['N2O'] < data_positive['N2O'].quantile(0.99)) & 
    (data_positive['TP_Load_Per_Volume'] > data_positive['TP_Load_Per_Volume'].quantile(0.01)) &
    (data_positive['TP_Load_Per_Volume'] < data_positive['TP_Load_Per_Volume'].quantile(0.99))
].copy()
print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")

# å¯¹æ•°å˜æ¢æ‰€æœ‰å˜é‡
data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'])
data_filtered['Log_TP_Load_Per_Volume'] = np.log10(data_filtered['TP_Load_Per_Volume'])
data_filtered['Log_Lake_area'] = np.log10(data_filtered['Lake_area'])

# å‡†å¤‡å˜é‡ï¼ˆä½¿ç”¨å¯¹æ•°å˜æ¢åçš„æ•°æ®ï¼‰
X = data_filtered['Log_TP_Load_Per_Volume']
y = data_filtered['Log_N2O']
colors = data_filtered['Log_Lake_area']

# åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
before_drop = len(X)
complete_cases = X.notna() & y.notna() & colors.notna()
X = X[complete_cases]
y = y[complete_cases]
colors = colors[complete_cases]
after_drop = len(X)

if before_drop != after_drop:
    print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
else:
    print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")

# åˆ›å»ºå›¾å½¢
plt.figure(figsize=(10, 8))

# åˆ›å»ºæ•£ç‚¹å›¾ï¼Œé¢œè‰²æ˜ å°„Lake_area
scatter = plt.scatter(X, y, c=colors, cmap='viridis', 
                     alpha=0.7, s=50, edgecolors='white', linewidth=0.5)

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(scatter)
cbar.set_label('Logâ‚â‚€(Lake Area)', fontsize=12, fontweight='bold')

# è®¡ç®—å¹¶ç»˜åˆ¶æ‹Ÿåˆçº¿
if len(X) > 1:
    # æ•°æ®å·²ç»æ˜¯å¯¹æ•°å˜æ¢åçš„ï¼Œç›´æ¥è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)
    
    # ç”Ÿæˆæ‹Ÿåˆçº¿
    x_fit = np.linspace(X.min(), X.max(), 100)
    y_fit = slope * x_fit + intercept
    
    plt.plot(x_fit, y_fit, 'r-', linewidth=2, alpha=0.8, 
             label=f'æ‹Ÿåˆçº¿ (RÂ² = {r_value**2:.3f}, p = {p_value:.3f})')
    
    # æ·»åŠ 95%ç½®ä¿¡åŒºé—´
    from scipy.stats import t
    n = len(X)
    dof = n - 2  # è‡ªç”±åº¦
    t_val = t.ppf(0.975, dof)  # 95%ç½®ä¿¡åŒºé—´çš„tå€¼
    
    # è®¡ç®—æ ‡å‡†è¯¯å·®
    residuals = y - (slope * X + intercept)
    mse = np.sum(residuals**2) / dof
    se = np.sqrt(mse * (1/n + (x_fit - X.mean())**2 / np.sum((X - X.mean())**2)))
    
    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
    ci = t_val * se
    plt.fill_between(x_fit, y_fit - ci, y_fit + ci, alpha=0.2, color='red',
                     label='95% ç½®ä¿¡åŒºé—´')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
plt.xlabel('Logâ‚â‚€(TP Load Per Volume)', fontsize=14, fontweight='bold')
plt.ylabel('Logâ‚â‚€(Nâ‚‚O) (Î¼mol/mÂ²/yr)', fontsize=14, fontweight='bold')
plt.title('Log-transformed Nâ‚‚O Emissions vs TP Load Per Volume\n(Color gradient represents Logâ‚â‚€(Lake Area))', 
          fontsize=16, fontweight='bold', pad=20)

# è®¾ç½®åæ ‡è½´
plt.grid(True, alpha=0.3)
plt.tight_layout()

# æ·»åŠ å›¾ä¾‹
plt.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)

# æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
textstr = f'æ ·æœ¬æ•°: {len(X)}\n' + \
          f'RÂ²: {r_value**2:.3f}\n' + \
          f'Logâ‚â‚€(Nâ‚‚O) èŒƒå›´: {y.min():.2f} - {y.max():.2f}\n' + \
          f'Logâ‚â‚€(TPè´Ÿè·/ä½“ç§¯) èŒƒå›´: {X.min():.2f} - {X.max():.2f}'
props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
         verticalalignment='top', bbox=props)

plt.show()

# æ‰“å°ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\n=== å¯¹æ•°å˜æ¢åæ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"Logâ‚â‚€(N2O) æè¿°ç»Ÿè®¡:")
print(y.describe())
print(f"\nLogâ‚â‚€(TP_Load_Per_Volume) æè¿°ç»Ÿè®¡:")
print(X.describe())
print(f"\nLogâ‚â‚€(Lake_area) æè¿°ç»Ÿè®¡:")
print(colors.describe())

# æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå¯¹æ¯”
print("\n=== åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"åŸå§‹ N2O æè¿°ç»Ÿè®¡:")
print(data_filtered['N2O'].describe())
print(f"\nåŸå§‹ TP_Load_Per_Volume æè¿°ç»Ÿè®¡:")
print(data_filtered['TP_Load_Per_Volume'].describe())
print(f"\nåŸå§‹ Lake_area æè¿°ç»Ÿè®¡:")
print(data_filtered['Lake_area'].describe())

# è®¡ç®—ç›¸å…³ç³»æ•°
correlation = np.corrcoef(X, y)[0, 1]
print(f"\nPearsonç›¸å…³ç³»æ•°: {correlation:.4f}")

# Spearmanç›¸å…³ç³»æ•°ï¼ˆå¯¹éçº¿æ€§å…³ç³»æ›´æ•æ„Ÿï¼‰
spearman_corr, spearman_p = stats.spearmanr(X, y)
print(f"Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f} (p-value: {spearman_p:.4f})")

# é¢å¤–åˆ†æï¼šæ¯”è¾ƒä¸äººå£å¯†åº¦çš„ç›¸å…³æ€§å·®å¼‚
if 'Population_Density' in data.columns:
    # é‡æ–°å¤„ç†äººå£å¯†åº¦æ•°æ®è¿›è¡Œæ¯”è¾ƒ
    data_pop = data_filtered[data_filtered['Population_Density'] > 0].copy()
    if len(data_pop) > 0:
        data_pop['Log_Population_Density'] = np.log10(data_pop['Population_Density'])
        
        # è®¡ç®—ä¸äººå£å¯†åº¦çš„ç›¸å…³æ€§ï¼ˆä½¿ç”¨ç›¸åŒçš„æ ·æœ¬ï¼‰
        common_indices = data_pop.index.intersection(data_filtered.index)
        if len(common_indices) > 10:  # è‡³å°‘éœ€è¦è¶³å¤Ÿçš„æ ·æœ¬
            pop_x = data_pop.loc[common_indices, 'Log_Population_Density']
            tp_x = data_filtered.loc[common_indices, 'Log_TP_Load_Per_Volume']
            n2o_y = data_filtered.loc[common_indices, 'Log_N2O']
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æœ‰æ•ˆ
            valid_mask = pop_x.notna() & tp_x.notna() & n2o_y.notna()
            if valid_mask.sum() > 10:
                pop_corr = np.corrcoef(pop_x[valid_mask], n2o_y[valid_mask])[0, 1]
                tp_corr = np.corrcoef(tp_x[valid_mask], n2o_y[valid_mask])[0, 1]
                
                print(f"\n=== ç›¸å…³æ€§æ¯”è¾ƒ ===")
                print(f"N2Oä¸äººå£å¯†åº¦çš„ç›¸å…³ç³»æ•°: {pop_corr:.4f}")
                print(f"N2Oä¸TPè´Ÿè·/ä½“ç§¯çš„ç›¸å…³ç³»æ•°: {tp_corr:.4f}")
                print(f"TPè´Ÿè·/ä½“ç§¯çš„è§£é‡ŠåŠ›æ›´å¼º: {'æ˜¯' if abs(tp_corr) > abs(pop_corr) else 'å¦'}")

print(f"\n=== åˆ†ææ€»ç»“ ===")
print(f"æœ€ç»ˆåˆ†ææ ·æœ¬æ•°: {len(X)}")
print(f"TPè´Ÿè·/ä½“ç§¯ä¸N2Oæ’æ”¾çš„çº¿æ€§ç›¸å…³æ€§: {correlation:.4f}")
print(f"å†³å®šç³»æ•°RÂ²: {r_value**2:.3f} ({r_value**2*100:.1f}%çš„å˜å¼‚å¯è¢«è§£é‡Š)")
if p_value < 0.001:
    print("ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.001 (é«˜åº¦æ˜¾è‘—)")
elif p_value < 0.05:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (æ˜¾è‘—)")
else:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (ä¸æ˜¾è‘—)")

#%% äººå£å¯†åº¦vsN2O è‰²é˜¶å‘ˆç° TPè´Ÿè· 0817


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è¯»å–æ•°æ®
data = pd.read_csv("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(data)}")

# æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
required_columns = ['N2O', 'Population_Density', 'TP_Load_Per_Volume']
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print(f"è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_columns}")
    print(f"å¯ç”¨åˆ—: {list(data.columns)}")

# æ£€æŸ¥TP_Load_Per_Volumeçš„æ•°æ®è´¨é‡
print(f"\nTP_Load_Per_Volume æ•°æ®æ¦‚è§ˆ:")
print(f"æ€»æ•°æ®é‡: {len(data)}")
print(f"éç©ºå€¼æ•°é‡: {data['TP_Load_Per_Volume'].notna().sum()}")
print(f"ç¼ºå¤±å€¼æ•°é‡: {data['TP_Load_Per_Volume'].isna().sum()}")
print(f"é›¶å€¼æ•°é‡: {(data['TP_Load_Per_Volume'] == 0).sum()}")
print(f"è´Ÿå€¼æ•°é‡: {(data['TP_Load_Per_Volume'] < 0).sum()}")

# è¿‡æ»¤å¼‚å¸¸å€¼
# é¦–å…ˆè¿‡æ»¤æ‰è´Ÿå€¼å’Œé›¶å€¼ï¼ˆå› ä¸ºè¦åšå¯¹æ•°å˜æ¢ï¼‰
data_positive = data[
    (data['N2O'] > 0) & 
    (data['Population_Density'] > 0) &
    (data['TP_Load_Per_Volume'] > 0)
].copy()

print(f"è¿‡æ»¤è´Ÿå€¼å’Œé›¶å€¼åæ•°æ®é‡: {len(data_positive)}")

# å†è¿‡æ»¤æç«¯å¼‚å¸¸å€¼
data_filtered = data_positive[
    (data_positive['N2O'] > data_positive['N2O'].quantile(0.01)) & 
    (data_positive['N2O'] < data_positive['N2O'].quantile(0.99)) & 
    (data_positive['Population_Density'] > data_positive['Population_Density'].quantile(0.01)) &
    (data_positive['Population_Density'] < data_positive['Population_Density'].quantile(0.99)) &
    (data_positive['TP_Load_Per_Volume'] > data_positive['TP_Load_Per_Volume'].quantile(0.01)) &
    (data_positive['TP_Load_Per_Volume'] < data_positive['TP_Load_Per_Volume'].quantile(0.99))
].copy()
print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")

# å¯¹æ•°å˜æ¢æ‰€æœ‰å˜é‡
data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'])
data_filtered['Log_Population_Density'] = np.log10(data_filtered['Population_Density'])
data_filtered['Log_TP_Load_Per_Volume'] = np.log10(data_filtered['TP_Load_Per_Volume'])

# å‡†å¤‡å˜é‡ï¼ˆXè½´ï¼šäººå£å¯†åº¦ï¼ŒYè½´ï¼šN2Oï¼Œé¢œè‰²ï¼šTPè´Ÿè·ï¼‰
X = data_filtered['Log_Population_Density']
y = data_filtered['Log_N2O']
colors = data_filtered['Log_TP_Load_Per_Volume']

# åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
before_drop = len(X)
complete_cases = X.notna() & y.notna() & colors.notna()
X = X[complete_cases]
y = y[complete_cases]
colors = colors[complete_cases]
after_drop = len(X)

if before_drop != after_drop:
    print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
else:
    print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")

# åˆ›å»ºå›¾å½¢
plt.figure(figsize=(10, 8))

# åˆ›å»ºæ•£ç‚¹å›¾ï¼Œä½¿ç”¨RdBu_ré¢œè‰²æ˜ å°„
scatter = plt.scatter(X, y, c=colors, cmap='RdBu_r', 
                     alpha=0.7, s=50, edgecolors='white', linewidth=0.5)

# # æ·»åŠ é¢œè‰²æ¡ï¼Œè®¾ç½®æ›´å°çš„å°ºå¯¸
# cbar = plt.colorbar(scatter, shrink=0.6, aspect=20)
# cbar.set_label('Logâ‚â‚€(TP Load)', fontsize=10, fontweight='bold')

# æ·»åŠ é¢œè‰²æ¡ï¼Œè®¾ç½®ä½ç½®åœ¨å·¦ä¸Šè§’
cbar = plt.colorbar(scatter, shrink=0.3, aspect=10, pad=0.02, anchor=(0, 1.0))
cbar.set_label('Logâ‚â‚€(TP Load)', fontsize=10, fontweight='bold')

# è®¡ç®—å¹¶ç»˜åˆ¶æ‹Ÿåˆçº¿
if len(X) > 1:
    # æ•°æ®å·²ç»æ˜¯å¯¹æ•°å˜æ¢åçš„ï¼Œç›´æ¥è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)
    
    # ç”Ÿæˆæ‹Ÿåˆçº¿
    x_fit = np.linspace(X.min(), X.max(), 100)
    y_fit = slope * x_fit + intercept
    
    # ä½¿ç”¨æ›´æ·¡æ›´ä¼˜é›…çš„æµ…çº¢è‰²
    fit_color = '#FF8A80'  # æ·¡é›…çš„æµ…çº¢è‰²
    ci_color = '#FFCDD2'   # æ›´æ·¡çš„çº¢è‰²ç”¨äºç½®ä¿¡åŒºé—´
    
    plt.plot(x_fit, y_fit, color=fit_color, linewidth=2, alpha=0.8)
    
    # æ·»åŠ 95%ç½®ä¿¡åŒºé—´ï¼ˆä¸æ·»åŠ åˆ°å›¾ä¾‹ï¼‰
    from scipy.stats import t
    n = len(X)
    dof = n - 2  # è‡ªç”±åº¦
    t_val = t.ppf(0.975, dof)  # 95%ç½®ä¿¡åŒºé—´çš„tå€¼
    
    # è®¡ç®—æ ‡å‡†è¯¯å·®
    residuals = y - (slope * X + intercept)
    mse = np.sum(residuals**2) / dof
    se = np.sqrt(mse * (1/n + (x_fit - X.mean())**2 / np.sum((X - X.mean())**2)))
    
    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´
    ci = t_val * se
    plt.fill_between(x_fit, y_fit - ci, y_fit + ci, alpha=0.2, color='red')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
plt.xlabel('Logâ‚â‚€(Population Density)', fontsize=14)
plt.ylabel('Logâ‚â‚€(Nâ‚‚O) (mg N mâ»Â² dâ»Â¹)', fontsize=14)
plt.title('Log-transformed Nâ‚‚O Emissions vs Population Density\n(Color gradient represents Logâ‚â‚€(TP Load Per Volume))', 
          fontsize=16, pad=20)

# è®¾ç½®åæ ‡è½´
plt.grid(True, alpha=0.3)

# è°ƒæ•´åæ ‡è½´èŒƒå›´ä»¥å¢å¼ºæ–œç‡è§†è§‰æ•ˆæœ
x_range = X.max() - X.min()
y_range = y.max() - y.min()
x_margin = x_range * 0.05
y_margin = y_range * 0.15

plt.xlim(X.min() - x_margin, X.max() + x_margin)
plt.ylim(y.min() - y_margin, y.max() + y_margin)

plt.tight_layout()


# ä¿å­˜å›¾ç‰‡
plt.savefig('Log-transformed Nâ‚‚O Emissions vs Population Density.png', dpi=600, bbox_inches='tight')
plt.close()

# ç§»é™¤åŸæ¥çš„å›¾ä¾‹ä»£ç 

# # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†ï¼ŒåŒ…å«å›å½’ä¿¡æ¯
# textstr = f'æ‹Ÿåˆçº¿ (RÂ² = {r_value**2:.3f}, p = {p_value:.3f})\n' + \
#           f'95% ç½®ä¿¡åŒºé—´\n' + \
#           f'æ ·æœ¬æ•°: {len(X)}\n' + \
#           f'Logâ‚â‚€(Nâ‚‚O) èŒƒå›´: {y.min():.2f} - {y.max():.2f}\n' + \
#           f'Logâ‚â‚€(äººå£å¯†åº¦) èŒƒå›´: {X.min():.2f} - {X.max():.2f}'
# props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
# plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
#          verticalalignment='top', bbox=props)

# plt.show()

# æ‰“å°ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\n=== å¯¹æ•°å˜æ¢åæ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"Logâ‚â‚€(N2O) æè¿°ç»Ÿè®¡:")
print(y.describe())
print(f"\nLogâ‚â‚€(Population_Density) æè¿°ç»Ÿè®¡:")
print(X.describe())
print(f"\nLogâ‚â‚€(TP_Load_Per_Volume) æè¿°ç»Ÿè®¡:")
print(colors.describe())

# æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå¯¹æ¯”
print("\n=== åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"åŸå§‹ N2O æè¿°ç»Ÿè®¡:")
print(data_filtered['N2O'].describe())
print(f"\nåŸå§‹ Population_Density æè¿°ç»Ÿè®¡:")
print(data_filtered['Population_Density'].describe())
print(f"\nåŸå§‹ TP_Load_Per_Volume æè¿°ç»Ÿè®¡:")
print(data_filtered['TP_Load_Per_Volume'].describe())

# è®¡ç®—ç›¸å…³ç³»æ•°
correlation = np.corrcoef(X, y)[0, 1]
print(f"\nPearsonç›¸å…³ç³»æ•°: {correlation:.4f}")

# Spearmanç›¸å…³ç³»æ•°ï¼ˆå¯¹éçº¿æ€§å…³ç³»æ›´æ•æ„Ÿï¼‰
spearman_corr, spearman_p = stats.spearmanr(X, y)
print(f"Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f} (p-value: {spearman_p:.4f})")

# é¢å¤–åˆ†æï¼šæ¯”è¾ƒä¸TPè´Ÿè·çš„ç›¸å…³æ€§å·®å¼‚
print(f"\n=== å¤šå˜é‡ç›¸å…³æ€§åˆ†æ ===")
# è®¡ç®—äººå£å¯†åº¦ä¸N2Oçš„ç›¸å…³æ€§ï¼ˆä¸»è¦å…³ç³»ï¼‰
pop_n2o_corr = np.corrcoef(X, y)[0, 1]
print(f"äººå£å¯†åº¦ä¸N2Oçš„ç›¸å…³ç³»æ•°: {pop_n2o_corr:.4f}")

# è®¡ç®—TPè´Ÿè·ä¸N2Oçš„ç›¸å…³æ€§
tp_n2o_corr = np.corrcoef(colors, y)[0, 1]
print(f"TPè´Ÿè·ä¸N2Oçš„ç›¸å…³ç³»æ•°: {tp_n2o_corr:.4f}")

# è®¡ç®—äººå£å¯†åº¦ä¸TPè´Ÿè·çš„ç›¸å…³æ€§
pop_tp_corr = np.corrcoef(X, colors)[0, 1]
print(f"äººå£å¯†åº¦ä¸TPè´Ÿè·çš„ç›¸å…³ç³»æ•°: {pop_tp_corr:.4f}")

# åç›¸å…³åˆ†ææç¤º
print(f"\n=== å˜é‡å…³ç³»å¼ºåº¦æ¯”è¾ƒ ===")
print(f"æœ€å¼ºç›¸å…³å…³ç³»: ", end="")
correlations = {
    "äººå£å¯†åº¦-N2O": abs(pop_n2o_corr),
    "TPè´Ÿè·-N2O": abs(tp_n2o_corr),
    "äººå£å¯†åº¦-TPè´Ÿè·": abs(pop_tp_corr)
}
strongest = max(correlations, key=correlations.get)
print(f"{strongest} (r = {correlations[strongest]:.4f})")

print(f"\nå»ºè®®: è€ƒè™‘åˆ°äººå£å¯†åº¦ä¸TPè´Ÿè·ä¹‹é—´çš„ç›¸å…³æ€§ä¸º {pop_tp_corr:.4f}")
if abs(pop_tp_corr) > 0.3:
    print("ä¸¤ä¸ªé¢„æµ‹å˜é‡é—´å­˜åœ¨ä¸­ç­‰ç¨‹åº¦ç›¸å…³ï¼Œå»ºè®®è¿›è¡Œå¤šå…ƒå›å½’åˆ†æ")
else:
    print("ä¸¤ä¸ªé¢„æµ‹å˜é‡é—´ç›¸å…³æ€§è¾ƒå¼±ï¼Œå¯åˆ†åˆ«ä½œä¸ºç‹¬ç«‹é¢„æµ‹å› å­")

print(f"\n=== åˆ†ææ€»ç»“ ===")
print(f"æœ€ç»ˆåˆ†ææ ·æœ¬æ•°: {len(X)}")
print(f"äººå£å¯†åº¦ä¸N2Oæ’æ”¾çš„çº¿æ€§ç›¸å…³æ€§: {correlation:.4f}")
print(f"å†³å®šç³»æ•°RÂ²: {r_value**2:.3f} ({r_value**2*100:.1f}%çš„N2Oå˜å¼‚å¯è¢«äººå£å¯†åº¦è§£é‡Š)")
if p_value < 0.001:
    print("ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.001 (é«˜åº¦æ˜¾è‘—)")
elif p_value < 0.05:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (æ˜¾è‘—)")
else:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (ä¸æ˜¾è‘—)")
    
print(f"\né¢œè‰²æ¢¯åº¦ä¿¡æ¯:")
print(f"TPè´Ÿè·èŒƒå›´: {colors.min():.2f} - {colors.max():.2f} (å¯¹æ•°å°ºåº¦)")
print(f"å›¾ä¸­é¢œè‰²è¶Šæ·±(é»„è‰²)è¡¨ç¤ºTPè´Ÿè·è¶Šé«˜ï¼Œé¢œè‰²è¶Šæµ…(ç´«è‰²)è¡¨ç¤ºTPè´Ÿè·è¶Šä½")



#%% äººå£å¯†åº¦vsN2O è‰²é˜¶å‘ˆç° TPè´Ÿè· å–colorbar  0817

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è¯»å–æ•°æ®
data = pd.read_csv("GHGdata_LakeATLAS_final250714_cleaned_imputation.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(data)}")

# æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
required_columns = ['N2O', 'Population_Density', 'TP_Load_Per_Volume']
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print(f"è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_columns}")
    print(f"å¯ç”¨åˆ—: {list(data.columns)}")

# æ£€æŸ¥TP_Load_Per_Volumeçš„æ•°æ®è´¨é‡
print(f"\nTP_Load_Per_Volume æ•°æ®æ¦‚è§ˆ:")
print(f"æ€»æ•°æ®é‡: {len(data)}")
print(f"éç©ºå€¼æ•°é‡: {data['TP_Load_Per_Volume'].notna().sum()}")
print(f"ç¼ºå¤±å€¼æ•°é‡: {data['TP_Load_Per_Volume'].isna().sum()}")
print(f"é›¶å€¼æ•°é‡: {(data['TP_Load_Per_Volume'] == 0).sum()}")
print(f"è´Ÿå€¼æ•°é‡: {(data['TP_Load_Per_Volume'] < 0).sum()}")

# è¿‡æ»¤å¼‚å¸¸å€¼
# é¦–å…ˆè¿‡æ»¤æ‰è´Ÿå€¼å’Œé›¶å€¼ï¼ˆå› ä¸ºè¦åšå¯¹æ•°å˜æ¢ï¼‰
data_positive = data[
    (data['N2O'] > 0) & 
    (data['Population_Density'] > 0) &
    (data['TP_Load_Per_Volume'] > 0)
].copy()

print(f"è¿‡æ»¤è´Ÿå€¼å’Œé›¶å€¼åæ•°æ®é‡: {len(data_positive)}")

# å†è¿‡æ»¤æç«¯å¼‚å¸¸å€¼
data_filtered = data_positive[
    (data_positive['N2O'] > data_positive['N2O'].quantile(0.01)) & 
    (data_positive['N2O'] < data_positive['N2O'].quantile(0.99)) & 
    (data_positive['Population_Density'] > data_positive['Population_Density'].quantile(0.01)) &
    (data_positive['Population_Density'] < data_positive['Population_Density'].quantile(0.99)) &
    (data_positive['TP_Load_Per_Volume'] > data_positive['TP_Load_Per_Volume'].quantile(0.01)) &
    (data_positive['TP_Load_Per_Volume'] < data_positive['TP_Load_Per_Volume'].quantile(0.99))
].copy()
print(f"è¿‡æ»¤å¼‚å¸¸å€¼åæ•°æ®é‡: {len(data_filtered)}")

# å¯¹æ•°å˜æ¢æ‰€æœ‰å˜é‡
data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'])
data_filtered['Log_Population_Density'] = np.log10(data_filtered['Population_Density'])
data_filtered['Log_TP_Load_Per_Volume'] = np.log10(data_filtered['TP_Load_Per_Volume'])

# å‡†å¤‡å˜é‡ï¼ˆXè½´ï¼šäººå£å¯†åº¦ï¼ŒYè½´ï¼šN2Oï¼Œé¢œè‰²ï¼šTPè´Ÿè·ï¼‰
X = data_filtered['Log_Population_Density']
y = data_filtered['Log_N2O']
colors = data_filtered['Log_TP_Load_Per_Volume']

# åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
before_drop = len(X)
complete_cases = X.notna() & y.notna() & colors.notna()
X = X[complete_cases]
y = y[complete_cases]
colors = colors[complete_cases]
after_drop = len(X)

if before_drop != after_drop:
    print(f"åˆ é™¤ç¼ºå¤±å€¼åæ•°æ®é‡: {after_drop} (åˆ é™¤äº†{before_drop - after_drop}è¡Œ)")
else:
    print(f"æ— éœ€åˆ é™¤ç¼ºå¤±å€¼ï¼Œæœ€ç»ˆæ•°æ®é‡: {after_drop}")

# åˆ›å»ºå›¾å½¢
fig, ax = plt.subplots(figsize=(10, 8))

# åˆ›å»ºæ•£ç‚¹å›¾ï¼Œä½¿ç”¨RdBu_ré¢œè‰²æ˜ å°„
scatter = ax.scatter(X, y, c=colors, cmap='RdBu_r', 
                    alpha=0.7, s=50, edgecolors='white', linewidth=0.5)

# åˆ›å»ºæ•£ç‚¹å›¾åŒºåŸŸå·¦ä¸Šè§’çš„é¢œè‰²æ¡
from mpl_toolkits.axes_grid1.inset_locator import inset_axes
cbar_ax = inset_axes(ax, width="25%", height="4%", loc='upper left', 
                     bbox_to_anchor=(0.02, 0.98, 1, 1), bbox_transform=ax.transAxes, 
                     borderpad=0)
cbar = fig.colorbar(scatter, cax=cbar_ax, orientation='horizontal')
cbar.set_label('Logâ‚â‚€(TP Load Per Volume)', fontsize=9, fontweight='bold')
cbar.ax.tick_params(labelsize=8)

# è®¡ç®—å¹¶ç»˜åˆ¶æ‹Ÿåˆçº¿
if len(X) > 1:
    # æ•°æ®å·²ç»æ˜¯å¯¹æ•°å˜æ¢åçš„ï¼Œç›´æ¥è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)
    
    # ç”Ÿæˆæ‹Ÿåˆçº¿
    x_fit = np.linspace(X.min(), X.max(), 100)
    y_fit = slope * x_fit + intercept
    
    # ä½¿ç”¨æ›´ä¼˜é›…çš„æµ…çº¢è‰²
    elegant_light_red = '#FF6B6B'  # ä¼˜é›…çš„æµ…çº¢è‰²
    ax.plot(x_fit, y_fit, color=elegant_light_red, linewidth=2.5, alpha=0.9, label='æ‹Ÿåˆçº¿')
    
    # æ·»åŠ 95%ç½®ä¿¡åŒºé—´
    from scipy.stats import t
    n = len(X)
    dof = n - 2  # è‡ªç”±åº¦
    t_val = t.ppf(0.975, dof)  # 95%ç½®ä¿¡åŒºé—´çš„tå€¼
    
    # è®¡ç®—æ ‡å‡†è¯¯å·®
    residuals = y - (slope * X + intercept)
    mse = np.sum(residuals**2) / dof
    se = np.sqrt(mse * (1/n + (x_fit - X.mean())**2 / np.sum((X - X.mean())**2)))
    
    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´ï¼Œä½¿ç”¨æ›´æµ…çš„ä¼˜é›…çº¢è‰²
    ci = t_val * se
    ax.fill_between(x_fit, y_fit - ci, y_fit + ci, alpha=0.25, 
                   color=elegant_light_red, label='95% ç½®ä¿¡åŒºé—´')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('Logâ‚â‚€(Population Density)', fontsize=14, fontweight='bold')
ax.set_ylabel('Logâ‚â‚€(Nâ‚‚O) (Î¼mol/mÂ²/yr)', fontsize=14, fontweight='bold')
ax.set_title('Log-transformed Nâ‚‚O Emissions vs Population Density\n(Color gradient represents Logâ‚â‚€(TP Load Per Volume))', 
            fontsize=16, fontweight='bold', pad=20)

# è®¾ç½®åæ ‡è½´
ax.grid(True, alpha=0.3)

# è°ƒæ•´åæ ‡è½´èŒƒå›´ä»¥å¢å¼ºæ–œç‡è§†è§‰æ•ˆæœ
x_range = X.max() - X.min()
y_range = y.max() - y.min()
x_margin = x_range * 0.05
y_margin = y_range * 0.15

ax.set_xlim(X.min() - x_margin, X.max() + x_margin)
ax.set_ylim(y.min() - y_margin, y.max() + y_margin)

plt.tight_layout()

# ä¿å­˜å›¾ç‰‡
plt.savefig('Log-transformed Nâ‚‚O Emissions vs Population Density2.png', dpi=600, bbox_inches='tight')
plt.close()

# æ‰“å°ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\n=== å¯¹æ•°å˜æ¢åæ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"Logâ‚â‚€(N2O) æè¿°ç»Ÿè®¡:")
print(y.describe())
print(f"\nLogâ‚â‚€(Population_Density) æè¿°ç»Ÿè®¡:")
print(X.describe())
print(f"\nLogâ‚â‚€(TP_Load_Per_Volume) æè¿°ç»Ÿè®¡:")
print(colors.describe())

# æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºå¯¹æ¯”
print("\n=== åŸå§‹æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"åŸå§‹ N2O æè¿°ç»Ÿè®¡:")
print(data_filtered['N2O'].describe())
print(f"\nåŸå§‹ Population_Density æè¿°ç»Ÿè®¡:")
print(data_filtered['Population_Density'].describe())
print(f"\nåŸå§‹ TP_Load_Per_Volume æè¿°ç»Ÿè®¡:")
print(data_filtered['TP_Load_Per_Volume'].describe())

# è®¡ç®—ç›¸å…³ç³»æ•°
correlation = np.corrcoef(X, y)[0, 1]
print(f"\nPearsonç›¸å…³ç³»æ•°: {correlation:.4f}")

# Spearmanç›¸å…³ç³»æ•°ï¼ˆå¯¹éçº¿æ€§å…³ç³»æ›´æ•æ„Ÿï¼‰
spearman_corr, spearman_p = stats.spearmanr(X, y)
print(f"Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f} (p-value: {spearman_p:.4f})")

# é¢å¤–åˆ†æï¼šæ¯”è¾ƒä¸TPè´Ÿè·çš„ç›¸å…³æ€§å·®å¼‚
print(f"\n=== å¤šå˜é‡ç›¸å…³æ€§åˆ†æ ===")
# è®¡ç®—äººå£å¯†åº¦ä¸N2Oçš„ç›¸å…³æ€§ï¼ˆä¸»è¦å…³ç³»ï¼‰
pop_n2o_corr = np.corrcoef(X, y)[0, 1]
print(f"äººå£å¯†åº¦ä¸N2Oçš„ç›¸å…³ç³»æ•°: {pop_n2o_corr:.4f}")

# è®¡ç®—TPè´Ÿè·ä¸N2Oçš„ç›¸å…³æ€§
tp_n2o_corr = np.corrcoef(colors, y)[0, 1]
print(f"TPè´Ÿè·ä¸N2Oçš„ç›¸å…³ç³»æ•°: {tp_n2o_corr:.4f}")

# è®¡ç®—äººå£å¯†åº¦ä¸TPè´Ÿè·çš„ç›¸å…³æ€§
pop_tp_corr = np.corrcoef(X, colors)[0, 1]
print(f"äººå£å¯†åº¦ä¸TPè´Ÿè·çš„ç›¸å…³ç³»æ•°: {pop_tp_corr:.4f}")

# åç›¸å…³åˆ†ææç¤º
print(f"\n=== å˜é‡å…³ç³»å¼ºåº¦æ¯”è¾ƒ ===")
print(f"æœ€å¼ºç›¸å…³å…³ç³»: ", end="")
correlations = {
    "äººå£å¯†åº¦-N2O": abs(pop_n2o_corr),
    "TPè´Ÿè·-N2O": abs(tp_n2o_corr),
    "äººå£å¯†åº¦-TPè´Ÿè·": abs(pop_tp_corr)
}
strongest = max(correlations, key=correlations.get)
print(f"{strongest} (r = {correlations[strongest]:.4f})")

print(f"\nå»ºè®®: è€ƒè™‘åˆ°äººå£å¯†åº¦ä¸TPè´Ÿè·ä¹‹é—´çš„ç›¸å…³æ€§ä¸º {pop_tp_corr:.4f}")
if abs(pop_tp_corr) > 0.3:
    print("ä¸¤ä¸ªé¢„æµ‹å˜é‡é—´å­˜åœ¨ä¸­ç­‰ç¨‹åº¦ç›¸å…³ï¼Œå»ºè®®è¿›è¡Œå¤šå…ƒå›å½’åˆ†æ")
else:
    print("ä¸¤ä¸ªé¢„æµ‹å˜é‡é—´ç›¸å…³æ€§è¾ƒå¼±ï¼Œå¯åˆ†åˆ«ä½œä¸ºç‹¬ç«‹é¢„æµ‹å› å­")

print(f"\n=== åˆ†ææ€»ç»“ ===")
print(f"æœ€ç»ˆåˆ†ææ ·æœ¬æ•°: {len(X)}")
print(f"äººå£å¯†åº¦ä¸N2Oæ’æ”¾çš„çº¿æ€§ç›¸å…³æ€§: {correlation:.4f}")
print(f"å†³å®šç³»æ•°RÂ²: {r_value**2:.3f} ({r_value**2*100:.1f}%çš„N2Oå˜å¼‚å¯è¢«äººå£å¯†åº¦è§£é‡Š)")
if p_value < 0.001:
    print("ç»Ÿè®¡æ˜¾è‘—æ€§: p < 0.001 (é«˜åº¦æ˜¾è‘—)")
elif p_value < 0.05:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (æ˜¾è‘—)")
else:
    print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p = {p_value:.3f} (ä¸æ˜¾è‘—)")
    
print(f"\né¢œè‰²æ¢¯åº¦ä¿¡æ¯:")
print(f"TPè´Ÿè·èŒƒå›´: {colors.min():.2f} - {colors.max():.2f} (å¯¹æ•°å°ºåº¦)")
print(f"å›¾ä¸­é¢œè‰²è¶Šæ·±(é»„è‰²)è¡¨ç¤ºTPè´Ÿè·è¶Šé«˜ï¼Œé¢œè‰²è¶Šæµ…(ç´«è‰²)è¡¨ç¤ºTPè´Ÿè·è¶Šä½")



#%% SHAPåˆ†æ 0813


import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import RobustScaler
import matplotlib.pyplot as plt
import warnings
import pickle
from datetime import datetime
import shap
warnings.filterwarnings('ignore')

class N2OShapPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.best_params = None
        self.X = None  # ä¿å­˜è®­ç»ƒæ•°æ®ç”¨äºSHAPåˆ†æ
        self.y = None  # ä¿å­˜ç›®æ ‡å˜é‡ç”¨äºSHAPåˆ†æ
        
    def load_and_preprocess_data(self, filepath):
        """æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # ä½¿ç”¨RobustScalerè¿›è¡Œç¼©æ”¾
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        return X_scaled, y

    def train_model(self, X, y):
        """ä½¿ç”¨é¢„è®¾æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹"""
        
        # ä¿å­˜æ•°æ®ç”¨äºåç»­åˆ†æ
        self.X = X
        self.y = y
        
        # ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°
        best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        print(f"ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹:")
        print(f"å‚æ•°: {best_params}")
        
        # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
        rf_reg = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **best_params
        )
        
        print("è®­ç»ƒæ¨¡å‹...")
        rf_reg.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = rf_reg
        self.best_params = best_params
        
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"OOB Score: {rf_reg.oob_score_:.4f}")
        
        return self.best_model

    def evaluate_model(self, X_train, X_val, y_train, y_val):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        k_folds = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
        cv_scores = cross_val_score(self.best_model, X_train, y_train, cv=k_folds, scoring='r2')
        
        # å¯¹æ•°ç©ºé—´çš„é¢„æµ‹
        y_train_pred = self.best_model.predict(X_train)
        y_val_pred = self.best_model.predict(X_val)
        
        # å¯¹æ•°ç©ºé—´çš„R2
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)
        
        # åŸå§‹å°ºåº¦çš„RMSEè®¡ç®—
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
        
        # æ·»åŠ OOBåˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        oob_score = getattr(self.best_model, 'oob_score_', None)
        
        return {
            'cv_scores': cv_scores,
            'train_r2': train_r2,
            'val_r2': val_r2,
            'train_rmse': train_rmse,
            'val_rmse': val_rmse,
            'oob_score': oob_score,
            'y_val_true': y_val,
            'y_val_pred': y_val_pred
        }

    def clean_feature_name(self, feature_name):
        """
        æ¸…ç†ç‰¹å¾åç§°ï¼Œå°†Logå˜æ¢çš„å˜é‡åè½¬æ¢ä¸ºåŸå˜é‡å
        """
        if feature_name.startswith('Log1p_'):
            return feature_name.replace('Log1p_', '')
        else:
            return feature_name

    def shap_analysis_comprehensive(self, n_samples=1000, filename_prefix="shap_analysis"):
        """
        ç»¼åˆSHAPåˆ†æ - åŒ…å«å¤šç§SHAPå›¾è¡¨
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è¿›è¡ŒSHAPåˆ†æ...")
        print(f"ä½¿ç”¨æ ·æœ¬æ•°: {min(n_samples, len(self.X))}")
        
        # é€‰æ‹©æ ·æœ¬è¿›è¡ŒSHAPåˆ†æï¼ˆSHAPè®¡ç®—å¯èƒ½å¾ˆæ…¢ï¼Œæ‰€ä»¥é™åˆ¶æ ·æœ¬æ•°ï¼‰
        if len(self.X) > n_samples:
            sample_indices = np.random.RandomState(self.random_state).choice(
                len(self.X), n_samples, replace=False
            )
            X_sample = self.X.iloc[sample_indices]
            y_sample = self.y.iloc[sample_indices]
        else:
            X_sample = self.X
            y_sample = self.y
            
        print(f"å®é™…ä½¿ç”¨æ ·æœ¬æ•°: {len(X_sample)}")
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨ï¼ˆå¯¹äºéšæœºæ£®æ—ä½¿ç”¨TreeExplainerï¼‰
        print("åˆ›å»ºSHAPè§£é‡Šå™¨...")
        explainer = shap.TreeExplainer(self.best_model)
        
        # è®¡ç®—SHAPå€¼
        print("è®¡ç®—SHAPå€¼...")
        shap_values = explainer.shap_values(X_sample)
        
        # æ¸…ç†ç‰¹å¾åç§°
        clean_feature_names = [self.clean_feature_name(name) for name in X_sample.columns]
        
        # 1. Summary Plot (ç‰¹å¾é‡è¦æ€§æ¦‚è§ˆ)
        print("ç”ŸæˆSHAP Summary Plot...")
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_sample, 
                         feature_names=clean_feature_names,
                         show=False, max_display=20)
        plt.title('SHAP Summary Plot - Feature Importance and Impact Direction', 
                 fontsize=14, pad=20)
        plt.tight_layout()
        
        try:
            save_path = f"{filename_prefix}_summary.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"SHAP Summary Plotä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜Summary Plotæ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # 2. Bar Plot (å¹³å‡SHAPé‡è¦æ€§)
        print("ç”ŸæˆSHAP Bar Plot...")
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_sample, 
                         feature_names=clean_feature_names,
                         plot_type="bar", show=False, max_display=20)
        plt.title('SHAP Bar Plot - Mean Absolute SHAP Values', 
                 fontsize=14, pad=20)
        plt.tight_layout()
        
        try:
            save_path = f"{filename_prefix}_bar.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"SHAP Bar Plotä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜Bar Plotæ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # 3. è®¡ç®—å¹¶è¿”å›SHAPé‡è¦æ€§ç»Ÿè®¡
        shap_importance = pd.DataFrame({
            'feature': clean_feature_names,
            'mean_abs_shap': np.abs(shap_values).mean(0),
            'mean_shap': shap_values.mean(0),
            'std_shap': shap_values.std(0)
        })
        shap_importance = shap_importance.sort_values('mean_abs_shap', ascending=False)
        
        print("\nSHAPé‡è¦æ€§ç»Ÿè®¡ (å‰15ä¸ªç‰¹å¾):")
        print("-" * 70)
        print(f"{'Feature':<25} {'Mean|SHAP|':<12} {'Mean SHAP':<12} {'Std SHAP':<12}")
        print("-" * 70)
        for _, row in shap_importance.head(15).iterrows():
            print(f"{row['feature']:<25} {row['mean_abs_shap']:<12.6f} {row['mean_shap']:<12.6f} {row['std_shap']:<12.6f}")
        
        return shap_values, shap_importance

    def shap_dependence_plots(self, top_n_features=6, filename_prefix="shap_dependence"):
        """
        SHAPä¾èµ–å›¾ - æ˜¾ç¤ºç‰¹å¾ä¸SHAPå€¼çš„å…³ç³»
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨ç”ŸæˆSHAPä¾èµ–å›¾...")
        
        # é™åˆ¶æ ·æœ¬æ•°ä»¥æé«˜é€Ÿåº¦
        n_samples = min(1000, len(self.X))
        if len(self.X) > n_samples:
            sample_indices = np.random.RandomState(self.random_state).choice(
                len(self.X), n_samples, replace=False
            )
            X_sample = self.X.iloc[sample_indices]
        else:
            X_sample = self.X
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨å¹¶è®¡ç®—SHAPå€¼
        explainer = shap.TreeExplainer(self.best_model)
        shap_values = explainer.shap_values(X_sample)
        
        # è·å–æœ€é‡è¦çš„ç‰¹å¾
        mean_abs_shap = np.abs(shap_values).mean(0)
        top_feature_indices = np.argsort(mean_abs_shap)[-top_n_features:][::-1]
        
        # æ¸…ç†ç‰¹å¾åç§°
        clean_feature_names = [self.clean_feature_name(name) for name in X_sample.columns]
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, feature_idx in enumerate(top_feature_indices):
            if i >= len(axes):
                break
                
            feature_name = clean_feature_names[feature_idx]
            
            try:
                # ç”Ÿæˆä¾èµ–å›¾
                plt.sca(axes[i])
                shap.dependence_plot(feature_idx, shap_values, X_sample, 
                                   feature_names=clean_feature_names,
                                   show=False, ax=axes[i])
                axes[i].set_title(f'SHAP Dependence: {feature_name}', fontsize=12)
                
            except Exception as e:
                print(f"ç”Ÿæˆç‰¹å¾ {feature_name} çš„ä¾èµ–å›¾æ—¶å‡ºé”™: {e}")
                axes[i].text(0.5, 0.5, f'Error: {feature_name}', 
                           ha='center', va='center', transform=axes[i].transAxes)
        
        # éšè—å¤šä½™çš„å­å›¾
        for j in range(i+1, len(axes)):
            axes[j].set_visible(False)
        
        plt.suptitle('SHAP Dependence Plots - Top Features', fontsize=16, y=0.98)
        plt.tight_layout()
        
        try:
            save_path = f"{filename_prefix}_dependence.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"SHAPä¾èµ–å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜ä¾èµ–å›¾æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()

    def shap_waterfall_plots(self, n_examples=3, filename_prefix="shap_waterfall"):
        """
        SHAPç€‘å¸ƒå›¾ - æ˜¾ç¤ºå•ä¸ªé¢„æµ‹çš„ç‰¹å¾è´¡çŒ®
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨ç”ŸæˆSHAPç€‘å¸ƒå›¾...")
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬
        sample_indices = np.random.RandomState(self.random_state).choice(
            len(self.X), min(n_examples * 10, len(self.X)), replace=False
        )
        X_sample = self.X.iloc[sample_indices]
        y_sample = self.y.iloc[sample_indices]
        
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(self.best_model)
        
        # é€‰æ‹©ä¸åŒèŒƒå›´çš„æ ·æœ¬ï¼ˆé«˜ã€ä¸­ã€ä½N2Oå€¼ï¼‰
        y_sorted_indices = np.argsort(y_sample)
        selected_indices = [
            y_sorted_indices[len(y_sorted_indices)//4],      # ä½å€¼
            y_sorted_indices[len(y_sorted_indices)//2],      # ä¸­å€¼  
            y_sorted_indices[3*len(y_sorted_indices)//4]     # é«˜å€¼
        ][:n_examples]
        
        # æ¸…ç†ç‰¹å¾åç§°
        clean_feature_names = [self.clean_feature_name(name) for name in X_sample.columns]
        
        fig, axes = plt.subplots(n_examples, 1, figsize=(14, 6*n_examples))
        if n_examples == 1:
            axes = [axes]
        
        for i, idx in enumerate(selected_indices):
            sample_data = X_sample.iloc[[idx]]
            true_value = y_sample.iloc[idx]
            pred_value = self.best_model.predict(sample_data)[0]
            
            # è®¡ç®—SHAPå€¼
            shap_values_sample = explainer.shap_values(sample_data)
            
            try:
                # åˆ›å»ºExplanationå¯¹è±¡ç”¨äºç€‘å¸ƒå›¾
                explanation = shap.Explanation(
                    values=shap_values_sample[0],
                    base_values=explainer.expected_value,
                    data=sample_data.values[0],
                    feature_names=clean_feature_names
                )
                
                plt.sca(axes[i])
                shap.waterfall_plot(explanation, show=False, max_display=15)
                axes[i].set_title(f'Sample {i+1}: True={true_value:.3f}, Pred={pred_value:.3f}', 
                                fontsize=12)
                
            except Exception as e:
                print(f"ç”Ÿæˆæ ·æœ¬ {i+1} çš„ç€‘å¸ƒå›¾æ—¶å‡ºé”™: {e}")
                axes[i].text(0.5, 0.5, f'Error generating waterfall plot for sample {i+1}', 
                           ha='center', va='center', transform=axes[i].transAxes)
        
        plt.suptitle('SHAP Waterfall Plots - Individual Predictions', fontsize=16, y=0.98)
        plt.tight_layout()
        
        try:
            save_path = f"{filename_prefix}_waterfall.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"SHAPç€‘å¸ƒå›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜ç€‘å¸ƒå›¾æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()

    def shap_categorized_analysis(self, n_features=20, filename="shap_categorized.png"):
        """
        å¸¦ç±»åˆ«åˆ†ç±»çš„SHAPé‡è¦æ€§åˆ†æ
        """
        if self.best_model is None or self.X is None or self.y is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–æ•°æ®æœªä¿å­˜ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        print("æ­£åœ¨è¿›è¡Œå¸¦ç±»åˆ«çš„SHAPé‡è¦æ€§åˆ†æ...")
        
        # è®¡ç®—SHAPé‡è¦æ€§
        n_samples = min(1000, len(self.X))
        if len(self.X) > n_samples:
            sample_indices = np.random.RandomState(self.random_state).choice(
                len(self.X), n_samples, replace=False
            )
            X_sample = self.X.iloc[sample_indices]
        else:
            X_sample = self.X
            
        explainer = shap.TreeExplainer(self.best_model)
        shap_values = explainer.shap_values(X_sample)
        
        # åˆ›å»ºSHAPé‡è¦æ€§DataFrame
        importances = pd.DataFrame({
            'feature': self.analysis_vars,
            'importance': np.abs(shap_values).mean(0),
            'std': np.abs(shap_values).std(0)
        })
        
        # æ¸…ç†ç‰¹å¾åç§°ï¼ˆå»é™¤Log1p_å‰ç¼€ï¼‰
        importances['clean_feature'] = importances['feature'].apply(self.clean_feature_name)
        
        # ç‰¹å¾åˆ†ç±»å­—å…¸
        feature_categories = {
            # åœ°å½¢åœ°è²Œç‰¹å¾ (Physiography)
            'Elevation': 'Physiography',
            'slp_dg_uav': 'Physiography',
            'ele_mt_uav': 'Physiography',
            
            # æ°´æ–‡ç‰¹å¾ (Hydrology)
            'Depth_avg': 'Hydrology',
            'Vol_total': 'Hydrology',
            'Dis_avg': 'Hydrology',
            'Lake_area': 'Hydrology',
            'Wshd_area': 'Hydrology',
            'run_mm_vyr': 'Hydrology',
            'dis_m3_pyr': 'Hydrology',
            'Tyear_mean_open': 'Hydrology',
            'Tyear_mean': 'Hydrology',
            'Res_time': 'Hydrology',
            'lkv_mc_usu': 'Hydrology',
            
            # æ°”å€™ç‰¹å¾ (Climate)
            'pre_mm_uyr': 'Climate',
            'pre_mm_lyr': 'Climate',
            'tmp_dc_lyr': 'Climate',
            'ice_days': 'Climate',
            'ari_ix_lav': 'Climate',
            
            # äººä¸ºç‰¹å¾ (Anthropogenic)
            'Population_Density': 'Anthropogenic',
            'ppd_pk_vav': 'Anthropogenic',
            'hft_ix_v09': 'Anthropogenic',
            'urb_pc_vse': 'Anthropogenic',
            
            # åœŸåœ°è¦†ç›– (Landcover)
            'for_pc_vse': 'Landcover',
            'crp_pc_vse': 'Landcover',
            
            # åœŸå£¤ä¸åœ°è´¨ç‰¹å¾ (Soils & Geology)
            'soc_th_vav': 'Soils & Geology',
            'ero_kh_vav': 'Soils & Geology',
            'gwt_cm_vav': 'Soils & Geology',
            
            # æ°´è´¨ç‰¹å¾ (Water quality)
            'Chla_pred_RF': 'Water quality',
            'Chla_Preds_Mean': 'Water quality',
            'TN_Load_Per_Volume': 'Water quality',
            'TP_Load_Per_Volume': 'Water quality',
            'TN_Inputs_Mean': 'Water quality',
            'TP_Inputs_Mean': 'Water quality',
            'TN_Preds_Mean': 'Water quality',
            'TP_Preds_Mean': 'Water quality'
        }
                
        # æ·»åŠ ç±»åˆ«ä¿¡æ¯ï¼ˆåŸºäºæ¸…ç†åçš„ç‰¹å¾åï¼‰
        importances['category'] = importances['clean_feature'].map(
            lambda x: feature_categories.get(x, 'Other')
        )
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©é¡¶éƒ¨ç‰¹å¾
        importances = importances.sort_values('importance', ascending=True)
        top_importances = importances.tail(n_features)
        
        # é¢œè‰²æ˜ å°„
        category_colors = {
            'Climate': '#98D8A0',      # ç»¿è‰²
            'Hydrology': '#7FB3D5',    # è“è‰²
            'Anthropogenic': '#F1948A', # çº¢è‰²
            'Landcover': '#F4D03F',    # é»„è‰²
            'Physiography': '#BFC9CA', # ç°è‰²
            'Soils & Geology': '#E59866', # æ£•è‰²
            'Water quality': '#DDA0DD', # æ·¡ç´«è‰²
            'Other': '#D5D8DC'         # æµ…ç°è‰²
        }
    
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(14, 10))
        
        # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
        bars = ax.barh(range(len(top_importances)), 
                       top_importances['importance'],
                       color=[category_colors.get(cat, '#D5D8DC') for cat in top_importances['category']],
                       alpha=0.8,
                       edgecolor='black',
                       linewidth=0.5)
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(top_importances['importance'], range(len(top_importances)),
                    xerr=top_importances['std'], fmt='none', color='black', 
                    capsize=3, alpha=0.7, zorder=5)
        
        # è‡ªå®šä¹‰å›¾å½¢ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        ax.set_yticks(range(len(top_importances)))
        ax.set_yticklabels(top_importances['clean_feature'], fontsize=10)
        ax.set_xlabel('SHAP Importance (Mean |SHAP value|)', fontsize=12)
        ax.set_title('Main Drivers of N2O Concentrations in Lakes\n(SHAP Importance)', 
                     fontsize=14, pad=20)
        ax.grid(axis='x', alpha=0.3)
        
        # å›¾ä¾‹
        unique_categories = top_importances['category'].unique()
        legend_elements = [plt.Rectangle((0,0), 1, 1, facecolor=category_colors.get(cat, '#D5D8DC'), 
                                       label=cat, edgecolor='black', alpha=0.8) 
                          for cat in sorted(unique_categories)]
        
        ax.legend(handles=legend_elements, 
                 title='Category',
                 loc='center right',
                 fontsize=9,
                 title_fontsize=10)
        
        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        
        try:
            current_dir = os.getcwd()
            save_path = os.path.join(current_dir, filename)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"åˆ†ç±»SHAPé‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨æ¸…ç†åçš„ç‰¹å¾åï¼‰
        print("\nåˆ†ç±»SHAPé‡è¦æ€§åˆ†æç»“æœ:")
        print("-" * 60)
        print(f"å‰{n_features}ä¸ªæœ€é‡è¦ç‰¹å¾åŠå…¶ç±»åˆ«:")
        for i, (_, row) in enumerate(top_importances.iterrows(), 1):
            print(f"{i:2d}. {row['clean_feature']:30s} {row['category']:15s} {row['importance']:8.4f} Â± {row['std']:6.4f}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = top_importances.groupby('category').agg({
            'importance': ['count', 'mean', 'sum']
        }).round(4)
        print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        print(category_stats)
        
        return top_importances

    def save_model(self, filepath):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'analysis_vars': self.analysis_vars,
            'variables': self.variables,
            'variables_removed': self.variables_removed,
            'log_transform_vars': self.log_transform_vars
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹ä¿å­˜è‡³: {filepath}")

    def load_model(self, filepath):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.best_model = model_data['best_model']
        self.scaler = model_data['scaler']
        self.best_params = model_data['best_params']
        self.analysis_vars = model_data['analysis_vars']
        self.variables = model_data['variables']
        self.variables_removed = model_data['variables_removed']
        self.log_transform_vars = model_data['log_transform_vars']
        
        print(f"æ¨¡å‹ä» {filepath} åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹å‚æ•°: {self.best_params}")


def main_shap_analysis():
    """ä¸»å‡½æ•° - ä¸“æ³¨äºSHAPåˆ†æ"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - SHAPä¸“é¡¹åˆ†æç³»ç»Ÿ")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = N2OShapPredictor()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    training_data_path = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    model_path = "n2o_shap_model.pkl"
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²è®­ç»ƒçš„æ¨¡å‹
    if os.path.exists(model_path):
        print(f"\nå‘ç°å·²ä¿å­˜çš„æ¨¡å‹: {model_path}")
        choice = input("æ˜¯å¦åŠ è½½å·²æœ‰æ¨¡å‹ï¼Ÿ(y/n): ").lower()
        if choice == 'y':
            try:
                predictor.load_model(model_path)
                # è¿˜éœ€è¦åŠ è½½æ•°æ®
                X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
                predictor.X = X_scaled
                predictor.y = y
                print("æ¨¡å‹å’Œæ•°æ®åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                return
        else:
            print("å°†é‡æ–°è®­ç»ƒæ¨¡å‹...")
            X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
            predictor.train_model(X_scaled, y)
            predictor.save_model(model_path)
    else:
        if not os.path.exists(training_data_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ {training_data_path}")
            return
            
        print("\n1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
        X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
        print(f"æ•°æ®å½¢çŠ¶: X = {X_scaled.shape}, y = {y.shape}")
        
        print("\n2. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        predictor.train_model(X_scaled, y)
        
        # ç®€å•çš„æ€§èƒ½è¯„ä¼°
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.3, random_state=predictor.random_state
        )
        results = predictor.evaluate_model(X_train, X_val, y_train, y_val)
        print(f"\næ¨¡å‹æ€§èƒ½:")
        print(f"- è®­ç»ƒé›† RÂ²: {results['train_r2']:.4f}")
        print(f"- éªŒè¯é›† RÂ²: {results['val_r2']:.4f}")
        print(f"- OOB Score: {results['oob_score']:.4f}")
        
        print("\n3. ä¿å­˜æ¨¡å‹...")
        predictor.save_model(model_path)
    
    # è¿›è¡ŒSHAPåˆ†æ
    try:
        print("\n" + "="*60)
        print("å¼€å§‹SHAPåˆ†æ...")
        print("="*60)
        
        # 1. ç»¼åˆSHAPåˆ†æ
        print("\n1. ç»¼åˆSHAPåˆ†æ...")
        shap_values, shap_importance = predictor.shap_analysis_comprehensive(n_samples=1000)
        
        # 2. å¸¦ç±»åˆ«åˆ†ç±»çš„SHAPé‡è¦æ€§
        print("\n2. å¸¦ç±»åˆ«åˆ†ç±»çš„SHAPé‡è¦æ€§åˆ†æ...")
        categorized_importance = predictor.shap_categorized_analysis(n_features=20)
        
        # 3. SHAPä¾èµ–å›¾
        print("\n3. SHAPä¾èµ–å›¾...")
        predictor.shap_dependence_plots(top_n_features=6)
        
        # 4. SHAPç€‘å¸ƒå›¾
        print("\n4. SHAPç€‘å¸ƒå›¾...")
        predictor.shap_waterfall_plots(n_examples=3)
        
        print("\n" + "="*60)
        print("SHAPåˆ†æå®Œæˆï¼")
        print("="*60)
        print("\nç”Ÿæˆçš„SHAPåˆ†ææ–‡ä»¶:")
        print("- shap_analysis_summary.png: SHAPé‡è¦æ€§æ¦‚è§ˆå›¾ï¼ˆæ•£ç‚¹å›¾ï¼‰")
        print("- shap_analysis_bar.png: SHAPå¹³å‡é‡è¦æ€§æ¡å½¢å›¾")
        print("- shap_categorized.png: å¸¦ç±»åˆ«åˆ†ç±»çš„SHAPé‡è¦æ€§å›¾")
        print("- shap_dependence_dependence.png: SHAPä¾èµ–å›¾")
        print("- shap_waterfall_waterfall.png: SHAPç€‘å¸ƒå›¾")
        print(f"- {model_path}: è®­ç»ƒå¥½çš„æ¨¡å‹")
        
        # è¾“å‡ºå…³é”®å‘ç°æ‘˜è¦
        print("\nğŸ” å…³é”®å‘ç°æ‘˜è¦:")
        print("-" * 40)
        top_5_features = categorized_importance.tail(5)
        for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
            print(f"{i}. {row['clean_feature']} ({row['category']}) - SHAPé‡è¦æ€§: {row['importance']:.4f}")
        
        print("\nğŸ’¡ SHAPåˆ†æè¯´æ˜:")
        print("- Summary Plot: æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§å’Œå½±å“æ–¹å‘ï¼ˆæ­£è´Ÿæ•ˆåº”ï¼‰")
        print("- Bar Plot: æ˜¾ç¤ºå¹³å‡ç»å¯¹SHAPå€¼æ’å")
        print("- Dependence Plot: æ˜¾ç¤ºç‰¹å¾å€¼ä¸SHAPå€¼çš„å…³ç³»")
        print("- Waterfall Plot: è§£é‡Šå•ä¸ªé¢„æµ‹çš„ç‰¹å¾è´¡çŒ®")
        print("- é¢œè‰²åˆ†ç±»ï¼šç»¿è‰²=æ°”å€™ï¼Œè“è‰²=æ°´æ–‡ï¼Œçº¢è‰²=äººç±»æ´»åŠ¨ç­‰")
        print("- Logå˜æ¢çš„å˜é‡å·²æ˜¾ç¤ºä¸ºåŸå˜é‡å")
        
    except ImportError:
        print("é”™è¯¯: æœªå®‰è£…SHAPåº“ã€‚è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("pip install shap")
    except Exception as e:
        print(f"SHAPåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    
    return predictor


def main_quick_shap():
    """å¿«é€ŸSHAPåˆ†æ - ä»…ç”Ÿæˆæ ¸å¿ƒå›¾è¡¨"""
    print("="*60)
    print("N2Oé¢„æµ‹æ¨¡å‹ - å¿«é€ŸSHAPåˆ†æ")
    print("="*60)
    
    predictor = N2OShapPredictor()
    training_data_path = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
    model_path = "n2o_shap_model.pkl"
    
    # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
    if os.path.exists(model_path):
        try:
            predictor.load_model(model_path)
            X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
            predictor.X = X_scaled
            predictor.y = y
            print("æ¨¡å‹å’Œæ•°æ®åŠ è½½æˆåŠŸï¼")
        except:
            print("åŠ è½½å¤±è´¥ï¼Œé‡æ–°è®­ç»ƒ...")
            X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
            predictor.train_model(X_scaled, y)
            predictor.save_model(model_path)
    else:
        X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
        predictor.train_model(X_scaled, y)
        predictor.save_model(model_path)
    
    try:
        print("\nå¼€å§‹å¿«é€ŸSHAPåˆ†æ...")
        
        # ä»…ç”ŸæˆSummary Plotå’Œåˆ†ç±»é‡è¦æ€§å›¾
        print("\n1. SHAP Summary Plot...")
        shap_values, shap_importance = predictor.shap_analysis_comprehensive(n_samples=800)
        
        print("\n2. åˆ†ç±»SHAPé‡è¦æ€§åˆ†æ...")
        categorized_importance = predictor.shap_categorized_analysis(n_features=15)
        
        print("\nå¿«é€ŸSHAPåˆ†æå®Œæˆï¼")
        print("ç”Ÿæˆæ–‡ä»¶:")
        print("- shap_analysis_summary.png")
        print("- shap_analysis_bar.png") 
        print("- shap_categorized.png")
        
    except Exception as e:
        print(f"å¿«é€ŸSHAPåˆ†æå‡ºé”™: {str(e)}")
    
    return predictor


if __name__ == "__main__":
    print("é€‰æ‹©SHAPåˆ†ææ¨¡å¼:")
    print("1. å®Œæ•´SHAPåˆ†æï¼ˆåŒ…å«æ‰€æœ‰å›¾è¡¨ï¼‰")
    print("2. å¿«é€ŸSHAPåˆ†æï¼ˆä»…æ ¸å¿ƒå›¾è¡¨ï¼‰")
    
    choice = input("è¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        # è¿è¡Œå®Œæ•´SHAPåˆ†æ
        predictor = main_shap_analysis()
    elif choice == "2":
        # è¿è¡Œå¿«é€ŸSHAPåˆ†æ
        predictor = main_quick_shap()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå®Œæ•´SHAPåˆ†æ...")
        predictor = main_shap_analysis()

#%% ç®€åŒ–ç‰ˆé¢„æµ‹ä»£ç  0728

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RepeatedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, RFE
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
import warnings
import pickle
from datetime import datetime
warnings.filterwarnings('ignore')

class ImprovedN2OPredictor:
    def __init__(self, random_state=1113):
        self.random_state = random_state
        self.scaler = RobustScaler()
        self.variables = [
            'Lake_area', 'Depth_avg', 'Vol_total', 'Elevation', 'Dis_avg', 'Wshd_area',
            'Res_time', 'tmp_dc_lyr', 'pre_mm_uyr', 'dis_m3_pyr', 'run_mm_vyr',
            'lkv_mc_usu', 'gwt_cm_vav', 'ele_mt_uav', 'slp_dg_uav', 'pre_mm_lyr',
            'ari_ix_lav', 'for_pc_vse', 'crp_pc_vse', 'soc_th_vav', 'ero_kh_vav',
            'Population_Density', 'urb_pc_vse', 'hft_ix_v09', 'TN_Inputs_Mean', 'TP_Inputs_Mean',
            'TN_Preds_Mean', 'TP_Preds_Mean', 'Chla_pred_RF', 'ice_days',
            'Tyear_mean_open', 'Tyear_mean', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.variables_removed = [
            'dis_m3_pyr', 'ele_mt_uav', 'Tyear_mean', 'pre_mm_lyr', 'tmp_dc_lyr',
            'lkv_mc_usu', 'TN_Inputs_Mean', 'TP_Inputs_Mean', 'TN_Preds_Mean', 'TP_Preds_Mean'
        ]
        self.log_transform_vars = [
            'Lake_area', 'Wshd_area', 'Vol_total', 'Dis_avg', 'gwt_cm_vav', 'Res_time',
            'Population_Density', 'ero_kh_vav', 'ice_days', 'TN_Load_Per_Volume', 'TP_Load_Per_Volume'
        ]
        self.best_model = None
        self.selected_features = None
        self.best_params = None
        self.cv_results = None
        
    def load_and_preprocess_data(self, filepath):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        # è¯»å–æ•°æ®
        data = pd.read_csv(filepath, dtype={'N2O': float})
        print(f"Original data count: {len(data)}")
        
        # åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
        data_filtered = data[
            (data['N2O'] > data['N2O'].quantile(0.01)) & 
            (data['N2O'] < data['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
        ].copy()
        print(f"Data count after filtering: {len(data_filtered)}")
        
        # å¯¹æ•°è½¬æ¢ç›®æ ‡å˜é‡
        data_filtered['Log_N2O'] = np.log10(data_filtered['N2O'] + 1e-10)
        
        # å¯¹æŒ‡å®šå˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        for var in self.log_transform_vars:
            if var in data_filtered.columns:
                data_filtered[f'Log1p_{var}'] = np.log1p(data_filtered[var])
        
        # å‡†å¤‡åˆ†æå˜é‡
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = data_filtered[self.analysis_vars]
        y = data_filtered['Log_N2O']
        
        # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        
        # ä½¿ç”¨RobustScalerè¿›è¡Œç¼©æ”¾
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        return X_scaled, y

    def preprocess_prediction_data_simplified(self, filepath, chunk_size=50000):
        """
        ç®€åŒ–ç‰ˆé¢„æµ‹æ•°æ®é¢„å¤„ç† - é€å—å¤„ç†å¤§å‹CSVæ–‡ä»¶
        
        è¿™ä¸ªå‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨(generator)ï¼Œæ¯æ¬¡å¤„ç†ä¸€å—æ•°æ®å¹¶yieldç»“æœï¼Œ
        é¿å…å°†æ•´ä¸ªå¤§æ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­
        
        Parameters:
        -----------
        filepath : str
            é¢„æµ‹æ•°æ®æ–‡ä»¶è·¯å¾„
        chunk_size : int
            æ¯æ¬¡å¤„ç†çš„è¡Œæ•°ï¼Œé»˜è®¤50000è¡Œ
            
        Yields:
        -------
        dict : åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
            'X_scaled': æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ® (DataFrame)
            'hylak_ids': æ¹–æ³ŠIDåˆ—è¡¨ (æ¥è‡ªåŸå§‹CSVçš„'Hylak_id'åˆ—)
            'chunk_number': å½“å‰å¤„ç†çš„å—ç¼–å·
            'valid_rows': æœ‰æ•ˆè¡Œæ•°
        """
        print(f"å¼€å§‹åˆ†å—é¢„å¤„ç†é¢„æµ‹æ•°æ®: {filepath}")
        print(f"æ¯å—å¤„ç†è¡Œæ•°: {chunk_size:,}")
        
        # å‡†å¤‡åˆ†æå˜é‡åï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        regular_vars = [var for var in self.variables 
                       if var not in self.variables_removed 
                       and var not in self.log_transform_vars]
        log_vars = [f'Log1p_{var}' for var in self.log_transform_vars]
        self.analysis_vars = regular_vars + log_vars
        
        # éœ€è¦çš„åŸå§‹å˜é‡ï¼ˆç”¨äºåˆ›å»ºå¯¹æ•°å˜é‡ï¼‰
        required_vars = regular_vars + self.log_transform_vars
        
        chunk_count = 0
        total_processed = 0
        
        try:
            # åˆ†å—è¯»å–CSVæ–‡ä»¶ - è¿™é‡Œæ˜¯å…³é”®ï¼špandasè‡ªåŠ¨å°†å¤§æ–‡ä»¶åˆ†æˆå°å—
            for chunk in pd.read_csv(filepath, chunksize=chunk_size):
                chunk_count += 1
                input_rows = len(chunk)
                print(f"\nå¤„ç†ç¬¬ {chunk_count} å—æ•°æ®ï¼Œè¾“å…¥è¡Œæ•°: {input_rows:,}")
                
                try:
                    # ğŸ” å…³é”®æ­¥éª¤1ï¼šæå–æ¹–æ³ŠID
                    # chunkæ˜¯å½“å‰è¿™ä¸€å—çš„DataFrameï¼ŒåŒ…å«æ‰€æœ‰åˆ—
                    # ä»ä¸­æå–'Hylak_id'åˆ—ä½œä¸ºæ¹–æ³Šå”¯ä¸€æ ‡è¯†
                    if 'Hylak_id' in chunk.columns:
                        hylak_ids = chunk['Hylak_id'].copy()  # æå–æ¹–æ³ŠID
                        print(f"  æˆåŠŸæå– {len(hylak_ids)} ä¸ªæ¹–æ³ŠID")
                    else:
                        print(f"  âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°'Hylak_id'åˆ—!")
                        hylak_ids = chunk.index.copy()  # ä½¿ç”¨è¡Œç´¢å¼•ä½œä¸ºå¤‡ç”¨ID
                    
                    # ğŸ” å…³é”®æ­¥éª¤2ï¼šæ£€æŸ¥å’Œåˆ›å»ºéœ€è¦çš„ç‰¹å¾åˆ—
                    # å¦‚æœæŸäº›åˆ—åœ¨è¿™ä¸€å—ä¸­å®Œå…¨ç¼ºå¤±ï¼Œåˆ›å»ºå…¨NaNåˆ—
                    for var in required_vars:
                        if var not in chunk.columns:
                            chunk[var] = np.nan
                    
                    # ğŸ” å…³é”®æ­¥éª¤3ï¼šå¤„ç†æ— ç©·å€¼
                    for var in required_vars:
                        if var in chunk.columns:
                            chunk[var] = chunk[var].replace([np.inf, -np.inf], np.nan)
                    
                    # ğŸ” å…³é”®æ­¥éª¤4ï¼šåˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡
                    for var in self.log_transform_vars:
                        if var in chunk.columns:
                            # åªå¯¹éç¼ºå¤±ä¸”éè´Ÿçš„å€¼è¿›è¡Œå¯¹æ•°è½¬æ¢
                            valid_mask = ~chunk[var].isnull() & (chunk[var] >= 0)
                            chunk[f'Log1p_{var}'] = np.nan  # åˆå§‹åŒ–ä¸ºNaN
                            if valid_mask.any():
                                chunk.loc[valid_mask, f'Log1p_{var}'] = np.log1p(chunk.loc[valid_mask, var])
                        else:
                            chunk[f'Log1p_{var}'] = np.nan
                    
                    # ğŸ” å…³é”®æ­¥éª¤5ï¼šé€‰æ‹©åˆ†æå˜é‡
                    X_chunk = chunk[self.analysis_vars].copy()
                    
                    # ğŸ” å…³é”®æ­¥éª¤6ï¼šæ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„scalerï¼‰
                    try:
                        X_scaled = self.scaler.transform(X_chunk)
                        X_scaled_df = pd.DataFrame(X_scaled, columns=X_chunk.columns, index=X_chunk.index)
                        
                        valid_rows = len(X_scaled_df)
                        total_processed += valid_rows
                        
                        # ğŸ” å…³é”®æ­¥éª¤7ï¼šè¿”å›å¤„ç†ç»“æœ
                        # yieldå…³é”®å­—ä½¿è¿™ä¸ªå‡½æ•°æˆä¸ºç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªç»“æœå­—å…¸
                        yield {
                            'X_scaled': X_scaled_df,      # æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
                            'hylak_ids': hylak_ids,      # æ¹–æ³ŠIDï¼ˆæ¥è‡ªåŸå§‹CSVçš„'Hylak_id'åˆ—ï¼‰
                            'chunk_number': chunk_count,  # å—ç¼–å·
                            'valid_rows': valid_rows      # æœ‰æ•ˆè¡Œæ•°
                        }
                        
                    except Exception as scaler_error:
                        print(f"  æ ‡å‡†åŒ–å¤±è´¥: {scaler_error}")
                        # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œè¿”å›æœªæ ‡å‡†åŒ–çš„æ•°æ®
                        yield {
                            'X_scaled': X_chunk,
                            'hylak_ids': hylak_ids,
                            'chunk_number': chunk_count,
                            'valid_rows': len(X_chunk),
                            'scaled': False
                        }
                    
                except Exception as e:
                    print(f"  å¤„ç†ç¬¬ {chunk_count} å—æ—¶å‡ºé”™: {e}")
                    continue
                
        except Exception as e:
            print(f"è¯»å–æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
        
        print(f"\næ•°æ®é¢„å¤„ç†å®Œæˆ:")
        print(f"  æ€»å…±å¤„ç†äº† {chunk_count} ä¸ªæ•°æ®å—")
        print(f"  æ€»å…±å¤„ç†äº† {total_processed:,} è¡Œæ•°æ®")

    def predict_large_dataset_simplified(self, filepath, output_filepath=None, chunk_size=50000):
        """
        ç®€åŒ–ç‰ˆå¤§å‹æ•°æ®é›†é¢„æµ‹ - åªè¾“å‡ºç”¨æˆ·éœ€è¦çš„ä¸‰åˆ—
        
        Parameters:
        -----------
        filepath : str
            è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
        output_filepath : str
            è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        chunk_size : int
            åˆ†å—å¤„ç†å¤§å°
            
        Returns:
        --------
        str : è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if self.best_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        if output_filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filepath = f"N2O_predictions_simplified_{timestamp}.csv"
        
        print(f"å¼€å§‹é¢„æµ‹N2Oæ’æ”¾...")
        print(f"è¾“å…¥æ–‡ä»¶: {filepath}")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_filepath}")
        
        # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœ
        all_hylak_ids = []      # æ¹–æ³ŠID
        all_log_n2o = []        # å¯¹æ•°å°ºåº¦N2O
        all_original_n2o = []   # åŸå§‹å°ºåº¦N2O
        
        processed_chunks = 0
        total_predictions = 0
        failed_predictions = 0
        
        try:
            # ğŸ” å…³é”®å¾ªç¯ï¼šåˆ†å—å¤„ç†å’Œé¢„æµ‹
            # preprocess_prediction_data_simplifiedæ˜¯ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡yieldä¸€ä¸ªå¤„ç†ç»“æœ
            for chunk_result in self.preprocess_prediction_data_simplified(filepath, chunk_size):
                # ä»å¤„ç†ç»“æœä¸­æå–æ•°æ®
                X_scaled = chunk_result['X_scaled']        # æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
                hylak_ids = chunk_result['hylak_ids']      # æ¹–æ³ŠIDï¼ˆè¿™å°±æ˜¯åŸå§‹CSVä¸­çš„'Hylak_id'ï¼‰
                chunk_number = chunk_result['chunk_number'] # å—ç¼–å·
                
                print(f"æ­£åœ¨é¢„æµ‹ç¬¬ {chunk_number} å—æ•°æ®...")
                
                try:
                    # ğŸ” å…³é”®é¢„æµ‹æ­¥éª¤
                    # ä½¿ç”¨è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆè¾“å‡ºæ˜¯å¯¹æ•°å°ºåº¦ï¼‰
                    y_pred_log = self.best_model.predict(X_scaled)
                    
                    # è½¬æ¢åˆ°åŸå§‹å°ºåº¦ (mg N mâ»Â² dâ»Â¹)
                    y_pred_original = 10 ** y_pred_log - 1e-10
                    # ç¡®ä¿ä¸ºæ­£æ•° - é¿å…å¯¹æ•°é€†è½¬æ¢çš„æ•°å€¼ç²¾åº¦é—®é¢˜å¯¼è‡´çš„å¾®å°è´Ÿå€¼
                    y_pred_original = np.maximum(y_pred_original, 1e-10)
                    
                    # ä¿å­˜ç»“æœ
                    all_hylak_ids.extend(hylak_ids)           # ä¿å­˜æ¹–æ³ŠID
                    all_log_n2o.extend(y_pred_log)           # ä¿å­˜å¯¹æ•°å°ºåº¦é¢„æµ‹å€¼
                    all_original_n2o.extend(y_pred_original) # ä¿å­˜åŸå§‹å°ºåº¦é¢„æµ‹å€¼
                    
                    total_predictions += len(y_pred_log)
                    processed_chunks += 1
                    
                except Exception as pred_error:
                    print(f"  é¢„æµ‹å¤±è´¥: {pred_error}")
                    # é¢„æµ‹å¤±è´¥æ—¶ï¼Œä»ç„¶ä¿å­˜IDï¼Œä½†é¢„æµ‹å€¼è®¾ä¸ºNaN
                    all_hylak_ids.extend(hylak_ids)
                    all_log_n2o.extend([np.nan] * len(hylak_ids))
                    all_original_n2o.extend([np.nan] * len(hylak_ids))
                    failed_predictions += len(hylak_ids)
                    continue
                
                # æ¯å¤„ç†10å—æ•°æ®æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if processed_chunks % 10 == 0:
                    print(f"å·²æˆåŠŸå¤„ç† {processed_chunks} å—ï¼Œé¢„æµ‹ {total_predictions:,} ä¸ªæ¹–æ³Š")
        
        except Exception as e:
            print(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
        
        # ğŸ” å…³é”®æ­¥éª¤ï¼šåˆ›å»ºæœ€ç»ˆç»“æœDataFrameï¼ˆåªåŒ…å«ç”¨æˆ·éœ€è¦çš„ä¸‰åˆ—ï¼‰
        results_df = pd.DataFrame({
            'Hylak_id': all_hylak_ids,        # æ¹–æ³Šå”¯ä¸€ID
            'logN2O': all_log_n2o,            # å¯¹æ•°å°ºåº¦N2Oé¢„æµ‹å€¼
            'N2O': all_original_n2o           # åŸå§‹å°ºåº¦N2Oé¢„æµ‹å€¼ï¼ˆmg N mâ»Â² dâ»Â¹ï¼‰
        })
        
        # ä¿å­˜ç»“æœ
        results_df.to_csv(output_filepath, index=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        successful_predictions = results_df['N2O'].notna().sum()
        
        print(f"\n{'='*60}")
        print(f"é¢„æµ‹å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"æ€»æ¹–æ³Šæ•°é‡: {len(results_df):,}")
        print(f"æˆåŠŸé¢„æµ‹æ•°é‡: {successful_predictions:,}")
        print(f"é¢„æµ‹å¤±è´¥æ•°é‡: {failed_predictions:,}")
        print(f"é¢„æµ‹æˆåŠŸç‡: {(successful_predictions/len(results_df))*100:.2f}%")
        print(f"ç»“æœä¿å­˜è‡³: {output_filepath}")
        
        if successful_predictions > 0:
            successful_results = results_df.loc[results_df['N2O'].notna(), 'N2O']
            print(f"\nN2Oé¢„æµ‹å€¼ç»Ÿè®¡ (mg N mâ»Â² dâ»Â¹):")
            print(f"  æœ€å°å€¼: {successful_results.min():.6f}")
            print(f"  æœ€å¤§å€¼: {successful_results.max():.6f}")
            print(f"  å¹³å‡å€¼: {successful_results.mean():.6f}")
            print(f"  ä¸­ä½æ•°: {successful_results.median():.6f}")
        
        return output_filepath

    def create_prediction_summary_plot(self, results_filepath, plot_filepath=None):
        """
        åˆ›å»ºé¢„æµ‹ç»“æœæ‘˜è¦å›¾
        
        Parameters:
        -----------
        results_filepath : str
            é¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
        plot_filepath : str
            å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if plot_filepath is None:
            plot_filepath = results_filepath.replace('.csv', '_summary_plot.png')
        
        print(f"æ­£åœ¨åˆ›å»ºé¢„æµ‹ç»“æœå¯è§†åŒ–å›¾è¡¨...")
        
        # è¯»å–é¢„æµ‹ç»“æœ
        results_df = pd.read_csv(results_filepath)
        
        # è¿‡æ»¤æ‰NaNå€¼
        valid_predictions = results_df['N2O'].dropna()
        valid_log_predictions = results_df['logN2O'].dropna()
        
        if len(valid_predictions) == 0:
            print("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœï¼Œæ— æ³•åˆ›å»ºå›¾è¡¨")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'å…¨çƒæ¹–æ³ŠN2Oæ’æ”¾é¢„æµ‹ç»“æœæ‘˜è¦\n(æœ‰æ•ˆé¢„æµ‹æ•°: {len(valid_predictions):,})', fontsize=14)
        
        # 1. å¯¹æ•°å°ºåº¦é¢„æµ‹å€¼åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 0].hist(valid_log_predictions, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_xlabel('Log10(N2O) [mg N mâ»Â² dâ»Â¹]')
        axes[0, 0].set_ylabel('é¢‘æ•°')
        axes[0, 0].set_title('N2Oé¢„æµ‹å€¼åˆ†å¸ƒ (å¯¹æ•°å°ºåº¦)')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. åŸå§‹å°ºåº¦é¢„æµ‹å€¼åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆæˆªå–95%åˆ†ä½æ•°ä»¥ä¾¿è§‚å¯Ÿï¼‰
        q95 = valid_predictions.quantile(0.95)
        filtered_preds = valid_predictions[valid_predictions <= q95]
        axes[0, 1].hist(filtered_preds, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[0, 1].set_xlabel('N2O [mg N mâ»Â² dâ»Â¹]')
        axes[0, 1].set_ylabel('é¢‘æ•°')
        axes[0, 1].set_title(f'N2Oé¢„æµ‹å€¼åˆ†å¸ƒ (åŸå§‹å°ºåº¦, â‰¤95%åˆ†ä½æ•°: {q95:.4f})')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
        sorted_preds = np.sort(valid_predictions)
        cumulative_prob = np.arange(1, len(sorted_preds) + 1) / len(sorted_preds)
        axes[1, 0].semilogx(sorted_preds, cumulative_prob, linewidth=2)
        axes[1, 0].set_xlabel('N2O [mg N mâ»Â² dâ»Â¹]')
        axes[1, 0].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
        axes[1, 0].set_title('N2Oé¢„æµ‹å€¼ç´¯ç§¯åˆ†å¸ƒå‡½æ•°')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ç®±çº¿å›¾ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
        box_plot = axes[1, 1].boxplot(valid_log_predictions, vert=True, patch_artist=True)
        box_plot['boxes'][0].set_facecolor('lightcoral')
        axes[1, 1].set_ylabel('Log10(N2O) [mg N mâ»Â² dâ»Â¹]')
        axes[1, 1].set_title('N2Oé¢„æµ‹å€¼ç®±çº¿å›¾ (å¯¹æ•°å°ºåº¦)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(plot_filepath, dpi=300, bbox_inches='tight')
        print(f"é¢„æµ‹ç»“æœæ‘˜è¦å›¾ä¿å­˜è‡³: {plot_filepath}")
        plt.show()
        plt.close()
        
        return plot_filepath

    def save_model(self, filepath):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'best_model': self.best_model,
            'scaler': self.scaler,
            'best_params': self.best_params,
            'analysis_vars': self.analysis_vars,
            'variables': self.variables,
            'variables_removed': self.variables_removed,
            'log_transform_vars': self.log_transform_vars
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹ä¿å­˜è‡³: {filepath}")

    def load_model(self, filepath):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.best_model = model_data['best_model']
        self.scaler = model_data['scaler']
        self.best_params = model_data['best_params']
        self.analysis_vars = model_data['analysis_vars']
        self.variables = model_data['variables']
        self.variables_removed = model_data['variables_removed']
        self.log_transform_vars = model_data['log_transform_vars']
        
        print(f"æ¨¡å‹ä» {filepath} åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹å‚æ•°: {self.best_params}")

    def train_improved_model_with_repeated_cv(self, X, y, scoring_metric='neg_mean_squared_error'):
        """ä½¿ç”¨é¢„è®¾æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹"""
        
        # ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°
        best_params = {
            'max_depth': None,
            'max_features': 15,
            'min_samples_leaf': 6,
            'min_samples_split': 15,
            'n_estimators': 1200
        }
        
        print(f"ä½¿ç”¨é¢„è®¾çš„æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹:")
        print(f"å‚æ•°: {best_params}")
        
        # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
        rf_reg = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            oob_score=True,
            **best_params
        )
        
        print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        rf_reg.fit(X, y)
        
        # ä¿å­˜ç»“æœ
        self.best_model = rf_reg
        self.best_params = best_params
        
        print(f"æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"OOB Score: {rf_reg.oob_score_:.4f}")
        
        return self.best_model


def main_simplified_prediction():
    """ç®€åŒ–ç‰ˆé¢„æµ‹ä¸»å‡½æ•°"""
    print("="*60)
    print("å…¨çƒæ¹–æ³ŠN2Oæ’æ”¾é¢„æµ‹ç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = ImprovedN2OPredictor()
    
    # é€‰é¡¹1: ä»å¤´è®­ç»ƒæ¨¡å‹æˆ–åŠ è½½å·²æœ‰æ¨¡å‹
    train_new_model = input("æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹? (y/n): ").lower() == 'y'
    
    if train_new_model:
        print("\n1. è®­ç»ƒæ–°æ¨¡å‹...")
        training_data_path = "GHGdata_LakeATLAS_final250714_cleaned_imputation.csv"
        
        if not os.path.exists(training_data_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ {training_data_path}")
            return
        
        X_scaled, y = predictor.load_and_preprocess_data(training_data_path)
        predictor.train_improved_model_with_repeated_cv(X_scaled, y)
        
        # ä¿å­˜æ¨¡å‹
        model_save_path = "n2o_prediction_model.pkl"
        predictor.save_model(model_save_path)
        
    else:
        print("\n1. åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹...")
        model_path = "n2o_prediction_model.pkl"
        
        if not os.path.exists(model_path):
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
            return
        
        predictor.load_model(model_path)
    
    # é¢„æµ‹æ•°æ®è·¯å¾„
    prediction_data_path = "Hydrolakes_LakeATLAS_final250714_cleaned_imputation_simplified.csv"
    
    if not os.path.exists(prediction_data_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°é¢„æµ‹æ•°æ®æ–‡ä»¶ {prediction_data_path}")
        return
    
    # 2. è¿›è¡Œé¢„æµ‹
    print(f"\n2. å¼€å§‹å¯¹å…¨çƒæ¹–æ³Šè¿›è¡ŒN2Oé¢„æµ‹...")
    
    # è®¾ç½®åˆ†å—å¤§å°
    chunk_size = 50000
    
    try:
        # æ‰§è¡Œé¢„æµ‹ï¼ˆè¾“å‡ºç®€åŒ–çš„ä¸‰åˆ—ç»“æœï¼‰
        output_file = predictor.predict_large_dataset_simplified(
            filepath=prediction_data_path,
            output_filepath=None,  # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
            chunk_size=chunk_size
        )
        
        print(f"\n3. åˆ›å»ºé¢„æµ‹ç»“æœå¯è§†åŒ–...")
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        plot_file = predictor.create_prediction_summary_plot(output_file)
        
        print(f"\né¢„æµ‹ä»»åŠ¡å®Œæˆ!")
        print(f"ç»“æœæ–‡ä»¶: {output_file}")
        print(f"å›¾è¡¨æ–‡ä»¶: {plot_file}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ ¼å¼
        print(f"\næœ€ç»ˆè¾“å‡ºæ–‡ä»¶åŒ…å«ä»¥ä¸‹ä¸‰åˆ—:")
        print(f"  - Hylak_id: æ¹–æ³Šå”¯ä¸€æ ‡è¯†ç¬¦")
        print(f"  - logN2O: å¯¹æ•°å°ºåº¦N2Oé¢„æµ‹å€¼")
        print(f"  - N2O: åŸå§‹å°ºåº¦N2Oé¢„æµ‹å€¼ (mg N mâ»Â² dâ»Â¹)")
        
        return predictor, output_file
        
    except Exception as e:
        print(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return None, None


if __name__ == "__main__":
    # è¿è¡Œç®€åŒ–ç‰ˆé¢„æµ‹æµç¨‹
    predictor, output_file = main_simplified_prediction()


é¢„æµ‹å®Œæˆï¼
============================================================
æ€»æ¹–æ³Šæ•°é‡: 1,427,688
æˆåŠŸé¢„æµ‹æ•°é‡: 1,427,688
é¢„æµ‹å¤±è´¥æ•°é‡: 0
é¢„æµ‹æˆåŠŸç‡: 100.00%
ç»“æœä¿å­˜è‡³: N2O_predictions_simplified_20250728_201652.csv

N2Oé¢„æµ‹å€¼ç»Ÿè®¡ (mg N mâ»Â² dâ»Â¹):
  æœ€å°å€¼: 0.002183
  æœ€å¤§å€¼: 2.884413
  å¹³å‡å€¼: 0.063704
  ä¸­ä½æ•°: 0.055655

æœ€ç»ˆè¾“å‡ºæ–‡ä»¶åŒ…å«ä»¥ä¸‹ä¸‰åˆ—:
  - Hylak_id: æ¹–æ³Šå”¯ä¸€æ ‡è¯†ç¬¦
  - logN2O: å¯¹æ•°å°ºåº¦N2Oé¢„æµ‹å€¼
  - N2O: åŸå§‹å°ºåº¦N2Oé¢„æµ‹å€¼ (mg N mâ»Â² dâ»Â¹)




#%% ç»™é¢„æµ‹ç»“æœåŠ ä¸Šåæ ‡

import pandas as pd

# è¯»å–hydrolakesçš„æ•°æ®
hydrolakes = pd.read_csv(r"D:\Code_running\Global_lake_GHG\HydroLAKES_polys_v10_shp\HydroLAKES_polys_v10.csv")

# lakesn2o = pd.read_csv("D:\Code_running\Global_lake_GHG\Lake N2O code\global_n2o_predictions_with_missing0212.csv")
lakesn2o = pd.read_csv('N2O_predictions_simplified_20250728_201652.csv')

# åˆå¹¶æ¹–æ³Šä¸­å¿ƒç»çº¬åº¦æ•°æ®
merged_data = pd.merge(
    lakesn2o,
    hydrolakes[['Hylak_id','Centr_lat', 'Centr_lon','Lake_area','Country','Continent']],
    how="left",
    on='Hylak_id'
)

# è®¡ç®—N2Oæ’æ”¾é‡ (Lake_area * N2O)   Lake_areaçš„å•ä½æ˜¯å¹³æ–¹åƒç±³ï¼›N2O mg N m-2 d-1 
# ä¹˜ç§¯å N2Oemission å•ä½ kg N y-1
merged_data['N2Oemission'] = merged_data['Lake_area'] * merged_data['N2O'] * 365

# ä¿å­˜åˆ°Excelæ–‡ä»¶
# merged_data.to_csv("global_N2O_predictions0212.csv", index=False)
merged_data.to_csv("global_N2O_predictions0728.csv", index=False)


#%% æ£€æŸ¥N2Oçš„å®é™…åˆ†å¸ƒæƒ…å†µ GHGdata_LakeATLAS_final250714.csv æ­¤è¡¨éƒ½æœ‰Hylak_idåŒ¹é…

import pandas as pd
import numpy as np

# Load data
data = pd.read_csv("GHGdata_LakeATLAS_final250714.csv")

data2 = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
# data2 = data2[
#     (data2['N2O'] >= data2['N2O'].quantile(0.01)) & 
#     (data2['N2O'] <= data2['N2O'].quantile(0.99))
# ].copy()

# data2 = data2[data2['N2O'] >= 0]

# æ‰“å°åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nN2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data['N2O'].describe())

# æ‰“å°åˆ†ä½æ•°ä¿¡æ¯
print("\nåˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data['N2O'].quantile(np.array(percentiles)/100))


# æ‰“å°åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nN2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data2['N2O'].describe())

# æ‰“å°åˆ†ä½æ•°ä¿¡æ¯
print("\nåˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data2['N2O'].quantile(np.array(percentiles)/100))


# N2Oéç©ºä¸”Hylak_idéç©ºçš„æ¹–æ³Šæ•°: 3078

# N2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š
# count    3078.000000
# mean        0.452080
# std         1.353626
# min        -0.465960
# 25%         0.031659
# 50%         0.098643
# 75%         0.328671
# max        39.522938
# Name: N2O, dtype: float64

# åˆ†ä½æ•°ä¿¡æ¯ï¼š
# 0.00    -0.465960
# 0.01     0.000294
# 0.25     0.031659
# 0.50     0.098643
# 0.75     0.328671
# 0.90     0.955279
# 0.95     2.591010
# 0.99     4.291003
# 1.00    39.522938
# Name: N2O, dtype: float64

# N2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼šä»…ç­›é€‰N2Oä¸ºæ­£æ•°
# count    3710.000000
# mean        0.601953
# std         3.508163
# min         0.000000
# 25%         0.038942
# 50%         0.127257
# 75%         0.352943
# max       145.807444
# Name: N2O, dtype: float64

# åˆ†ä½æ•°ä¿¡æ¯ï¼š
# 0.00      0.000000
# 0.01      0.000547
# 0.25      0.038942
# 0.50      0.127257
# 0.75      0.352943
# 0.90      1.273461
# 0.95      2.591010
# 0.99      5.565199
# 1.00    145.807444
# Name: N2O, dtype: float64

# N2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š
# count    3829.000000
# mean        0.575214
# std         3.460473
# min        -9.939328
# 25%         0.034760
# 50%         0.116571
# 75%         0.343314
# max       145.807444
# Name: N2O, dtype: float64

# åˆ†ä½æ•°ä¿¡æ¯ï¼š
# 0.00     -9.939328
# 0.01     -0.188654
# 0.25      0.034760
# 0.50      0.116571
# 0.75      0.343314
# 0.90      1.072953
# 0.95      2.591010
# 0.99      5.474118
# 1.00    145.807444
# Name: N2O, dtype: float64


#%% æ£€æŸ¥N2Oçš„å®é™…åˆ†å¸ƒæƒ…å†µ GHGdata_All250724_attributes_means.xlsx æ­¤è¡¨å°†'Areakm2'å®ŒæˆåŒ¹é…

import pandas as pd
import numpy as np

# Load data
data = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# Select valid N2O data
data = data[data['N2O'].notna() & (data['N2O'] >= 0) & data['Areakm2'].notna()].copy()

data = data[
    (data['N2O'] >= data['N2O'].quantile(0.01)) & 
    (data['N2O'] <= data['N2O'].quantile(0.99))
].copy()


# æ‰“å°åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nN2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data['N2O'].describe())

# æ‰“å°åˆ†ä½æ•°ä¿¡æ¯
print("\nåˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data['N2O'].quantile(np.array(percentiles)/100))

# N2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š
# count    3169.000000
# mean        0.391884
# std         0.771400
# min         0.000548
# 25%         0.035151
# 50%         0.101794
# 75%         0.336000
# max         4.840000
# Name: N2O, dtype: float64

# åˆ†ä½æ•°ä¿¡æ¯ï¼š
# 0.00    0.000548
# 0.01    0.001153
# 0.25    0.035151
# 0.50    0.101794
# 0.75    0.336000
# 0.90    0.888199
# 0.95    2.394016
# 0.99    3.929104
# 1.00    4.840000
# Name: N2O, dtype: float64


#%% N2Oé¢„æµ‹é€šé‡ä»¥åŠæ’æ”¾é‡çš„æ•°æ®åˆ†å¸ƒæƒ…å†µ 0815


import pandas as pd
import numpy as np

# Load data
data = pd.read_csv("global_N2O_predictions0728.csv")

# æ‰“å°åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nN2Oé€šé‡æ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data['N2O'].describe())
print("\nN2Oæ’æ”¾é‡æ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data['N2Oemission'].describe())

# æ‰“å°åˆ†ä½æ•°ä¿¡æ¯
print("\né€šé‡åˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data['N2O'].quantile(np.array(percentiles)/100))

print("\næ’æ”¾é‡åˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data['N2Oemission'].quantile(np.array(percentiles)/100))

# è¡¥å……ï¼šå•ç‹¬æ‰“å°å‡å€¼ä¿¡æ¯
print("\n=== å‡å€¼ä¿¡æ¯ ===")
print(f"N2Oé€šé‡å‡å€¼: {data['N2O'].mean():.6f}")
print(f"N2Oæ’æ”¾é‡å‡å€¼: {data['N2Oemission'].mean():.6f}")

# å¯é€‰ï¼šåŒæ—¶æ˜¾ç¤ºæ ‡å‡†å·®ä»¥ä¾¿æ›´å¥½ç†è§£æ•°æ®åˆ†å¸ƒ
print(f"\nN2Oé€šé‡æ ‡å‡†å·®: {data['N2O'].std():.6f}")
print(f"N2Oæ’æ”¾é‡æ ‡å‡†å·®: {data['N2Oemission'].std():.6f}")


é€šé‡åˆ†ä½æ•°ä¿¡æ¯ï¼š
0.00    0.002183
0.01    0.019622
0.25    0.044440
0.50    0.055655
0.75    0.074689
0.90    0.098062
0.95    0.120952
0.99    0.173275
1.00    2.884413
Name: N2O, dtype: float64

æ’æ”¾é‡åˆ†ä½æ•°ä¿¡æ¯ï¼š
0.00    1.021999e-01
0.01    1.029098e+00
0.25    2.451639e+00
0.50    4.795348e+00
0.75    1.287014e+01
0.90    4.184998e+01
0.95    8.772530e+01
0.99    4.599689e+02
1.00    2.493211e+07
Name: N2Oemission, dtype: float64

=== å‡å€¼ä¿¡æ¯ ===
N2Oé€šé‡å‡å€¼: 0.063704
N2Oæ’æ”¾é‡å‡å€¼: 95.500000

N2Oé€šé‡æ ‡å‡†å·®: 0.037938
N2Oæ’æ”¾é‡æ ‡å‡†å·®: 22497.375118


#%% æ£€æŸ¥N2Oçš„å®é™…åˆ†å¸ƒæƒ…å†µ

import pandas as pd
import numpy as np

# Load data
data = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# Select valid N2O data
data = data[data['N2O'].notna() & data['Areakm2'].notna()].copy()

data = data[
    (data['N2O'] >= data['N2O'].quantile(0.01)) & 
    (data['N2O'] <= data['N2O'].quantile(0.99))
].copy()

data = data[data['N2O'] >= 0] 

# æ‰“å°åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("\nN2Oæ•°æ®åŸºæœ¬ç»Ÿè®¡ï¼š")
print(data['N2O'].describe())

# æ‰“å°åˆ†ä½æ•°ä¿¡æ¯
print("\nåˆ†ä½æ•°ä¿¡æ¯ï¼š")
percentiles = [0, 1, 25, 50, 75, 90, 95, 99, 100]
print(data['N2O'].quantile(np.array(percentiles)/100))

print(data['Areakm2'].quantile(np.array(percentiles)/100))



#%% ç»˜åˆ¶åŸå§‹N2Oæ•°æ®çš„äº‘é›¨å›¾ 0813

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import ptitprince as pt

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['N2O'] >= 0) & GHGdata['Areakm2'].notna()].copy()

# 2. æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
df = df[
    (df['N2O'] > df['N2O'].quantile(0.01)) & 
    (df['N2O'] < df['N2O'].quantile(0.99))
].copy()

# 3. å®šä¹‰é¢ç§¯åˆ†ç»„
bins = [0, 0.001, 0.01, 0.1, 1, 10, 100, np.inf]
labels = ['<0.001', '0.001-0.01', '0.01-0.1', '0.1-1', '1-10', '10-100', '>100']

# 4. åˆ›å»ºé¢ç§¯åˆ†ç»„
df['Area_Group'] = pd.cut(df['Areakm2'], bins=bins, labels=labels, right=False)

# 5. ç§»é™¤å¯èƒ½çš„ç©ºå€¼åˆ†ç»„
df = df[df['Area_Group'].notna()].copy()

# 6. è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡
sample_counts = df['Area_Group'].value_counts().sort_index()

# 7. å®šä¹‰è‡ªå®šä¹‰é…è‰²
custom_colors = ['#274753', '#297270', '#299d8f', '#8ab07c', '#e7c66b', '#f3a361', '#e66d50']

# 8. åˆ›å»ºé«˜è´¨é‡é›¨äº‘å›¾
fig = plt.figure(figsize=(12, 6), dpi=300)
ax = fig.add_subplot(111)

# è®¾ç½®å˜é‡
dx = "Area_Group"  # xè½´:é¢ç§¯åˆ†ç»„
dy = "N2O"         # yè½´:N2Oé€šé‡

# ç¬¬ä¸€å±‚:åŠå°æç´å›¾ - æ˜¾ç¤ºåˆ†å¸ƒå¯†åº¦(å»æ‰è¾¹æ¡†,æ·»åŠ é€æ˜åº¦)
ax = pt.half_violinplot(x=dx, y=dy, data=df, palette=custom_colors,
                        bw=.2, cut=0., scale="area", width=.6,
                        inner=None, orient="v", ax=ax,
                        linewidth=0, alpha=0.7)  # æ·»åŠ é€æ˜åº¦å’Œå»æ‰è¾¹æ¡†

# ç¬¬äºŒå±‚:æ•£ç‚¹å›¾ - æ˜¾ç¤ºåŸå§‹æ•°æ®ç‚¹
ax = sns.stripplot(x=dx, y=dy, data=df, palette=custom_colors,
                   edgecolor="white", size=2.5, jitter=0.25,
                   zorder=1, alpha=0.7, ax=ax)

# ç¬¬ä¸‰å±‚:ç®±çº¿å›¾ - æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
ax = sns.boxplot(x=dx, y=dy, data=df,
                 width=0.15, palette=custom_colors,
                 fliersize=3, linewidth=1.2,
                 zorder=10, showcaps=True,
                 boxprops={'facecolor':'none', "zorder":10},
                 showfliers=True, 
                 whiskerprops={"zorder":10},
                 saturation=1, ax=ax)

# 9. è®¾ç½®æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾(ä½¿ç”¨æ­£ç¡®çš„LaTeXè¯­æ³•æ˜¾ç¤ºä¸Šè§’æ ‡)
ax.set_title("N$_2$O Flux Distribution by Lake Size Class",
             fontsize=14, pad=15)
ax.set_xlabel("Lake size class (km$^2$)", fontsize=12)
ax.set_ylabel("N$_2$O flux (mg N m$^{-2}$ d$^{-1}$)", fontsize=12)

# 10. è®¾ç½®åˆ»åº¦å‚æ•°
ax.tick_params(labelsize=10)
plt.xticks(rotation=45, ha='right')

# 11. æ·»åŠ ç½‘æ ¼çº¿
plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.4)

# 12. ä¼˜åŒ–yè½´èŒƒå›´ - å¢åŠ 0ä»¥ä¸‹çš„ç©ºé—´
y_min, y_max = df['N2O'].min(), df['N2O'].max()
y_range = y_max - y_min
# å¢åŠ åº•éƒ¨ç©ºé—´ä»¥å®¹çº³æ ·æœ¬æ•°é‡æ ‡æ³¨
bottom_extension = max(y_range * 0.08, 0.05)  # è‡³å°‘å¢åŠ 15%çš„ç©ºé—´æˆ–0.1çš„ç»å¯¹å€¼
ax.set_ylim(y_min - bottom_extension, y_max + y_range*0.1)

# 13. æ·»åŠ æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡æ ‡æ³¨
for i, (category, count) in enumerate(sample_counts.items()):
    # åœ¨æ¯ä¸ªç±»åˆ«ä¸‹æ–¹æ·»åŠ æ ·æœ¬æ•°é‡
    y_position = y_min - bottom_extension * 0.5  # ä½ç½®åœ¨åº•éƒ¨æ‰©å±•ç©ºé—´çš„70%å¤„
    ax.text(i, y_position, f'n = {count}', 
            ha='center', va='center', 
            fontsize=9, fontweight='normal',
            bbox=dict(boxstyle='round,pad=0.3', 
                     facecolor='white', 
                     edgecolor='none', 
                     alpha=0.8))

# 14. è®¾ç½®è¾¹æ¡†æ ·å¼
for spine in ax.spines.values():
    spine.set_linewidth(1.0)
    spine.set_color('black')

# 15. è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# 16. ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡
plt.savefig('N2O_raincloud_plot_enhanced.png', dpi=600, bbox_inches='tight')
plt.show()

# 17. æ‰“å°æ ·æœ¬æ•°é‡ç»Ÿè®¡ä¿¡æ¯
print("å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ ·æœ¬æ•°é‡:")
for category, count in sample_counts.items():
    print(f"{category}: {count}ä¸ªæ¹–æ³Š")


#%% ç»˜åˆ¶å°æç´ä¸ç®±çº¿å›¾ 0814

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['N2O'] >= 0) & GHGdata['Areakm2'].notna()].copy()

# 2. æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
df = df[
    (df['N2O'] > df['N2O'].quantile(0.01)) & 
    (df['N2O'] < df['N2O'].quantile(0.99))
].copy()

# 3. å®šä¹‰é¢ç§¯åˆ†ç»„
bins = [0, 0.001, 0.01, 0.1, 1, 10, 100, np.inf]
labels = ['<0.001', '0.001-0.01', '0.01-0.1', '0.1-1', '1-10', '10-100', '>100']

# 4. åˆ›å»ºé¢ç§¯åˆ†ç»„
df['Area_Group'] = pd.cut(df['Areakm2'], bins=bins, labels=labels, right=False)

# 5. ç§»é™¤å¯èƒ½çš„ç©ºå€¼åˆ†ç»„
df = df[df['Area_Group'].notna()].copy()

# 6. è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡
sample_counts = df['Area_Group'].value_counts().sort_index()

# 7. å®šä¹‰è‡ªå®šä¹‰é…è‰²
custom_colors = ['#274753', '#297270', '#299d8f', '#8ab07c', '#e7c66b', '#f3a361', '#e66d50']

# 8. åˆ›å»ºé«˜è´¨é‡å°æç´å›¾+ç®±çº¿å›¾
fig = plt.figure(figsize=(12, 6), dpi=300)
ax = fig.add_subplot(111)

# è®¾ç½®å˜é‡
dx = "Area_Group"  # xè½´:é¢ç§¯åˆ†ç»„
dy = "N2O"         # yè½´:N2Oé€šé‡

# ç¬¬ä¸€å±‚:å°æç´å›¾ - æ˜¾ç¤ºåˆ†å¸ƒå¯†åº¦
ax = sns.violinplot(x=dx, y=dy, data=df, palette=custom_colors,
                    inner=None, alpha=0.6, linewidth=0, ax=ax,
                    cut=0)  # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼Œ0è¡¨ç¤ºä¸åœ¨æ•°æ®èŒƒå›´å¤–å»¶ä¼¸

# ç¬¬äºŒå±‚:ç®±çº¿å›¾ - æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦(åœ¨å°æç´å›¾å†…éƒ¨)
ax = sns.boxplot(x=dx, y=dy, data=df,
                 width=0.08, palette=custom_colors,
                 fliersize=3, linewidth=1.2,
                 zorder=10, showcaps=True,
                 boxprops={'facecolor':'white', "zorder":10, 'alpha':0.8},
                 showfliers=True, 
                 whiskerprops={"zorder":10, "linewidth":1.2},
                 capprops={"zorder":10, "linewidth":1.2},
                 medianprops={"zorder":10, "linewidth":2, "color":"black"},
                 saturation=1, ax=ax)

# 9. è®¾ç½®æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾(ä½¿ç”¨æ­£ç¡®çš„LaTeXè¯­æ³•æ˜¾ç¤ºä¸Šè§’æ ‡)
ax.set_title("N$_2$O Flux Distribution by Lake Size Class",
             fontsize=14, pad=15)
ax.set_xlabel("Lake size class (km$^2$)", fontsize=12)
ax.set_ylabel("N$_2$O flux (mg N m$^{-2}$ d$^{-1}$)", fontsize=12)

# 10. è®¾ç½®åˆ»åº¦å‚æ•°
ax.tick_params(labelsize=10)
plt.xticks(rotation=45, ha='right')

# 11. æ·»åŠ ç½‘æ ¼çº¿
plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.4)

# 12. ä¼˜åŒ–yè½´èŒƒå›´ - å¢åŠ 0ä»¥ä¸‹çš„ç©ºé—´
y_min, y_max = df['N2O'].min(), df['N2O'].max()
y_range = y_max - y_min
# å¢åŠ åº•éƒ¨ç©ºé—´ä»¥å®¹çº³æ ·æœ¬æ•°é‡æ ‡æ³¨
bottom_extension = max(y_range * 0.12, 0.06)  # è‡³å°‘å¢åŠ 8%çš„ç©ºé—´æˆ–0.05çš„ç»å¯¹å€¼
ax.set_ylim(y_min - bottom_extension, y_max + y_range*0.1)

# 13. æ·»åŠ æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡æ ‡æ³¨
for i, (category, count) in enumerate(sample_counts.items()):
    # åœ¨æ¯ä¸ªç±»åˆ«ä¸‹æ–¹æ·»åŠ æ ·æœ¬æ•°é‡
    y_position = y_min - bottom_extension * 0.5  # ä½ç½®åœ¨åº•éƒ¨æ‰©å±•ç©ºé—´çš„50%å¤„
    ax.text(i, y_position, f'n = {count}', 
            ha='center', va='center', 
            fontsize=9, fontweight='normal',
            bbox=dict(boxstyle='round,pad=0.3', 
                     facecolor='white', 
                     edgecolor='none', 
                     alpha=0.8))

# 14. è®¾ç½®è¾¹æ¡†æ ·å¼
for spine in ax.spines.values():
    spine.set_linewidth(1.0)
    spine.set_color('black')

# 15. è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# 16. ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡
plt.savefig('N2O_violin_box_plot0820.png', dpi=600, bbox_inches='tight')
plt.show()

# 17. æ‰“å°æ ·æœ¬æ•°é‡ç»Ÿè®¡ä¿¡æ¯
print("å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ ·æœ¬æ•°é‡:")
for category, count in sample_counts.items():
    print(f"{category}: {count}ä¸ªæ¹–æ³Š")

#%% ç»˜åˆ¶å°æç´ä¸ç®±çº¿å›¾ logå°ºåº¦ 251020

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['N2O'] >= 0) & GHGdata['Areakm2'].notna()].copy()

# 2. æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
df = df[
    (df['N2O'] > df['N2O'].quantile(0.01)) & 
    (df['N2O'] < df['N2O'].quantile(0.99))
].copy()

# 2.5 å°†N2Oè½¬æ¢ä¸ºå¯¹æ•°å°ºåº¦
df['Log_N2O'] = np.log10(df['N2O'] + 1e-10)

# 3. å®šä¹‰é¢ç§¯åˆ†ç»„
bins = [0, 0.001, 0.01, 0.1, 1, 10, 100, np.inf]
labels = ['<0.001', '0.001-0.01', '0.01-0.1', '0.1-1', '1-10', '10-100', '>100']

# 4. åˆ›å»ºé¢ç§¯åˆ†ç»„
df['Area_Group'] = pd.cut(df['Areakm2'], bins=bins, labels=labels, right=False)

# 5. ç§»é™¤å¯èƒ½çš„ç©ºå€¼åˆ†ç»„
df = df[df['Area_Group'].notna()].copy()

# 6. è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡
sample_counts = df['Area_Group'].value_counts().sort_index()

# 7. å®šä¹‰è‡ªå®šä¹‰é…è‰²
custom_colors = ['#274753', '#297270', '#299d8f', '#8ab07c', '#e7c66b', '#f3a361', '#e66d50']

# 8. åˆ›å»ºé«˜è´¨é‡å°æç´å›¾+ç®±çº¿å›¾
fig = plt.figure(figsize=(12, 6), dpi=300)
ax = fig.add_subplot(111)

# è®¾ç½®å˜é‡
dx = "Area_Group"  # xè½´:é¢ç§¯åˆ†ç»„
dy = "Log_N2O"     # yè½´:Log_N2Oé€šé‡

# ç¬¬ä¸€å±‚:å°æç´å›¾ - æ˜¾ç¤ºåˆ†å¸ƒå¯†åº¦
ax = sns.violinplot(x=dx, y=dy, data=df, palette=custom_colors,
                    inner=None, alpha=0.6, linewidth=0, ax=ax,
                    cut=0)  # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼Œ0è¡¨ç¤ºä¸åœ¨æ•°æ®èŒƒå›´å¤–å»¶ä¼¸

# ç¬¬äºŒå±‚:ç®±çº¿å›¾ - æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦(åœ¨å°æç´å›¾å†…éƒ¨)
ax = sns.boxplot(x=dx, y=dy, data=df,
                 width=0.08, palette=custom_colors,
                 fliersize=3, linewidth=1.2,
                 zorder=10, showcaps=True,
                 boxprops={'facecolor':'white', "zorder":10, 'alpha':0.8},
                 showfliers=True, 
                 whiskerprops={"zorder":10, "linewidth":1.2},
                 capprops={"zorder":10, "linewidth":1.2},
                 medianprops={"zorder":10, "linewidth":2, "color":"black"},
                 saturation=1, ax=ax)

# 9. è®¾ç½®æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾(ä½¿ç”¨æ­£ç¡®çš„LaTeXè¯­æ³•æ˜¾ç¤ºä¸Šè§’æ ‡)
ax.set_title("N$_2$O Flux Distribution by Lake Size Class (Log Scale)",
             fontsize=14, pad=15)
ax.set_xlabel("Lake size class (km$^2$)", fontsize=12)
ax.set_ylabel("log$_{10}$(N$_2$O flux) (mg N m$^{-2}$ d$^{-1}$)", fontsize=12)

# 10. è®¾ç½®åˆ»åº¦å‚æ•°
ax.tick_params(labelsize=10)
plt.xticks(rotation=0)

# 11. æ·»åŠ ç½‘æ ¼çº¿
plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.4)

# 12. ä¼˜åŒ–yè½´èŒƒå›´ - å¢åŠ åº•éƒ¨ç©ºé—´
y_min, y_max = df['Log_N2O'].min(), df['Log_N2O'].max()
y_range = y_max - y_min
# å¢åŠ åº•éƒ¨ç©ºé—´ä»¥å®¹çº³æ ·æœ¬æ•°é‡æ ‡æ³¨
bottom_extension = max(y_range * 0.12, 0.5)  # æ ¹æ®å¯¹æ•°å°ºåº¦è°ƒæ•´æ‰©å±•ç©ºé—´
ax.set_ylim(y_min - bottom_extension, y_max + y_range*0.1)

# 13. æ·»åŠ æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡æ ‡æ³¨
for i, (category, count) in enumerate(sample_counts.items()):
    # åœ¨æ¯ä¸ªç±»åˆ«ä¸‹æ–¹æ·»åŠ æ ·æœ¬æ•°é‡
    y_position = y_min - bottom_extension * 0.5  # ä½ç½®åœ¨åº•éƒ¨æ‰©å±•ç©ºé—´çš„50%å¤„
    ax.text(i, y_position, f'n = {count}', 
            ha='center', va='center', 
            fontsize=9, fontweight='normal',
            bbox=dict(boxstyle='round,pad=0.3', 
                     facecolor='white', 
                     edgecolor='none', 
                     alpha=0.8))

# 14. è®¾ç½®è¾¹æ¡†æ ·å¼
for spine in ax.spines.values():
    spine.set_linewidth(1.0)
    spine.set_color('black')

# 15. è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# 16. ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡
plt.savefig('N2O_violin_box_plot_log_scale.png', dpi=600, bbox_inches='tight')
plt.show()

# 17. æ‰“å°æ ·æœ¬æ•°é‡ç»Ÿè®¡ä¿¡æ¯
print("å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ ·æœ¬æ•°é‡:")
for category, count in sample_counts.items():
    print(f"{category}: {count}ä¸ªæ¹–æ³Š")

# 18. æ‰“å°Log_N2Oçš„ç»Ÿè®¡ä¿¡æ¯
print("\nLog_N2Oç»Ÿè®¡ä¿¡æ¯:")
print(df['Log_N2O'].describe())




#%% ç»˜åˆ¶å°æç´ä¸ç®±çº¿å›¾ å¹¶è®¡ç®—æ˜¾è‘—æ€§ 0820


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import kruskal, mannwhitneyu
import itertools

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['N2O'] >= 0) & GHGdata['Areakm2'].notna()].copy()

# 2. æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
df = df[
    (df['N2O'] > df['N2O'].quantile(0.01)) & 
    (df['N2O'] < df['N2O'].quantile(0.99))
].copy()

# 3. å®šä¹‰é¢ç§¯åˆ†ç»„
bins = [0, 0.001, 0.01, 0.1, 1, 10, 100, np.inf]
labels = ['<0.001', '0.001-0.01', '0.01-0.1', '0.1-1', '1-10', '10-100', '>100']

# 4. åˆ›å»ºé¢ç§¯åˆ†ç»„
df['Area_Group'] = pd.cut(df['Areakm2'], bins=bins, labels=labels, right=False)

# 5. ç§»é™¤å¯èƒ½çš„ç©ºå€¼åˆ†ç»„
df = df[df['Area_Group'].notna()].copy()

# 6. è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡
sample_counts = df['Area_Group'].value_counts().sort_index()

# 7. å®šä¹‰è‡ªå®šä¹‰é…è‰²
custom_colors = ['#274753', '#297270', '#299d8f', '#8ab07c', '#e7c66b', '#f3a361', '#e66d50']

# 8. åˆ›å»ºé«˜è´¨é‡å°æç´å›¾+ç®±çº¿å›¾
fig = plt.figure(figsize=(12, 6), dpi=300)
ax = fig.add_subplot(111)

# è®¾ç½®å˜é‡
dx = "Area_Group"  # xè½´:é¢ç§¯åˆ†ç»„
dy = "N2O"         # yè½´:N2Oé€šé‡

# ç¬¬ä¸€å±‚:å°æç´å›¾ - æ˜¾ç¤ºåˆ†å¸ƒå¯†åº¦
ax = sns.violinplot(x=dx, y=dy, data=df, palette=custom_colors,
                    inner=None, alpha=0.6, linewidth=0, ax=ax,
                    cut=0)  # æ·»åŠ è¿™ä¸ªå‚æ•°ï¼Œ0è¡¨ç¤ºä¸åœ¨æ•°æ®èŒƒå›´å¤–å»¶ä¼¸

# ç¬¬äºŒå±‚:ç®±çº¿å›¾ - æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦(åœ¨å°æç´å›¾å†…éƒ¨)
ax = sns.boxplot(x=dx, y=dy, data=df,
                 width=0.08, palette=custom_colors,
                 fliersize=3, linewidth=1.2,
                 zorder=10, showcaps=True,
                 boxprops={'facecolor':'white', "zorder":10, 'alpha':0.8},
                 showfliers=True, 
                 whiskerprops={"zorder":10, "linewidth":1.2},
                 capprops={"zorder":10, "linewidth":1.2},
                 medianprops={"zorder":10, "linewidth":2, "color":"black"},
                 saturation=1, ax=ax)

# 9. è®¾ç½®æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾(ä½¿ç”¨æ­£ç¡®çš„LaTeXè¯­æ³•æ˜¾ç¤ºä¸Šè§’æ ‡)
ax.set_title("N$_2$O Flux Distribution by Lake Size Class",
             fontsize=14, pad=15)
ax.set_xlabel("Lake size class (km$^2$)", fontsize=12)
ax.set_ylabel("N$_2$O flux (mg N m$^{-2}$ d$^{-1}$)", fontsize=12)

# 10. è®¾ç½®åˆ»åº¦å‚æ•°
ax.tick_params(labelsize=10)
plt.xticks(rotation=45, ha='right')

# 11. æ·»åŠ ç½‘æ ¼çº¿
plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.4)

# 12. ä¼˜åŒ–yè½´èŒƒå›´ - å¢åŠ 0ä»¥ä¸‹çš„ç©ºé—´
y_min, y_max = df['N2O'].min(), df['N2O'].max()
y_range = y_max - y_min
# å¢åŠ åº•éƒ¨ç©ºé—´ä»¥å®¹çº³æ ·æœ¬æ•°é‡æ ‡æ³¨
bottom_extension = max(y_range * 0.12, 0.06)  # è‡³å°‘å¢åŠ 8%çš„ç©ºé—´æˆ–0.05çš„ç»å¯¹å€¼
ax.set_ylim(y_min - bottom_extension, y_max + y_range*0.1)

# 13. æ·»åŠ æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡æ ‡æ³¨
for i, (category, count) in enumerate(sample_counts.items()):
    # åœ¨æ¯ä¸ªç±»åˆ«ä¸‹æ–¹æ·»åŠ æ ·æœ¬æ•°é‡
    y_position = y_min - bottom_extension * 0.5  # ä½ç½®åœ¨åº•éƒ¨æ‰©å±•ç©ºé—´çš„50%å¤„
    ax.text(i, y_position, f'n = {count}', 
            ha='center', va='center', 
            fontsize=9, fontweight='normal',
            bbox=dict(boxstyle='round,pad=0.3', 
                     facecolor='white', 
                     edgecolor='none', 
                     alpha=0.8))

# 14. è®¾ç½®è¾¹æ¡†æ ·å¼
for spine in ax.spines.values():
    spine.set_linewidth(1.0)
    spine.set_color('black')

# 15. è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# 16. ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾ç‰‡
plt.savefig('N2O_violin_box_plot0820.png', dpi=600, bbox_inches='tight')
plt.show()

# 17. æ‰“å°æ ·æœ¬æ•°é‡ç»Ÿè®¡ä¿¡æ¯
print("=" * 60)
print("å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ ·æœ¬æ•°é‡:")
print("=" * 60)
for category, count in sample_counts.items():
    print(f"{category}: {count}ä¸ªæ¹–æ³Š")

# 18. æ–°å¢ï¼šæ‰“å°æ¯ä¸ªå¤§å°ç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
print("\n" + "=" * 80)
print("å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„N2Oé€šé‡ç»Ÿè®¡ä¿¡æ¯:")
print("=" * 80)
print(f"{'ç±»åˆ«':<12} {'æ ·æœ¬æ•°':<8} {'èŒƒå›´':<25} {'ä¸­ä½æ•°':<12} {'å‡å€¼':<12}")
print("-" * 80)

for category in labels:
    if category in sample_counts.index:
        data_subset = df[df['Area_Group'] == category]['N2O']
        if len(data_subset) > 0:
            min_val = data_subset.min()
            max_val = data_subset.max()
            median_val = data_subset.median()
            mean_val = data_subset.mean()
            
            print(f"{category:<12} {len(data_subset):<8} "
                  f"{min_val:.3f} - {max_val:.3f}{'':<8} "
                  f"{median_val:<12.3f} {mean_val:<12.3f}")

# 19. æ–°å¢ï¼šæ˜¾è‘—æ€§å·®å¼‚æ£€éªŒ
print("\n" + "=" * 80)
print("æ˜¾è‘—æ€§å·®å¼‚æ£€éªŒ:")
print("=" * 80)

# é¦–å…ˆè¿›è¡ŒKruskal-Wallisæ£€éªŒï¼ˆéå‚æ•°æ£€éªŒï¼Œé€‚ç”¨äºå¤šç»„æ¯”è¾ƒï¼‰
groups_data = []
group_names = []
for category in labels:
    if category in sample_counts.index:
        data_subset = df[df['Area_Group'] == category]['N2O']
        if len(data_subset) > 0:
            groups_data.append(data_subset.values)
            group_names.append(category)

# Kruskal-Wallisæ£€éªŒ
if len(groups_data) > 2:
    kruskal_stat, kruskal_p = kruskal(*groups_data)
    print(f"Kruskal-Wallisæ£€éªŒç»“æœ:")
    print(f"  ç»Ÿè®¡é‡ = {kruskal_stat:.4f}")
    print(f"  På€¼ = {kruskal_p:.6f}")
    
    # åˆ¤æ–­æ˜¾è‘—æ€§æ°´å¹³
    if kruskal_p <= 0.0001:
        significance = "****"
    elif kruskal_p <= 0.001:
        significance = "***"
    elif kruskal_p <= 0.01:
        significance = "**"
    elif kruskal_p <= 0.05:
        significance = "*"
    else:
        significance = "ns"
    
    print(f"  æ˜¾è‘—æ€§: {significance}")
    print(f"  ç»“è®º: {'å„ç»„ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚' if kruskal_p <= 0.05 else 'å„ç»„ä¹‹é—´æ— æ˜¾è‘—å·®å¼‚'}")
    
    # å¦‚æœKruskal-Wallisæ£€éªŒæ˜¾è‘—ï¼Œè¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ
    if kruskal_p <= 0.05:
        print(f"\nä¸¤ä¸¤æ¯”è¾ƒç»“æœ (Mann-Whitney Uæ£€éªŒ):")
        print("-" * 60)
        
        # åˆ›å»ºç»“æœçŸ©é˜µ
        n_groups = len(group_names)
        p_matrix = np.ones((n_groups, n_groups))
        
        for i, j in itertools.combinations(range(n_groups), 2):
            if len(groups_data[i]) > 0 and len(groups_data[j]) > 0:
                try:
                    statistic, p_value = mannwhitneyu(groups_data[i], groups_data[j], 
                                                    alternative='two-sided')
                    p_matrix[i, j] = p_value
                    p_matrix[j, i] = p_value
                    
                    # åˆ¤æ–­æ˜¾è‘—æ€§
                    if p_value <= 0.0001:
                        sig_symbol = "****"
                    elif p_value <= 0.001:
                        sig_symbol = "***"
                    elif p_value <= 0.01:
                        sig_symbol = "**"
                    elif p_value <= 0.05:
                        sig_symbol = "*"
                    else:
                        sig_symbol = "ns"
                    
                    print(f"{group_names[i]:<12} vs {group_names[j]:<12}: "
                          f"P = {p_value:.6f} {sig_symbol}")
                          
                except Exception as e:
                    print(f"{group_names[i]:<12} vs {group_names[j]:<12}: "
                          f"æ— æ³•è®¡ç®— (é”™è¯¯: {str(e)})")

print(f"\næ˜¾è‘—æ€§æ ‡è®°è¯´æ˜:")
print(f"*P â‰¤ 0.05; **P â‰¤ 0.01; ***P â‰¤ 0.001; ****P â‰¤ 0.0001; ns = ä¸æ˜¾è‘—")
print("=" * 80)


å„æ¹–æ³Šå¤§å°ç±»åˆ«çš„N2Oé€šé‡ç»Ÿè®¡ä¿¡æ¯:
================================================================================
ç±»åˆ«           æ ·æœ¬æ•°      èŒƒå›´                        ä¸­ä½æ•°          å‡å€¼          
--------------------------------------------------------------------------------
<0.001       27       0.051 - 4.840         0.388        1.228       
0.001-0.01   29       0.001 - 3.373         0.264        0.500       
0.01-0.1     30       0.003 - 3.360         0.181        0.518       
0.1-1        2331     0.001 - 4.674         0.080        0.348       
1-10         598      0.003 - 4.521         0.177        0.484       
10-100       85       0.001 - 4.020         0.197        0.774       
>100         69       0.011 - 1.008         0.126        0.183       

================================================================================
æ˜¾è‘—æ€§å·®å¼‚æ£€éªŒ:
================================================================================
Kruskal-Wallisæ£€éªŒç»“æœ:
  ç»Ÿè®¡é‡ = 177.6229
  På€¼ = 0.000000
  æ˜¾è‘—æ€§: ****
  ç»“è®º: å„ç»„ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚

ä¸¤ä¸¤æ¯”è¾ƒç»“æœ (Mann-Whitney Uæ£€éªŒ):
------------------------------------------------------------
<0.001       vs 0.001-0.01  : P = 0.015229 *
<0.001       vs 0.01-0.1    : P = 0.001687 **
<0.001       vs 0.1-1       : P = 0.000000 ****
<0.001       vs 1-10        : P = 0.000066 ****
<0.001       vs 10-100      : P = 0.017267 *
<0.001       vs >100        : P = 0.000001 ****
0.001-0.01   vs 0.01-0.1    : P = 0.490264 ns
0.001-0.01   vs 0.1-1       : P = 0.002414 **
0.001-0.01   vs 1-10        : P = 0.528794 ns
0.001-0.01   vs 10-100      : P = 0.571339 ns
0.001-0.01   vs >100        : P = 0.028164 *
0.01-0.1     vs 0.1-1       : P = 0.029032 *
0.01-0.1     vs 1-10        : P = 0.721610 ns
0.01-0.1     vs 10-100      : P = 0.215385 ns
0.01-0.1     vs >100        : P = 0.186467 ns
0.1-1        vs 1-10        : P = 0.000000 ****
0.1-1        vs 10-100      : P = 0.000000 ****
0.1-1        vs >100        : P = 0.056928 ns
1-10         vs 10-100      : P = 0.054190 ns
1-10         vs >100        : P = 0.004726 **
10-100       vs >100        : P = 0.000477 ***

æ˜¾è‘—æ€§æ ‡è®°è¯´æ˜:
*P â‰¤ 0.05; **P â‰¤ 0.01; ***P â‰¤ 0.001; ****P â‰¤ 0.0001; ns = ä¸æ˜¾è‘—

#%% N2Oå…¨çƒåœ°ç†åˆ†å¸ƒå›¾ 0813

import pandas as pd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import os
from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm
import numpy as np

# è®¾ç½®å­—ä½“,ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# Load data
data = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# Select valid N2O data
data = data[data['N2O'].notna() & data['Areakm2'].notna()].copy()
data_n2o = data[data['N2O'] >= 0]

# è®¡ç®—åœ†åœˆå¤§å° - ä½¿ç”¨åŒºé—´åˆ†ç»„
def calculate_marker_sizes_by_intervals(areas):
    """
    æ ¹æ®æ¹–æ³Šé¢ç§¯åŒºé—´è®¡ç®—æ ‡è®°ç‚¹å¤§å°
    ä½¿ç”¨åŒºé—´åˆ†ç»„çš„æ–¹å¼ï¼Œæ›´ç›´è§‚æ˜“æ‡‚
    """
    sizes = np.zeros(len(areas))
    
    # å®šä¹‰åŒºé—´å’Œå¯¹åº”çš„å¤§å°ï¼ˆåƒç´ ï¼‰
    # åŒºé—´: [ä¸‹é™, ä¸Šé™), å¤§å°
    intervals = [
        (0, 0.1, 8),      # 0-0.1 kmÂ²
        (0.1, 0.5, 15),   # 0.1-0.5 kmÂ²
        (0.5, 1, 25),     # 0.5-1 kmÂ²
        (1, 5, 40),       # 1-5 kmÂ²
        (5, 100, 60),     # 5-100 kmÂ²
        (100, np.inf, 80) # >100 kmÂ²
    ]
    
    for lower, upper, size in intervals:
        mask = (areas >= lower) & (areas < upper)
        sizes[mask] = size
    
    return sizes, intervals

# è®¡ç®—æ ‡è®°ç‚¹å¤§å°
marker_sizes, size_intervals = calculate_marker_sizes_by_intervals(data_n2o['Areakm2'])

# Create custom colormap using the new color scheme
# colors_new = ['#FFEAD3', '#FFDDB3', '#FFCC8F', '#FF9554', '#FF6E39', '#C63C29', '#A40001']
colors_new = ['#fbe1a1', '#fea974', '#f6735d', '#d94669', '#a9327d', '#4a107a', '#1a1041']
custom_cmap = LinearSegmentedColormap.from_list('custom_orange_red', colors_new, N=256)

# Create the map
fig = plt.figure(figsize=(14, 9))
projection = ccrs.Robinson(central_longitude=0)
ax = fig.add_subplot(1, 1, 1, projection=projection)

# Add map features
ax.set_global()
ax.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
ax.add_feature(cfeature.OCEAN, facecolor='lightblue')

# Create optimized boundaries based on data distribution
bounds = np.array([0, 0.05, 0.1, 0.2, 0.35, 0.5, 1.0, 2.0, 3.0, 5.0, np.inf])

# Create labels for the colorbar
tick_labels = ['0', '0.05', '0.1', '0.2', '0.35', '0.5', '1.0', '2.0', '3.0', '5.0', '>5']
tick_positions = [0, 0.05, 0.1, 0.2, 0.35, 0.5, 1.0, 2.0, 3.0, 5.0, 6.0]

# Handle data > 5 by capping it at 6 for visualization
data_n2o_capped = data_n2o.copy()
data_n2o_capped['N2O_viz'] = np.where(data_n2o_capped['N2O'] > 5, 6.0, data_n2o_capped['N2O'])

# Create norm with the capped bounds
bounds_viz = np.array([0, 0.05, 0.1, 0.2, 0.35, 0.5, 1.0, 2.0, 3.0, 5.0, 6.0])
norm = BoundaryNorm(boundaries=bounds_viz, ncolors=custom_cmap.N)

# Plot N2O data with variable sizes based on lake area
sc = ax.scatter(
    data_n2o_capped['lon'], 
    data_n2o_capped['lat'], 
    s=marker_sizes,
    c=data_n2o_capped['N2O_viz'], 
    cmap=custom_cmap,
    norm=norm,
    alpha=0.7,
    edgecolor='k', 
    linewidth=0.1, 
    transform=ccrs.PlateCarree()
)

# æ·»åŠ æ ‡é¢˜
title_text = 'Nâ‚‚O flux (mg N mâ»Â² dâ»Â¹)'
ax.set_title(title_text, fontsize=16, pad=20)

# æ·»åŠ é¢ç§¯å›¾ä¾‹ï¼ˆç©ºå¿ƒåœ†åœˆæ ·å¼ï¼‰
legend_ax = fig.add_axes([0.1, 0.15, 0.8, 0.06])
legend_ax.set_xlim(0, 1)
legend_ax.set_ylim(0, 1)
legend_ax.axis('off')

# æ·»åŠ é¢ç§¯å›¾ä¾‹æ ‡é¢˜
legend_ax.text(0.1, 0.5, 'Lake Area (kmÂ²)', ha='left', va='center', fontsize=10)

# åˆ›å»ºå›¾ä¾‹ä¿¡æ¯ï¼Œä½¿ç”¨ä¸ä¸»å›¾å®Œå…¨ç›¸åŒçš„å¤§å°
area_legend_info = [
    ("0-0.1", size_intervals[0][2]),
    ("0.1-0.5", size_intervals[1][2]), 
    ("0.5-1", size_intervals[2][2]),
    ("1-5", size_intervals[3][2]),
    ("5-100", size_intervals[4][2]),
    (">100", size_intervals[5][2])
]

# ä½¿ç”¨ç©ºå¿ƒåœ†åœˆæ ·å¼ï¼Œå‚è€ƒæ‚¨æä¾›çš„æ ¼å¼
start_x = 0.22
spacing_x = 0.125

for i, (label, size) in enumerate(area_legend_info):
    x_pos = start_x + i * spacing_x
    
    # ç»˜åˆ¶ç©ºå¿ƒåœ†åœˆï¼šç™½è‰²å¡«å……ï¼Œé»‘è‰²ç²—è¾¹æ¡†
    legend_ax.scatter(x_pos, 0.5, s=size, facecolor='white', 
                     edgecolor='black', linewidth=1.2, alpha=1.0)
    
    # æ·»åŠ æ ‡ç­¾åœ¨åœ†åœˆå³ä¾§
    legend_ax.text(x_pos + 0.015, 0.5, label, ha='left', va='center', fontsize=9)

# Create colorbar for N2O flux
cbar = plt.colorbar(sc, ax=ax, orientation='horizontal', pad=0.06, shrink=0.7, aspect=40)
cbar_label = 'Nâ‚‚O flux (mg N mâ»Â² dâ»Â¹)'
cbar.set_label(cbar_label, fontsize=13)
cbar.set_ticks(tick_positions)
cbar.set_ticklabels(tick_labels)

# Add gridlines
ax.gridlines(draw_labels=False, linewidth=0.5, color='gray', alpha=0.5)

# Save the figure
plt.tight_layout()
plt.savefig('N2O_flux_map_area0815.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print("å¤§å°ä¸€è‡´çš„N2Oåœ°å›¾å·²ä¿å­˜!")
print(f"æ•°æ®ç‚¹æ€»æ•°: {len(data_n2o)}")
print(f"å¤§äº5çš„N2Oæ•°æ®ç‚¹: {len(data_n2o[data_n2o['N2O'] > 5])} ({len(data_n2o[data_n2o['N2O'] > 5])/len(data_n2o)*100:.1f}%)")
print(f"é¢ç§¯èŒƒå›´: {data_n2o['Areakm2'].min():.6f} - {data_n2o['Areakm2'].max():.1f} kmÂ²")

# æ˜¾ç¤ºå„åŒºé—´çš„æ•°æ®ç‚¹æ•°é‡
print("\nå„é¢ç§¯åŒºé—´çš„æ•°æ®ç‚¹åˆ†å¸ƒ:")
for i, (lower, upper, size) in enumerate(size_intervals):
    if upper == np.inf:
        mask = data_n2o['Areakm2'] >= lower
        print(f">{lower} kmÂ² (å¤§å°={size}): {mask.sum()} ä¸ªæ¹–æ³Š ({mask.sum()/len(data_n2o)*100:.1f}%)")
    else:
        mask = (data_n2o['Areakm2'] >= lower) & (data_n2o['Areakm2'] < upper)
        print(f"{lower}-{upper} kmÂ² (å¤§å°={size}): {mask.sum()} ä¸ªæ¹–æ³Š ({mask.sum()/len(data_n2o)*100:.1f}%)")

# éªŒè¯å›¾ä¾‹å¤§å°ä¸å®é™…ä½¿ç”¨å¤§å°çš„ä¸€è‡´æ€§
print("\nå›¾ä¾‹å¤§å°éªŒè¯:")
for i, (label, legend_size) in enumerate(area_legend_info):
    actual_size = size_intervals[i][2]
    print(f"{label}: å›¾ä¾‹å¤§å°={legend_size}, å®é™…å¤§å°={actual_size}, ä¸€è‡´={legend_size==actual_size}")
    
    
# æ•°æ®ç‚¹æ€»æ•°: 3238
# å¤§äº5çš„N2Oæ•°æ®ç‚¹: 33 (1.0%)
# é¢ç§¯èŒƒå›´: 0.000488 - 6782.8 kmÂ²

# å„é¢ç§¯åŒºé—´çš„æ•°æ®ç‚¹åˆ†å¸ƒ:
# 0-0.1 kmÂ² (å¤§å°=8): 93 ä¸ªæ¹–æ³Š (2.9%)
# 0.1-0.5 kmÂ² (å¤§å°=15): 1868 ä¸ªæ¹–æ³Š (57.7%)
# 0.5-1 kmÂ² (å¤§å°=25): 510 ä¸ªæ¹–æ³Š (15.8%)
# 1-5 kmÂ² (å¤§å°=40): 536 ä¸ªæ¹–æ³Š (16.6%)
# 5-100 kmÂ² (å¤§å°=60): 160 ä¸ªæ¹–æ³Š (4.9%)
# >100 kmÂ² (å¤§å°=80): 71 ä¸ªæ¹–æ³Š (2.2%)    
    
 
#%% æå–å®æµ‹æ¹–æ³Šæ‰€åœ¨ä½ç½®çš„æ°”å€™ç±»å‹ 251012

import pandas as pd
import rasterio
from rasterio.transform import rowcol
import numpy as np

# Load data
data = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# Select valid N2O data
data = data[data['N2O'].notna() & data['Areakm2'].notna()].copy()
data_n2o = data[data['N2O'] >= 0].copy()

# KÃ¶ppen-Geiger climate classification mapping
koppen_mapping = {
    1: 'Af',   2: 'Am',   3: 'Aw',   4: 'BWh',  5: 'BWk',
    6: 'BSh',  7: 'BSk',  8: 'Csa',  9: 'Csb',  10: 'Csc',
    11: 'Cwa', 12: 'Cwb', 13: 'Cwc', 14: 'Cfa', 15: 'Cfb',
    16: 'Cfc', 17: 'Dsa', 18: 'Dsb', 19: 'Dsc', 20: 'Dsd',
    21: 'Dwa', 22: 'Dwb', 23: 'Dwc', 24: 'Dwd', 25: 'Dfa',
    26: 'Dfb', 27: 'Dfc', 28: 'Dfd', 29: 'ET',  30: 'EF'
}

koppen_description = {
    1: 'Tropical, rainforest',
    2: 'Tropical, monsoon',
    3: 'Tropical, savannah',
    4: 'Arid, desert, hot',
    5: 'Arid, desert, cold',
    6: 'Arid, steppe, hot',
    7: 'Arid, steppe, cold',
    8: 'Temperate, dry summer, hot summer',
    9: 'Temperate, dry summer, warm summer',
    10: 'Temperate, dry summer, cold summer',
    11: 'Temperate, dry winter, hot summer',
    12: 'Temperate, dry winter, warm summer',
    13: 'Temperate, dry winter, cold summer',
    14: 'Temperate, no dry season, hot summer',
    15: 'Temperate, no dry season, warm summer',
    16: 'Temperate, no dry season, cold summer',
    17: 'Cold, dry summer, hot summer',
    18: 'Cold, dry summer, warm summer',
    19: 'Cold, dry summer, cold summer',
    20: 'Cold, dry summer, very cold winter',
    21: 'Cold, dry winter, hot summer',
    22: 'Cold, dry winter, warm summer',
    23: 'Cold, dry winter, cold summer',
    24: 'Cold, dry winter, very cold winter',
    25: 'Cold, no dry season, hot summer',
    26: 'Cold, no dry season, warm summer',
    27: 'Cold, no dry season, cold summer',
    28: 'Cold, no dry season, very cold winter',
    29: 'Polar, tundra',
    30: 'Polar, frost'
}

# Climate zone mapping based on Color Index
def get_climate_zone(climate_index):
    """Map climate index to broader climate zone"""
    if pd.isna(climate_index):
        return 'Unknown'
    
    index = int(climate_index)
    if 1 <= index <= 3:
        return 'Tropical'
    elif 4 <= index <= 7:
        return 'Arid'
    elif 8 <= index <= 16:
        return 'Temperate'
    elif 17 <= index <= 28:
        return 'Cold'
    elif 29 <= index <= 30:
        return 'Polar'
    else:
        return 'Unknown'

# Load KÃ¶ppen-Geiger TIF file
tif_path = r"D:\Code_running\Global_lake_GHG\koppen_geiger_tif\1991_2020\koppen_geiger_0p00833333.tif"

with rasterio.open(tif_path) as src:
    # Get the transformation matrix
    transform = src.transform
    
    # Initialize lists to store results
    climate_indices = []
    climate_codes = []
    climate_descriptions = []
    
    # Extract climate data for each lake location
    for idx, row in data_n2o.iterrows():
        lon = row['lon']
        lat = row['lat']
        
        try:
            # Convert lon/lat to pixel row/col
            py, px = rowcol(transform, lon, lat)
            
            # Check if coordinates are within raster bounds
            if 0 <= py < src.height and 0 <= px < src.width:
                # Read the pixel value (climate index)
                window = rasterio.windows.Window(px, py, 1, 1)
                pixel_value = src.read(1, window=window)[0, 0]
                
                # Store the climate index
                climate_indices.append(int(pixel_value))
                
                # Map to climate code
                climate_code = koppen_mapping.get(int(pixel_value), 'Unknown')
                climate_codes.append(climate_code)
                
                # Map to description
                climate_desc = koppen_description.get(int(pixel_value), 'Unknown')
                climate_descriptions.append(climate_desc)
            else:
                # Coordinates outside raster bounds
                climate_indices.append(np.nan)
                climate_codes.append('OutOfBounds')
                climate_descriptions.append('Out of bounds')
                
        except Exception as e:
            print(f"Error processing row {idx}: {e}")
            climate_indices.append(np.nan)
            climate_codes.append('Error')
            climate_descriptions.append('Error')

# Add results to dataframe
data_n2o['climate_index'] = climate_indices
data_n2o['climate_code'] = climate_codes
data_n2o['climate_description'] = climate_descriptions

# Add climate zone column (æ–°å¢çš„æ°”å€™å¸¦åˆ—)
data_n2o['climate_zone'] = data_n2o['climate_index'].apply(get_climate_zone)

# Display results
print(f"Total lakes: {len(data_n2o)}")
print(f"\nClimate code distribution:")
print(data_n2o['climate_code'].value_counts())
print(f"\nClimate zone distribution:")
print(data_n2o['climate_zone'].value_counts())

# Optional: Save results
data_n2o.to_excel('GHGdata_N2O_with_climate.xlsx', index=False)
print("\nResults saved to 'GHGdata_N2O_with_climate.xlsx'")


#%% module AttributeError é”™è¯¯è§£å†³ 251012

pip install matplotlib==3.7.3


#%% ç»˜åˆ¶ä¸åŒæ°”å€™å¸¦æ¹–æ³ŠN2Oç®±çº¿å›¾ 251012


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®å­—ä½“, ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# åŠ è½½æ•°æ®
data = pd.read_excel('GHGdata_N2O_with_climate.xlsx')

# é€‰æ‹©æœ‰æ•ˆçš„N2Oæ•°æ®
data = data[data['N2O'].notna() & data['Areakm2'].notna()].copy()
# data_n2o = data[data['N2O'] >= 0].copy()

# æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
data_n2o = data[
    (data['N2O'] > data['N2O'].quantile(0.01)) & 
    (data['N2O'] < data['N2O'].quantile(0.99))
].copy()


# ç»Ÿè®¡æ°”å€™å¸¦çš„é¡ºåº
zone_order = ['Tropical', 'Arid', 'Temperate', 'Cold', 'Polar']
zone_order = [z for z in zone_order if z in data_n2o['climate_zone'].unique()]

# è‡ªå®šä¹‰é…è‰²æ–¹æ¡ˆ
custom_colors = ['#2A6B2D', '#F5A623', '#8E44AD', '#2980B9', '#D35400']
palette = custom_colors[:len(zone_order)]

# åˆ›å»ºå›¾å½¢
fig, ax = plt.subplots(figsize=(10, 6))

# ä½¿ç”¨seabornç»˜åˆ¶ç®±çº¿å›¾
sns.boxplot(data=data_n2o, 
            x='climate_zone', 
            y='N2O',
            order=zone_order,
            palette=palette,
            width=0.6,
            linewidth=1.5,
            flierprops=dict(marker='o', markerfacecolor='red', markersize=5, 
                           markeredgecolor='red', alpha=0.5),
            medianprops=dict(color='darkred', linewidth=2.5),
            boxprops=dict(edgecolor='black', linewidth=1.5, alpha=0.8),
            whiskerprops=dict(color='black', linewidth=1.5),
            capprops=dict(color='black', linewidth=1.5),
            ax=ax)

# æ·»åŠ å‡å€¼ç‚¹
means = data_n2o.groupby('climate_zone')['N2O'].mean()
positions = range(len(zone_order))
ax.scatter(positions, [means[zone] for zone in zone_order], 
          color='blue', s=100, marker='D', zorder=3, 
          edgecolors='white', linewidth=1.5, label='Mean')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('Climate Zone', fontsize=14, fontweight='bold')
ax.set_ylabel('N$_2$O flux (mg N m$^{-2}$ d$^{-1}$)', fontsize=14, fontweight='bold')
ax.set_title('Nâ‚‚O Flux Distribution across Climate Zones', 
             fontsize=16, fontweight='bold', pad=15)

# æ—‹è½¬xè½´æ ‡ç­¾
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=12)

# ä¼˜åŒ–ç½‘æ ¼
ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.8)
ax.set_axisbelow(True)

# æ·»åŠ å›¾ä¾‹
from matplotlib.lines import Line2D
from matplotlib.patches import Patch

legend_elements = [
    Line2D([0], [0], color='darkred', linewidth=2.5, label='Median'),
    Line2D([0], [0], marker='D', color='w', markerfacecolor='blue', 
           markersize=8, markeredgecolor='white', markeredgewidth=1.5, label='Mean'),
    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', 
           markersize=5, alpha=0.5, linestyle='none', label='Outliers')
]
ax.legend(handles=legend_elements, loc='upper right', fontsize=12, framealpha=0.9)

# åœ¨æ¯ä¸ªç®±çº¿å›¾ä¸‹æ–¹æ ‡æ³¨æ ·æœ¬æ•°
y_min = ax.get_ylim()[0]
for i, zone in enumerate(zone_order):
    count = len(data_n2o[data_n2o['climate_zone'] == zone])
    ax.text(i, y_min, f'n={count}', 
            ha='center', va='top', fontsize=10, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                     edgecolor='gray', alpha=0.7))

# ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§è¾¹æ¡†
sns.despine()

plt.tight_layout()
plt.savefig('N2O_Climate_Zones_Boxplot_Optimized.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nå›¾è¡¨å·²ä¿å­˜ä¸º 'N2O_Climate_Zones_Boxplot_Optimized.png'")

# æ‰“å°æ¯ä¸ªæ°”å€™å¸¦çš„å…³é”®ç»Ÿè®¡é‡
print("\nå„æ°”å€™å¸¦å…³é”®ç»Ÿè®¡é‡:")
for zone in zone_order:
    zone_data = data_n2o[data_n2o['climate_zone'] == zone]['N2O']
    print(f"\n{zone}:")
    print(f"  æ ·æœ¬æ•°: {len(zone_data)}")
    print(f"  å¹³å‡å€¼: {zone_data.mean():.2f} Î¼mol/mÂ²/d")
    print(f"  ä¸­ä½æ•°: {zone_data.median():.2f} Î¼mol/mÂ²/d")
    print(f"  æ ‡å‡†å·®: {zone_data.std():.2f} Î¼mol/mÂ²/d")
    print(f"  èŒƒå›´: {zone_data.min():.5f} - {zone_data.max():.5f} Î¼mol/mÂ²/d")


#%% ç»˜åˆ¶ä¸åŒæ°”å€™å¸¦æ¹–æ³ŠN2Oç®±çº¿å›¾-ä½¿ç”¨logN2O  251018


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®å­—ä½“, ç¡®ä¿ä¸Šæ ‡æ­£å¸¸æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS','DejaVu Sans', 'SimHei']
plt.rcParams['mathtext.fontset'] = 'dejavusans'

# åŠ è½½æ•°æ®
data = pd.read_excel('GHGdata_N2O_with_climate.xlsx')

# é€‰æ‹©æœ‰æ•ˆçš„N2Oæ•°æ®
data = data[data['N2O'].notna() & data['Areakm2'].notna()].copy()

# æ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼(ä¿ç•™99%æ•°æ®)
data_n2o = data[
    (data['N2O'] > data['N2O'].quantile(0.01)) & 
    (data['N2O'] < data['N2O'].quantile(0.99))
].copy()

# æ·»åŠ å¯¹æ•°è½¬æ¢
data_n2o['Log_N2O'] = np.log10(data_n2o['N2O'] + 1e-10)

# ç»Ÿè®¡æ°”å€™å¸¦çš„é¡ºåº
zone_order = ['Tropical', 'Arid', 'Temperate', 'Cold', 'Polar']
zone_order = [z for z in zone_order if z in data_n2o['climate_zone'].unique()]

# è‡ªå®šä¹‰é…è‰²æ–¹æ¡ˆ
custom_colors = ['#2A6B2D', '#F5A623', '#8E44AD', '#2980B9', '#D35400']
palette = custom_colors[:len(zone_order)]

# **åˆ›å»ºå›¾å½¢ - å¢åŠ é«˜åº¦**
fig, ax = plt.subplots(figsize=(10, 7))  # ä»6æ”¹ä¸º7

# ä½¿ç”¨seabornç»˜åˆ¶ç®±çº¿å›¾
sns.boxplot(data=data_n2o, 
            x='climate_zone', 
            y='Log_N2O',
            order=zone_order,
            palette=palette,
            width=0.6,
            linewidth=1.5,
            flierprops=dict(marker='o', markerfacecolor='red', markersize=5, 
                           markeredgecolor='red', alpha=0.5),
            medianprops=dict(color='darkred', linewidth=2.5),
            boxprops=dict(edgecolor='black', linewidth=1.5, alpha=0.8),
            whiskerprops=dict(color='black', linewidth=1.5),
            capprops=dict(color='black', linewidth=1.5),
            ax=ax)

# æ·»åŠ å‡å€¼ç‚¹
means = data_n2o.groupby('climate_zone')['Log_N2O'].mean()
positions = range(len(zone_order))
ax.scatter(positions, [means[zone] for zone in zone_order], 
          color='blue', s=100, marker='D', zorder=3, 
          edgecolors='white', linewidth=1.5, label='Mean')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('Climate Zone', fontsize=14, fontweight='bold')
ax.set_ylabel('log$_{10}$(N$_2$O flux) (mg N m$^{-2}$ d$^{-1}$)', fontsize=14, fontweight='bold')
ax.set_title('Nâ‚‚O Flux Distribution across Climate Zones', 
             fontsize=16, fontweight='bold', pad=15)

# å°†xè½´æ ‡ç­¾æ”¹ä¸ºæ°´å¹³æ˜¾ç¤º
plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center', fontsize=12)

# **æ‰‹åŠ¨è®¾ç½®yè½´èŒƒå›´ï¼Œå¢åŠ åº•éƒ¨ç©ºé—´**
y_min_data, y_max_data = ax.get_ylim()
# æ–¹æ³•1ï¼šåœ¨åº•éƒ¨å¢åŠ å›ºå®šçš„ç©ºé—´ï¼ˆä¾‹å¦‚å¢åŠ 0.5ä¸ªå•ä½ï¼‰
ax.set_ylim(y_min_data - 0.5, y_max_data)
# æ–¹æ³•2ï¼šæŒ‰æ¯”ä¾‹æ‰©å±•ï¼ˆä¾‹å¦‚åº•éƒ¨æ‰©å±•15%ï¼‰
# y_range = y_max_data - y_min_data
# ax.set_ylim(y_min_data - 0.15 * y_range, y_max_data)

# ä¼˜åŒ–ç½‘æ ¼
ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.8)
ax.set_axisbelow(True)

# æ·»åŠ å›¾ä¾‹
from matplotlib.lines import Line2D
legend_elements = [
    Line2D([0], [0], color='darkred', linewidth=2.5, label='Median'),
    Line2D([0], [0], marker='D', color='w', markerfacecolor='blue', 
           markersize=8, markeredgecolor='white', markeredgewidth=1.5, label='Mean'),
    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', 
           markersize=5, alpha=0.5, linestyle='none', label='Outliers')
]
ax.legend(handles=legend_elements, loc='upper right', fontsize=12, framealpha=0.9)

# **åœ¨æ¯ä¸ªç®±çº¿å›¾ä¸‹æ–¹æ ‡æ³¨æ ·æœ¬æ•° - ä½¿ç”¨æ–°çš„y_min**
y_min = ax.get_ylim()[0]
for i, zone in enumerate(zone_order):
    count = len(data_n2o[data_n2o['climate_zone'] == zone])
    ax.text(i, y_min + 0.1, f'n={count}',  # ç¨å¾®å‘ä¸Šåç§»0.1
            ha='center', va='bottom', fontsize=10, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                     edgecolor='gray', alpha=0.7))

# ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§è¾¹æ¡†
sns.despine()
plt.tight_layout()
plt.savefig('N2O_Climate_Zones_Boxplot_Optimized_v2.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nå›¾è¡¨å·²ä¿å­˜ä¸º 'N2O_Climate_Zones_Boxplot_Optimized_v2.png'")

# æ‰“å°æ¯ä¸ªæ°”å€™å¸¦çš„å…³é”®ç»Ÿè®¡é‡
print("\nå„æ°”å€™å¸¦å…³é”®ç»Ÿè®¡é‡:")
for zone in zone_order:
    zone_data = data_n2o[data_n2o['climate_zone'] == zone]['N2O']
    zone_data_log = data_n2o[data_n2o['climate_zone'] == zone]['Log_N2O']
    print(f"\n{zone}:")
    print(f"  æ ·æœ¬æ•°: {len(zone_data)}")
    print(f"  åŸå°ºåº¦å¹³å‡å€¼: {zone_data.mean():.2f} mg N/mÂ²/d")
    print(f"  åŸå°ºåº¦ä¸­ä½æ•°: {zone_data.median():.2f} mg N/mÂ²/d")
    print(f"  å¯¹æ•°å°ºåº¦å¹³å‡å€¼: {zone_data_log.mean():.4f}")
    print(f"  å¯¹æ•°å°ºåº¦ä¸­ä½æ•°: {zone_data_log.median():.4f}")
    print(f"  æ ‡å‡†å·®: {zone_data.std():.2f} mg N/mÂ²/d")
    print(f"  èŒƒå›´: {zone_data.min():.5f} - {zone_data.max():.5f} mg N/mÂ²/d")
    
    
#%% æ£€æŸ¥N2Oçš„æ€»æ’æ”¾é‡ 0728

import pandas as pd
import numpy as np

# Load data
df = pd.read_csv("global_N2O_predictions0728.csv")


# Calculate and print total global emissions
total_global_emissions = df['N2Oemission'].sum() / 1e9  # Convert to Tg
print(f"Total global lake N2O emissions: {total_global_emissions:.4f} Tg N2O yâ»Â¹")


Total global lake N2O emissions: 0.1363 Tg N2O yâ»Â¹


#%% å›½å®¶å’Œå¤§æ´²N2Oæ’æ”¾ç»Ÿè®¡ 0821


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è½½æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# æ•°æ®åŸºæœ¬ä¿¡æ¯
print("=== æ•°æ®åŸºæœ¬ä¿¡æ¯ ===")
print(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
print(f"åŒ…å«çš„å›½å®¶æ•°: {df['Country'].nunique()}")
print(f"åŒ…å«çš„å¤§æ´²æ•°: {df['Continent'].nunique()}")
print(f"\nå„å¤§æ´²åŒ…å«çš„å›½å®¶:")
for continent in df['Continent'].unique():
    countries = df[df['Continent'] == continent]['Country'].unique()
    print(f"  {continent}: {len(countries)}ä¸ªå›½å®¶")

print("\n" + "="*50)

# 1. è®¡ç®—æ€»çš„å…¨çƒæ’æ”¾é‡
total_global_emissions = df['N2Oemission'].sum() / 1e9  # è½¬æ¢ä¸ºTg
print(f"æ€»å…¨çƒæ¹–æ³ŠN2Oæ’æ”¾é‡: {total_global_emissions:.4f} Tg N2O yâ»Â¹")

print("\n" + "="*50)

# 2. æŒ‰å›½å®¶ç»Ÿè®¡N2Oæ’æ”¾
print("=== æŒ‰å›½å®¶ç»Ÿè®¡ ===")
country_emissions = df.groupby('Country')['N2Oemission'].sum().sort_values(ascending=False)
country_emissions_tg = country_emissions / 1e9  # è½¬æ¢ä¸ºTg

print("å‰10åå›½å®¶N2Oæ’æ”¾é‡:")
for i, (country, emission) in enumerate(country_emissions_tg.head(10).items(), 1):
    percentage = (emission / total_global_emissions) * 100
    print(f"{i:2d}. {country:<20}: {emission:.4f} Tg ({percentage:.2f}%)")

print("\n" + "="*50)

# 3. æŒ‰å¤§æ´²ç»Ÿè®¡N2Oæ’æ”¾
print("=== æŒ‰å¤§æ´²ç»Ÿè®¡ ===")
continent_emissions = df.groupby('Continent')['N2Oemission'].sum().sort_values(ascending=False)
continent_emissions_tg = continent_emissions / 1e9  # è½¬æ¢ä¸ºTg

print("å„å¤§æ´²N2Oæ’æ”¾é‡:")
for i, (continent, emission) in enumerate(continent_emissions_tg.items(), 1):
    percentage = (emission / total_global_emissions) * 100
    print(f"{i}. {continent:<15}: {emission:.4f} Tg ({percentage:.2f}%)")

print("\n" + "="*50)

# 4. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
print("=== è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===")

# æ¯ä¸ªå¤§æ´²çš„è¯¦ç»†ä¿¡æ¯
print("\nå„å¤§æ´²è¯¦ç»†ç»Ÿè®¡:")
for continent in continent_emissions_tg.index:
    continent_data = df[df['Continent'] == continent]
    continent_total = continent_data['N2Oemission'].sum() / 1e9
    country_count = continent_data['Country'].nunique()
    avg_per_country = continent_total / country_count
    
    print(f"\n{continent}:")
    print(f"  æ€»æ’æ”¾é‡: {continent_total:.4f} Tg")
    print(f"  å›½å®¶æ•°é‡: {country_count}")
    print(f"  å¹³å‡æ¯å›½: {avg_per_country:.4f} Tg")
    
    # è¯¥å¤§æ´²å‰5åå›½å®¶
    top_countries = continent_data.groupby('Country')['N2Oemission'].sum().sort_values(ascending=False).head(5)
    print(f"  ä¸»è¦å›½å®¶:")
    for country, emission in (top_countries / 1e9).items():
        print(f"    {country}: {emission:.4f} Tg")

print("\n" + "="*50)

# 5. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 5.1 å¤§æ´²æ’æ”¾é‡é¥¼å›¾
axes[0, 0].pie(continent_emissions_tg.values, labels=continent_emissions_tg.index, 
               autopct='%1.1f%%', startangle=90)
axes[0, 0].set_title('å„å¤§æ´²N2Oæ’æ”¾é‡åˆ†å¸ƒ')

# 5.2 å¤§æ´²æ’æ”¾é‡æŸ±çŠ¶å›¾
continent_emissions_tg.plot(kind='bar', ax=axes[0, 1], color='skyblue')
axes[0, 1].set_title('å„å¤§æ´²N2Oæ’æ”¾é‡')
axes[0, 1].set_ylabel('æ’æ”¾é‡ (Tg)')
axes[0, 1].tick_params(axis='x', rotation=45)

# 5.3 å‰15åå›½å®¶æ’æ”¾é‡
top15_countries = country_emissions_tg.head(15)
top15_countries.plot(kind='bar', ax=axes[1, 0], color='lightcoral')
axes[1, 0].set_title('å‰15åå›½å®¶N2Oæ’æ”¾é‡')
axes[1, 0].set_ylabel('æ’æ”¾é‡ (Tg)')
axes[1, 0].tick_params(axis='x', rotation=45)

# 5.4 å„å¤§æ´²å›½å®¶æ•°é‡åˆ†å¸ƒ
continent_country_count = df.groupby('Continent')['Country'].nunique()
continent_country_count.plot(kind='bar', ax=axes[1, 1], color='lightgreen')
axes[1, 1].set_title('å„å¤§æ´²å›½å®¶æ•°é‡')
axes[1, 1].set_ylabel('å›½å®¶æ•°é‡')
axes[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 6. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
print("=== ä¿å­˜ç»“æœ ===")

# ä¿å­˜å›½å®¶æ’æ”¾ç»Ÿè®¡
country_results = pd.DataFrame({
    'Country': country_emissions.index,
    'N2O_Emission_Gg': country_emissions.values,
    'N2O_Emission_Tg': country_emissions_tg.values,
    'Percentage': (country_emissions_tg / total_global_emissions * 100).values
})

# æ·»åŠ å¤§æ´²ä¿¡æ¯
country_continent_map = df.groupby('Country')['Continent'].first()
country_results['Continent'] = country_results['Country'].map(country_continent_map)

# ä¿å­˜å¤§æ´²æ’æ”¾ç»Ÿè®¡
continent_results = pd.DataFrame({
    'Continent': continent_emissions.index,
    'N2O_Emission_Gg': continent_emissions.values,
    'N2O_Emission_Tg': continent_emissions_tg.values,
    'Percentage': (continent_emissions_tg / total_global_emissions * 100).values,
    'Country_Count': [df[df['Continent'] == cont]['Country'].nunique() for cont in continent_emissions.index]
})

# ä¿å­˜åˆ°CSVæ–‡ä»¶
country_results.to_csv('country_N2O_emissions.csv', index=False, encoding='utf-8-sig')
continent_results.to_csv('continent_N2O_emissions.csv', index=False, encoding='utf-8-sig')

print("ç»“æœå·²ä¿å­˜åˆ°:")
print("- country_N2O_emissions.csv (å›½å®¶æ’æ”¾ç»Ÿè®¡)")
print("- continent_N2O_emissions.csv (å¤§æ´²æ’æ”¾ç»Ÿè®¡)")

# 7. ç®€è¦æ€»ç»“
print("\n" + "="*50)
print("=== åˆ†ææ€»ç»“ ===")
print(f"1. å…¨çƒæ¹–æ³ŠN2Oæ€»æ’æ”¾é‡: {total_global_emissions:.4f} Tg/å¹´")
print(f"2. æ’æ”¾é‡æœ€é«˜çš„å¤§æ´²: {continent_emissions_tg.index[0]} ({continent_emissions_tg.iloc[0]:.4f} Tg)")
print(f"3. æ’æ”¾é‡æœ€é«˜çš„å›½å®¶: {country_emissions_tg.index[0]} ({country_emissions_tg.iloc[0]:.4f} Tg)")
print(f"4. å…±æ¶‰åŠ {df['Country'].nunique()} ä¸ªå›½å®¶ï¼Œ{df['Continent'].nunique()} ä¸ªå¤§æ´²")

# å‰3åå¤§æ´²è´¡çŒ®çš„æ¯”ä¾‹
top3_continents_pct = (continent_emissions_tg.head(3).sum() / total_global_emissions * 100)
print(f"5. å‰3åå¤§æ´²è´¡çŒ®äº†å…¨çƒ {top3_continents_pct:.1f}% çš„æ’æ”¾é‡")



=== æŒ‰å›½å®¶ç»Ÿè®¡ ===
å‰10åå›½å®¶N2Oæ’æ”¾é‡:
 1. Russia              : 0.0385 Tg (28.25%)
 2. Canada              : 0.0281 Tg (20.58%)
 3. United States of America: 0.0229 Tg (16.82%)
 4. China               : 0.0057 Tg (4.20%)
 5. Uganda              : 0.0039 Tg (2.83%)
 6. Democratic Republic of the Congo: 0.0026 Tg (1.92%)
 7. Kazakhstan          : 0.0024 Tg (1.79%)
 8. Brazil              : 0.0020 Tg (1.47%)
 9. Sweden              : 0.0020 Tg (1.47%)
10. Australia           : 0.0018 Tg (1.29%)

=== æŒ‰å¤§æ´²ç»Ÿè®¡ ===
å„å¤§æ´²N2Oæ’æ”¾é‡:
1. North America  : 0.0526 Tg (38.55%)
2. Europe         : 0.0460 Tg (33.71%)
3. Asia           : 0.0160 Tg (11.76%)
4. Africa         : 0.0138 Tg (10.09%)
5. South America  : 0.0054 Tg (3.97%)
6. Oceania        : 0.0026 Tg (1.93%)

å„å¤§æ´²è¯¦ç»†ç»Ÿè®¡:

North America:
  æ€»æ’æ”¾é‡: 0.0526 Tg
  å›½å®¶æ•°é‡: 22
  å¹³å‡æ¯å›½: 0.0024 Tg
  ä¸»è¦å›½å®¶:
    Canada: 0.0281 Tg
    United States of America: 0.0229 Tg
    Mexico: 0.0006 Tg
    Nicaragua: 0.0004 Tg
    Denmark: 0.0003 Tg

Europe:
  æ€»æ’æ”¾é‡: 0.0460 Tg
  å›½å®¶æ•°é‡: 39
  å¹³å‡æ¯å›½: 0.0012 Tg
  ä¸»è¦å›½å®¶:
    Russia: 0.0385 Tg
    Sweden: 0.0020 Tg
    Finland: 0.0017 Tg
    Ukraine: 0.0010 Tg
    Norway: 0.0006 Tg

Asia:
  æ€»æ’æ”¾é‡: 0.0160 Tg
  å›½å®¶æ•°é‡: 51
  å¹³å‡æ¯å›½: 0.0003 Tg
  ä¸»è¦å›½å®¶:
    China: 0.0057 Tg
    Kazakhstan: 0.0024 Tg
    India: 0.0010 Tg
    Uzbekistan: 0.0010 Tg
    Turkey: 0.0008 Tg

Africa:
  æ€»æ’æ”¾é‡: 0.0138 Tg
  å›½å®¶æ•°é‡: 54
  å¹³å‡æ¯å›½: 0.0003 Tg
  ä¸»è¦å›½å®¶:
    Uganda: 0.0039 Tg
    Democratic Republic of the Congo: 0.0026 Tg
    Malawi: 0.0015 Tg
    Chad: 0.0010 Tg
    Botswana: 0.0005 Tg

South America:
  æ€»æ’æ”¾é‡: 0.0054 Tg
  å›½å®¶æ•°é‡: 13
  å¹³å‡æ¯å›½: 0.0004 Tg
  ä¸»è¦å›½å®¶:
    Brazil: 0.0020 Tg
    Argentina: 0.0015 Tg
    Bolivia: 0.0008 Tg
    Chile: 0.0005 Tg
    Colombia: 0.0002 Tg

Oceania:
  æ€»æ’æ”¾é‡: 0.0026 Tg
  å›½å®¶æ•°é‡: 14
  å¹³å‡æ¯å›½: 0.0002 Tg
  ä¸»è¦å›½å®¶:
    Australia: 0.0018 Tg
    Papua New Guinea: 0.0007 Tg
    New Zealand: 0.0001 Tg
    France: 0.0000 Tg
    Kiribati: 0.0000 Tg

#%% å›½å®¶å’Œå¤§æ´²åœ¨æ¹–æ³Šé¢ç§¯ N2Oæ’æ”¾å¼ºåº¦ æ’æ”¾æ€»é‡ 251105

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è½½æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# æ•°æ®åŸºæœ¬ä¿¡æ¯
print("=== æ•°æ®åŸºæœ¬ä¿¡æ¯ ===")
print(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}")
print(f"åŒ…å«çš„å›½å®¶æ•°: {df['Country'].nunique()}")
print(f"åŒ…å«çš„å¤§æ´²æ•°: {df['Continent'].nunique()}")
print(f"\nå„å¤§æ´²åŒ…å«çš„å›½å®¶:")
for continent in df['Continent'].unique():
    countries = df[df['Continent'] == continent]['Country'].unique()
    print(f"  {continent}: {len(countries)}ä¸ªå›½å®¶")

print("\n" + "="*50)

# 1. è®¡ç®—æ€»çš„å…¨çƒæ’æ”¾é‡å’Œæ¹–æ³Šé¢ç§¯
total_global_emissions = df['N2Oemission'].sum() / 1e9  # è½¬æ¢ä¸ºTg
total_global_lake_area = df['Lake_area'].sum() / 1e6  # è½¬æ¢ä¸ºç™¾ä¸‡kmÂ²

print(f"æ€»å…¨çƒæ¹–æ³ŠN2Oæ’æ”¾é‡: {total_global_emissions:.4f} Tg N2O yâ»Â¹")
print(f"æ€»å…¨çƒæ¹–æ³Šé¢ç§¯: {total_global_lake_area:.4f} ç™¾ä¸‡ kmÂ²")

print("\n" + "="*50)

# 2. æŒ‰å›½å®¶ç»Ÿè®¡N2Oæ’æ”¾å’Œæ¹–æ³Šé¢ç§¯
print("=== æŒ‰å›½å®¶ç»Ÿè®¡ ===")
country_stats = df.groupby('Country').agg({
    'N2Oemission': 'sum',
    'Lake_area': 'sum'
}).sort_values('N2Oemission', ascending=False)

country_stats['N2Oemission_Tg'] = country_stats['N2Oemission'] / 1e9
country_stats['Lake_area_km2'] = country_stats['Lake_area']
country_stats['Lake_area_million_km2'] = country_stats['Lake_area'] / 1e6
country_stats['Emission_percentage'] = (country_stats['N2Oemission_Tg'] / total_global_emissions) * 100
country_stats['Area_percentage'] = (country_stats['Lake_area'] / df['Lake_area'].sum()) * 100
country_stats['Emission_intensity'] = country_stats['N2Oemission'] / country_stats['Lake_area']  # g/kmÂ²

print("\nå‰10åå›½å®¶N2Oæ’æ”¾é‡å’Œæ¹–æ³Šé¢ç§¯:")
print(f"{'æ’å':<4} {'å›½å®¶':<20} {'æ’æ”¾é‡(Tg)':<12} {'æ’æ”¾å æ¯”':<10} {'æ¹–æ³Šé¢ç§¯(ä¸‡kmÂ²)':<15} {'é¢ç§¯å æ¯”':<10} {'æ’æ”¾å¼ºåº¦(g/kmÂ²)':<15}")
print("-" * 110)
for i, (country, row) in enumerate(country_stats.head(10).iterrows(), 1):
    print(f"{i:<4} {country:<20} {row['N2Oemission_Tg']:<12.4f} {row['Emission_percentage']:<10.2f}% "
          f"{row['Lake_area_km2']/1e4:<15.2f} {row['Area_percentage']:<10.2f}% "
          f"{row['Emission_intensity']:<15.2f}")

# æŒ‰æ¹–æ³Šé¢ç§¯æ’åºçš„å‰10å
country_stats_by_area = country_stats.sort_values('Lake_area', ascending=False)
print("\nå‰10åå›½å®¶æ¹–æ³Šé¢ç§¯:")
print(f"{'æ’å':<4} {'å›½å®¶':<20} {'æ¹–æ³Šé¢ç§¯(ä¸‡kmÂ²)':<15} {'é¢ç§¯å æ¯”':<10} {'æ’æ”¾é‡(Tg)':<12} {'æ’æ”¾å æ¯”':<10}")
print("-" * 90)
for i, (country, row) in enumerate(country_stats_by_area.head(10).iterrows(), 1):
    print(f"{i:<4} {country:<20} {row['Lake_area_km2']/1e4:<15.2f} {row['Area_percentage']:<10.2f}% "
          f"{row['N2Oemission_Tg']:<12.4f} {row['Emission_percentage']:<10.2f}%")

print("\n" + "="*50)

# 3. æŒ‰å¤§æ´²ç»Ÿè®¡N2Oæ’æ”¾å’Œæ¹–æ³Šé¢ç§¯
print("=== æŒ‰å¤§æ´²ç»Ÿè®¡ ===")
continent_stats = df.groupby('Continent').agg({
    'N2Oemission': 'sum',
    'Lake_area': 'sum',
    'Country': 'nunique'
}).rename(columns={'Country': 'Country_count'})

continent_stats['N2Oemission_Tg'] = continent_stats['N2Oemission'] / 1e9
continent_stats['Lake_area_million_km2'] = continent_stats['Lake_area'] / 1e6
continent_stats['Emission_percentage'] = (continent_stats['N2Oemission_Tg'] / total_global_emissions) * 100
continent_stats['Area_percentage'] = (continent_stats['Lake_area'] / df['Lake_area'].sum()) * 100
continent_stats['Emission_intensity'] = continent_stats['N2Oemission'] / continent_stats['Lake_area']  # g/kmÂ²
continent_stats = continent_stats.sort_values('N2Oemission', ascending=False)

print("\nå„å¤§æ´²N2Oæ’æ”¾é‡å’Œæ¹–æ³Šé¢ç§¯:")
print(f"{'å¤§æ´²':<15} {'æ’æ”¾é‡(Tg)':<12} {'æ’æ”¾å æ¯”':<10} {'æ¹–æ³Šé¢ç§¯(ä¸‡kmÂ²)':<15} {'é¢ç§¯å æ¯”':<10} {'æ’æ”¾å¼ºåº¦(g/kmÂ²)':<15} {'å›½å®¶æ•°':<8}")
print("-" * 110)
for continent, row in continent_stats.iterrows():
    print(f"{continent:<15} {row['N2Oemission_Tg']:<12.4f} {row['Emission_percentage']:<10.2f}% "
          f"{row['Lake_area']/1e4:<15.2f} {row['Area_percentage']:<10.2f}% "
          f"{row['Emission_intensity']:<15.2f} {row['Country_count']:<8.0f}")

print("\n" + "="*50)

# 4. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
print("=== è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===")

for continent in continent_stats.index:
    continent_data = df[df['Continent'] == continent]
    continent_emission = continent_data['N2Oemission'].sum() / 1e9
    continent_area = continent_data['Lake_area'].sum() / 1e4  # ä¸‡kmÂ²
    country_count = continent_data['Country'].nunique()
    
    print(f"\n{continent}:")
    print(f"  æ€»æ’æ”¾é‡: {continent_emission:.4f} Tg ({continent_emission/total_global_emissions*100:.2f}%)")
    print(f"  æ€»æ¹–æ³Šé¢ç§¯: {continent_area:.2f} ä¸‡kmÂ² ({continent_area*1e4/df['Lake_area'].sum()*100:.2f}%)")
    print(f"  å›½å®¶æ•°é‡: {country_count}")
    print(f"  å¹³å‡æ¯å›½æ’æ”¾: {continent_emission/country_count:.4f} Tg")
    print(f"  å¹³å‡æ¯å›½é¢ç§¯: {continent_area/country_count:.2f} ä¸‡kmÂ²")
    print(f"  æ’æ”¾å¼ºåº¦: {continent_emission*1e9/continent_area/1e4:.2f} g/kmÂ²")
    
    # è¯¥å¤§æ´²å‰5åå›½å®¶
    top_countries = continent_data.groupby('Country').agg({
        'N2Oemission': 'sum',
        'Lake_area': 'sum'
    }).sort_values('N2Oemission', ascending=False).head(5)
    
    print(f"  ä¸»è¦å›½å®¶:")
    for country, row in top_countries.iterrows():
        emission = row['N2Oemission'] / 1e9
        area = row['Lake_area'] / 1e4
        print(f"    {country}: æ’æ”¾ {emission:.4f} Tg, é¢ç§¯ {area:.2f} ä¸‡kmÂ²")

print("\n" + "="*50)

# 5. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
fig = plt.figure(figsize=(18, 14))

# 5.1 å¤§æ´²æ’æ”¾é‡é¥¼å›¾
ax1 = plt.subplot(3, 3, 1)
ax1.pie(continent_stats['N2Oemission_Tg'].values, labels=continent_stats.index, 
        autopct='%1.1f%%', startangle=90)
ax1.set_title('å„å¤§æ´²N2Oæ’æ”¾é‡åˆ†å¸ƒ')

# 5.2 å¤§æ´²æ¹–æ³Šé¢ç§¯é¥¼å›¾
ax2 = plt.subplot(3, 3, 2)
ax2.pie(continent_stats['Lake_area'].values, labels=continent_stats.index, 
        autopct='%1.1f%%', startangle=90)
ax2.set_title('å„å¤§æ´²æ¹–æ³Šé¢ç§¯åˆ†å¸ƒ')

# 5.3 å¤§æ´²æ’æ”¾é‡æŸ±çŠ¶å›¾
ax3 = plt.subplot(3, 3, 3)
continent_stats['N2Oemission_Tg'].plot(kind='bar', ax=ax3, color='skyblue')
ax3.set_title('å„å¤§æ´²N2Oæ’æ”¾é‡')
ax3.set_ylabel('æ’æ”¾é‡ (Tg)')
ax3.tick_params(axis='x', rotation=45)

# 5.4 å¤§æ´²æ¹–æ³Šé¢ç§¯æŸ±çŠ¶å›¾
ax4 = plt.subplot(3, 3, 4)
(continent_stats['Lake_area']/1e4).plot(kind='bar', ax=ax4, color='lightgreen')
ax4.set_title('å„å¤§æ´²æ¹–æ³Šé¢ç§¯')
ax4.set_ylabel('é¢ç§¯ (ä¸‡kmÂ²)')
ax4.tick_params(axis='x', rotation=45)

# 5.5 å¤§æ´²æ’æ”¾å¼ºåº¦
ax5 = plt.subplot(3, 3, 5)
continent_stats['Emission_intensity'].plot(kind='bar', ax=ax5, color='orange')
ax5.set_title('å„å¤§æ´²æ’æ”¾å¼ºåº¦')
ax5.set_ylabel('æ’æ”¾å¼ºåº¦ (g/kmÂ²)')
ax5.tick_params(axis='x', rotation=45)

# 5.6 å‰15åå›½å®¶æ’æ”¾é‡
ax6 = plt.subplot(3, 3, 6)
country_stats.head(15)['N2Oemission_Tg'].plot(kind='bar', ax=ax6, color='lightcoral')
ax6.set_title('å‰15åå›½å®¶N2Oæ’æ”¾é‡')
ax6.set_ylabel('æ’æ”¾é‡ (Tg)')
ax6.tick_params(axis='x', rotation=45)

# 5.7 å‰15åå›½å®¶æ¹–æ³Šé¢ç§¯
ax7 = plt.subplot(3, 3, 7)
(country_stats_by_area.head(15)['Lake_area']/1e4).plot(kind='bar', ax=ax7, color='steelblue')
ax7.set_title('å‰15åå›½å®¶æ¹–æ³Šé¢ç§¯')
ax7.set_ylabel('é¢ç§¯ (ä¸‡kmÂ²)')
ax7.tick_params(axis='x', rotation=45)

# 5.8 æ’æ”¾é‡ vs æ¹–æ³Šé¢ç§¯æ•£ç‚¹å›¾ï¼ˆå¤§æ´²ï¼‰
ax8 = plt.subplot(3, 3, 8)
for continent in continent_stats.index:
    ax8.scatter(continent_stats.loc[continent, 'Lake_area']/1e4, 
               continent_stats.loc[continent, 'N2Oemission_Tg'],
               s=200, alpha=0.6, label=continent)
ax8.set_xlabel('æ¹–æ³Šé¢ç§¯ (ä¸‡kmÂ²)')
ax8.set_ylabel('N2Oæ’æ”¾é‡ (Tg)')
ax8.set_title('å„å¤§æ´²ï¼šæ’æ”¾é‡ vs æ¹–æ³Šé¢ç§¯')
ax8.legend(fontsize=8)
ax8.grid(True, alpha=0.3)

# 5.9 æ’æ”¾é‡å’Œé¢ç§¯å æ¯”å¯¹æ¯”ï¼ˆå¤§æ´²ï¼‰
ax9 = plt.subplot(3, 3, 9)
x = np.arange(len(continent_stats))
width = 0.35
ax9.bar(x - width/2, continent_stats['Emission_percentage'], width, label='æ’æ”¾é‡å æ¯”', color='skyblue')
ax9.bar(x + width/2, continent_stats['Area_percentage'], width, label='é¢ç§¯å æ¯”', color='lightgreen')
ax9.set_xlabel('å¤§æ´²')
ax9.set_ylabel('å æ¯” (%)')
ax9.set_title('å„å¤§æ´²æ’æ”¾é‡å æ¯” vs é¢ç§¯å æ¯”')
ax9.set_xticks(x)
ax9.set_xticklabels(continent_stats.index, rotation=45)
ax9.legend()
ax9.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('N2O_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# 6. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
print("=== ä¿å­˜ç»“æœ ===")

# ä¿å­˜å›½å®¶ç»Ÿè®¡
country_results = country_stats.copy()
country_results['Country'] = country_results.index
country_continent_map = df.groupby('Country')['Continent'].first()
country_results['Continent'] = country_results['Country'].map(country_continent_map)
country_results = country_results[['Country', 'Continent', 'N2Oemission', 'N2Oemission_Tg', 
                                   'Emission_percentage', 'Lake_area_km2', 'Lake_area_million_km2',
                                   'Area_percentage', 'Emission_intensity']]
country_results.columns = ['Country', 'Continent', 'N2O_Emission_g', 'N2O_Emission_Tg', 
                          'Emission_Percentage', 'Lake_Area_km2', 'Lake_Area_million_km2',
                          'Area_Percentage', 'Emission_Intensity_g_per_km2']

# ä¿å­˜å¤§æ´²ç»Ÿè®¡
continent_results = continent_stats.copy()
continent_results['Continent'] = continent_results.index
continent_results = continent_results[['Continent', 'N2Oemission', 'N2Oemission_Tg', 
                                      'Emission_percentage', 'Lake_area', 'Lake_area_million_km2',
                                      'Area_percentage', 'Emission_intensity', 'Country_count']]
continent_results.columns = ['Continent', 'N2O_Emission_g', 'N2O_Emission_Tg', 
                            'Emission_Percentage', 'Lake_Area_km2', 'Lake_Area_million_km2',
                            'Area_Percentage', 'Emission_Intensity_g_per_km2', 'Country_Count']

# ä¿å­˜åˆ°CSVæ–‡ä»¶
country_results.to_csv('country_N2O_emissions_with_area.csv', index=False, encoding='utf-8-sig')
continent_results.to_csv('continent_N2O_emissions_with_area.csv', index=False, encoding='utf-8-sig')

print("ç»“æœå·²ä¿å­˜åˆ°:")
print("- country_N2O_emissions_with_area.csv (å›½å®¶æ’æ”¾å’Œé¢ç§¯ç»Ÿè®¡)")
print("- continent_N2O_emissions_with_area.csv (å¤§æ´²æ’æ”¾å’Œé¢ç§¯ç»Ÿè®¡)")
print("- N2O_comprehensive_analysis.png (ç»¼åˆåˆ†æå›¾è¡¨)")

# 7. ç»¼åˆåˆ†ææ€»ç»“
print("\n" + "="*50)
print("=== ç»¼åˆåˆ†ææ€»ç»“ ===")
print(f"\nã€å…¨çƒæ€»é‡ã€‘")
print(f"  å…¨çƒæ¹–æ³ŠN2Oæ€»æ’æ”¾é‡: {total_global_emissions:.4f} Tg/å¹´")
print(f"  å…¨çƒæ¹–æ³Šæ€»é¢ç§¯: {total_global_lake_area:.4f} ç™¾ä¸‡ kmÂ² ({total_global_lake_area*100:.2f} ä¸‡ kmÂ²)")
print(f"  å…¨çƒå¹³å‡æ’æ”¾å¼ºåº¦: {total_global_emissions*1e9/total_global_lake_area/1e6:.2f} g/kmÂ²")

print(f"\nã€å¤§æ´²åˆ†æã€‘")
top_emission_continent = continent_stats.index[0]
top_area_continent = continent_stats.sort_values('Lake_area', ascending=False).index[0]
print(f"  æ’æ”¾é‡æœ€é«˜: {top_emission_continent} ({continent_stats.loc[top_emission_continent, 'N2Oemission_Tg']:.4f} Tg, "
      f"{continent_stats.loc[top_emission_continent, 'Emission_percentage']:.1f}%)")
print(f"  é¢ç§¯æœ€å¤§: {top_area_continent} ({continent_stats.loc[top_area_continent, 'Lake_area']/1e4:.2f} ä¸‡kmÂ², "
      f"{continent_stats.loc[top_area_continent, 'Area_percentage']:.1f}%)")
print(f"  æ’æ”¾å¼ºåº¦æœ€é«˜: {continent_stats['Emission_intensity'].idxmax()} "
      f"({continent_stats['Emission_intensity'].max():.2f} g/kmÂ²)")

print(f"\nã€å›½å®¶åˆ†æã€‘")
top_emission_country = country_stats.index[0]
top_area_country = country_stats_by_area.index[0]
print(f"  æ’æ”¾é‡æœ€é«˜: {top_emission_country} ({country_stats.loc[top_emission_country, 'N2Oemission_Tg']:.4f} Tg, "
      f"{country_stats.loc[top_emission_country, 'Emission_percentage']:.1f}%)")
print(f"  é¢ç§¯æœ€å¤§: {top_area_country} ({country_stats_by_area.loc[top_area_country, 'Lake_area_km2']/1e4:.2f} ä¸‡kmÂ², "
      f"{country_stats_by_area.loc[top_area_country, 'Area_percentage']:.1f}%)")
print(f"  æ’æ”¾å¼ºåº¦æœ€é«˜: {country_stats['Emission_intensity'].idxmax()} "
      f"({country_stats['Emission_intensity'].max():.2f} g/kmÂ²)")

print(f"\nã€é›†ä¸­åº¦åˆ†æã€‘")
top3_continents_emission_pct = continent_stats.head(3)['Emission_percentage'].sum()
top3_continents_area_pct = continent_stats.head(3)['Area_percentage'].sum()
top10_countries_emission_pct = country_stats.head(10)['Emission_percentage'].sum()
top10_countries_area_pct = country_stats.head(10)['Area_percentage'].sum()

print(f"  å‰3åå¤§æ´²:")
print(f"    æ’æ”¾é‡å æ¯”: {top3_continents_emission_pct:.1f}%")
print(f"    é¢ç§¯å æ¯”: {top3_continents_area_pct:.1f}%")
print(f"  å‰10åå›½å®¶:")
print(f"    æ’æ”¾é‡å æ¯”: {top10_countries_emission_pct:.1f}%")
print(f"    é¢ç§¯å æ¯”: {top10_countries_area_pct:.1f}%")

print(f"\nåˆ†æå®Œæˆ! å…±æ¶‰åŠ {df['Country'].nunique()} ä¸ªå›½å®¶ï¼Œ{df['Continent'].nunique()} ä¸ªå¤§æ´²")



#%% æ•°æ®åˆ†ä½æ•°ï¼Œåˆ›å»ºç¦»æ•£çš„é¢œè‰²åŒºé—´-ç»˜åˆ¶çš„æ˜¯N2Oé€šé‡

import pandas as pd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import numpy as np
from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm

# è¯»å–æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# åˆ›å»ºé¢œè‰²æ˜ å°„
colors = ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', 
          '#FF9800', '#FB8C00', '#F57C00', '#EF6C00', '#E65100',
          '#C2185B', '#7B1FA2', '#4A148C']

emission_cmap = LinearSegmentedColormap.from_list('emission_colors', colors, N=256)

# åˆ›å»ºå›¾å½¢
fig = plt.figure(figsize=(20, 12))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=0))

# è®¾ç½®åœ°å›¾èŒƒå›´å’Œç‰¹å¾
ax.set_global()
ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
ax.add_feature(cfeature.OCEAN, facecolor='lightcyan')
ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='darkgray')
ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.LAKES, alpha=0.5)
ax.add_feature(cfeature.RIVERS, linewidth=0.5, edgecolor='lightblue')

# è®¡ç®—æ•°æ®åˆ†ä½æ•°ï¼Œåˆ›å»ºç¦»æ•£çš„é¢œè‰²åŒºé—´
quantiles = np.linspace(0, 100, 15)  # åˆ›å»º15ä¸ªåŒºé—´
bounds = np.percentile(df['N2O'], quantiles)
norm = BoundaryNorm(bounds, emission_cmap.N)

# ç»˜åˆ¶æ•°æ®ç‚¹
sc = ax.scatter(
    df['Centr_lon'], 
    df['Centr_lat'], 
    s=0.01,  # å°ç‚¹çš„å¤§å°
    c=df['N2O'], 
    cmap=emission_cmap,
    norm=norm,
    alpha=0.6,  # é€‚å½“çš„é€æ˜åº¦
    transform=ccrs.PlateCarree()
)

# æ·»åŠ æ ‡é¢˜
plt.title('Global Lake Nâ‚‚O Emissions (mg N mâ»Â² dâ»Â¹)', 
         fontsize=16, pad=20)

# æ·»åŠ é¢œè‰²æ¡ï¼Œä½¿ç”¨ç¦»æ•£çš„åˆ»åº¦
cbar = plt.colorbar(sc, ax=ax, orientation='horizontal', pad=0.05, shrink=0.6,
                   extend='max', boundaries=bounds, ticks=bounds[::2])
cbar.set_label('Nâ‚‚O flux (mg N mâ»Â² dâ»Â¹)', fontsize=14)
cbar.ax.tick_params(labelsize=12)

# è®¾ç½®é¢œè‰²æ¡åˆ»åº¦æ ¼å¼
cbar.ax.set_xticklabels([f'{x:.3f}' for x in bounds[::2]])

# æ·»åŠ ç½‘æ ¼çº¿
gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,
                 linewidth=0.5, color='gray', alpha=0.3, linestyle='--')

# è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# ä¿å­˜å›¾ç‰‡
plt.savefig('global_n2o_flux_map0814.png', dpi=600, bbox_inches='tight')
plt.close()


#%% ç»˜åˆ¶çš„æ˜¯N2Oé€šé‡ è‡ªå®šä¹‰åŒºé—´ 0815

import pandas as pd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import numpy as np
from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm

# è¯»å–æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# åˆ›å»ºé¢œè‰²æ˜ å°„
colors = ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', 
          '#FF9800', '#FB8C00', '#F57C00', '#EF6C00', '#E65100',
          '#C2185B', '#7B1FA2', '#4A148C']

# æ–°é¢œè‰²ï¼ˆé»„è‰²åˆ°ç´«è‰²æ¸å˜ï¼‰
# colors = ['#fbe1a1', '#fea974', '#f6735d', '#d94669', '#a9327d', '#4a107a', '#1a1041'] 

emission_cmap = LinearSegmentedColormap.from_list('emission_colors', colors, N=256)

# åˆ›å»ºå›¾å½¢
fig = plt.figure(figsize=(20, 12))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=0))

# è®¾ç½®åœ°å›¾èŒƒå›´å’Œç‰¹å¾
ax.set_global()
ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
ax.add_feature(cfeature.OCEAN, facecolor='lightcyan')
ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='darkgray')
ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.LAKES, alpha=0.5)
ax.add_feature(cfeature.RIVERS, linewidth=0.5, edgecolor='lightblue')

# ä½¿ç”¨è‡ªå®šä¹‰çš„åŒºé—´è¾¹ç•Œ
bounds = np.array([0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 3])
norm = BoundaryNorm(bounds, emission_cmap.N)

# ç»˜åˆ¶æ•°æ®ç‚¹
sc = ax.scatter(
    df['Centr_lon'], 
    df['Centr_lat'], 
    s=0.01,  # å°ç‚¹çš„å¤§å°
    c=df['N2O'], 
    cmap=emission_cmap,
    norm=norm,
    alpha=0.6,  # é€‚å½“çš„é€æ˜åº¦
    transform=ccrs.PlateCarree()
)

# æ·»åŠ æ ‡é¢˜
plt.title('Global Lake Nâ‚‚O Emissions (mg N mâ»Â² dâ»Â¹)', 
         fontsize=16, pad=20)

# æ·»åŠ é¢œè‰²æ¡ï¼Œä½¿ç”¨ç¦»æ•£çš„åˆ»åº¦
cbar = plt.colorbar(sc, ax=ax, orientation='horizontal', pad=0.05, shrink=0.6,
                   extend='max', boundaries=bounds, ticks=bounds)
cbar.set_label('Nâ‚‚O flux (mg N mâ»Â² dâ»Â¹)', fontsize=14)
cbar.ax.tick_params(labelsize=12)

# è®¾ç½®é¢œè‰²æ¡åˆ»åº¦æ ¼å¼
cbar.ax.set_xticklabels([f'{x:.2f}' for x in bounds])

# æ·»åŠ ç½‘æ ¼çº¿
gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,
                 linewidth=0.5, color='gray', alpha=0.3, linestyle='--')

# è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# ä¿å­˜å›¾ç‰‡
plt.savefig('global_n2o_flux_map0815.png', dpi=600, bbox_inches='tight')
plt.close()


#%% æ•°æ®åˆ†ä½æ•°ï¼Œåˆ›å»ºç¦»æ•£çš„é¢œè‰²åŒºé—´â€”â€”ç»˜åˆ¶çš„æ˜¯N2Oå¹´å‡æ’æ”¾é‡

import pandas as pd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import numpy as np
from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm

# è¯»å–æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# æ¸…ç†æ•°æ® - ç§»é™¤æ‰€æœ‰åŒ…å« NaN çš„è¡Œ
df_clean = df.dropna(subset=['Centr_lon', 'Centr_lat', 'N2Oemission'])

# åˆ›å»ºé¢œè‰²æ˜ å°„
colors = ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', 
          '#FF9800', '#FB8C00', '#F57C00', '#EF6C00', '#E65100',
          '#C2185B', '#7B1FA2', '#4A148C']

emission_cmap = LinearSegmentedColormap.from_list('emission_colors', colors, N=256)

# åˆ›å»ºå›¾å½¢
fig = plt.figure(figsize=(20, 12))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=0))

# è®¾ç½®åœ°å›¾èŒƒå›´å’Œç‰¹å¾
ax.set_global()
ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
ax.add_feature(cfeature.OCEAN, facecolor='lightcyan')
ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='darkgray')
ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.LAKES, alpha=0.5)
ax.add_feature(cfeature.RIVERS, linewidth=0.5, edgecolor='lightblue')


# ä½¿ç”¨æ¸…ç†åçš„æ•°æ®è®¡ç®—åˆ†ä½æ•°
quantiles = np.linspace(0, 100, 15)
bounds = np.percentile(df_clean['N2Oemission'], quantiles)
norm = BoundaryNorm(bounds, emission_cmap.N)

# ä½¿ç”¨æ¸…ç†åçš„æ•°æ®ç»˜åˆ¶æ•£ç‚¹
sc = ax.scatter(
    df_clean['Centr_lon'], 
    df_clean['Centr_lat'], 
    s=0.01,
    c=df_clean['N2Oemission'], 
    cmap=emission_cmap,
    norm=norm,
    alpha=0.6,
    transform=ccrs.PlateCarree()
)

# æ·»åŠ æ ‡é¢˜
plt.title('Global Lake Nâ‚‚O Emissions (kg N yâ»Â¹)', 
         fontsize=16, pad=20)

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(sc, ax=ax, orientation='horizontal', pad=0.05, shrink=0.6,
                   extend='max', boundaries=bounds, ticks=bounds[::2])
cbar.set_label('Nâ‚‚O emissions (kg N yâ»Â¹)', fontsize=14)
cbar.ax.tick_params(labelsize=12)
cbar.ax.set_xticklabels([f'{x:.3f}' for x in bounds[::2]])

# æ·»åŠ ç½‘æ ¼çº¿
gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,
                 linewidth=0.5, color='gray', alpha=0.3, linestyle='--')

# è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# ä¿å­˜å›¾ç‰‡
plt.savefig('global_n2o_emissions_map0814.png', dpi=600, bbox_inches='tight')
plt.close()

#%% ç»˜åˆ¶çš„æ˜¯N2Oæ’æ”¾é‡ è‡ªå®šä¹‰åŒºé—´ 0815

import pandas as pd
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import numpy as np
from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm

# è¯»å–æ•°æ®
df = pd.read_csv("global_N2O_predictions0728.csv")

# æ¸…ç†æ•°æ® - ç§»é™¤æ‰€æœ‰åŒ…å« NaN çš„è¡Œ
df_clean = df.dropna(subset=['Centr_lon', 'Centr_lat', 'N2Oemission'])

# åˆ›å»ºé¢œè‰²æ˜ å°„
colors = ['#FFF3E0', '#FFE0B2', '#FFCC80', '#FFB74D', '#FFA726', 
          '#FF9800', '#FB8C00', '#F57C00', '#EF6C00', '#E65100',
          '#C2185B', '#7B1FA2', '#4A148C']
emission_cmap = LinearSegmentedColormap.from_list('emission_colors', colors, N=256)

# åˆ›å»ºå›¾å½¢
fig = plt.figure(figsize=(20, 12))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=0))

# è®¾ç½®åœ°å›¾èŒƒå›´å’Œç‰¹å¾
ax.set_global()
ax.add_feature(cfeature.LAND, facecolor='whitesmoke')
ax.add_feature(cfeature.OCEAN, facecolor='lightcyan')
ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='darkgray')
ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='gray')
ax.add_feature(cfeature.LAKES, alpha=0.5)
ax.add_feature(cfeature.RIVERS, linewidth=0.5, edgecolor='lightblue')

# ä½¿ç”¨è‡ªå®šä¹‰çš„åŒºé—´è¾¹ç•Œ
bounds = np.array([0, 1, 2, 3, 4, 8, 10, 20, 100, 25000000])
norm = BoundaryNorm(bounds, emission_cmap.N)

# ä½¿ç”¨æ¸…ç†åçš„æ•°æ®ç»˜åˆ¶æ•£ç‚¹
sc = ax.scatter(
    df_clean['Centr_lon'], 
    df_clean['Centr_lat'], 
    s=0.01,
    c=df_clean['N2Oemission'], 
    cmap=emission_cmap,
    norm=norm,
    alpha=0.6,
    transform=ccrs.PlateCarree()
)

# æ·»åŠ æ ‡é¢˜
plt.title('Global Lake Nâ‚‚O Emissions (kg N yâ»Â¹)', 
         fontsize=16, pad=20)

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(sc, ax=ax, orientation='horizontal', pad=0.05, shrink=0.6,
                   extend='max', boundaries=bounds, ticks=bounds)
cbar.set_label('Nâ‚‚O emissions (kg N yâ»Â¹)', fontsize=14)
cbar.ax.tick_params(labelsize=12)

# è®¾ç½®é¢œè‰²æ¡åˆ»åº¦æ ¼å¼ï¼Œå¯¹äºå¤§æ•°å€¼ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•
tick_labels = []
for x in bounds:
    if x >= 1000000:
        tick_labels.append(f'{x:.1e}')
    elif x >= 1000:
        tick_labels.append(f'{x:.0f}')
    else:
        tick_labels.append(f'{x:.0f}')

cbar.ax.set_xticklabels(tick_labels)

# æ·»åŠ ç½‘æ ¼çº¿
gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False,
                 linewidth=0.5, color='gray', alpha=0.3, linestyle='--')

# è°ƒæ•´å¸ƒå±€
plt.tight_layout()

# ä¿å­˜å›¾ç‰‡
plt.savefig('global_n2o_emissions_map0815.png', dpi=600, bbox_inches='tight')
plt.close()


#%% ç»˜åˆ¶ä¸åŒçº¬åº¦å¸¦æ¹–æ³ŠN2Oæ’æ”¾é‡ é¢ç§¯åˆ†å¸ƒ æ’æ”¾å¼ºåº¦ æ¹–æ³Šæ•°é‡ 0815


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Read the data
df = pd.read_csv("global_N2O_predictions0728.csv")

# Define latitude bands
bands = [
    (70, 90, '>70Â° N'),
    (60, 70, '60-70Â° N'),
    (50, 60, '50-60Â° N'),
    (40, 50, '40-50Â° N'),
    (30, 40, '30-40Â° N'),
    (20, 30, '20-30Â° N'),
    (10, 20, '10-20Â° N'),
    (0, 10, '0-10Â° N'),
    (-10, 0, '0-10Â° S'),
    (-20, -10, '10-20Â° S'),
    (-30, -20, '20-30Â° S'),
    (-40, -30, '30-40Â° S'),
    (-50, -40, '40-50Â° S'),
    (-90, -50, '>50Â° S')
][::-1]  # Reverse the bands to go from North to South

# Calculate emissions, lake area, emission intensity, and lake count for each latitude band
emissions_by_band = []
area_by_band = []
intensity_by_band = []
lake_count_by_band = []  # æ–°å¢ï¼šæ¹–æ³Šæ•°é‡ç»Ÿè®¡
labels = []

for min_lat, max_lat, label in bands:
    mask = (df['Centr_lat'] >= min_lat) & (df['Centr_lat'] < max_lat)
    total_emissions = df.loc[mask, 'N2Oemission'].sum() / 1e9  # Convert to Tg
    total_area = df.loc[mask, 'Lake_area'].sum() / 1e6  # Convert to million kmÂ²
    lake_count = mask.sum()  # è®¡ç®—è¯¥çº¬åº¦å¸¦å†…çš„æ¹–æ³Šæ•°é‡
    
    # Calculate emission intensity (avoid division by zero)
    if total_area > 0:
        intensity = total_emissions / total_area  # Tg N2O yâ»Â¹ per million kmÂ²
    else:
        intensity = 0
    
    emissions_by_band.append(total_emissions)
    area_by_band.append(total_area)
    intensity_by_band.append(intensity)
    lake_count_by_band.append(lake_count)
    labels.append(label)

# Create the figure with four subplots side by side
fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 8))

# Plot emissions (first subplot)
y_pos = np.arange(len(labels))
ax1.barh(y_pos, emissions_by_band, color='gray', alpha=0.7)
ax1.set_yticks(y_pos)
ax1.set_yticklabels(labels)
ax1.set_xlabel('Nâ‚‚O emissions (Tg Nâ‚‚O yâ»Â¹)')
ax1.set_title('Emissions Distribution')
ax1.grid(axis='x', linestyle='--', alpha=0.3)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# Plot lake area (second subplot)
ax2.barh(y_pos, area_by_band, color='steelblue', alpha=0.7)
ax2.set_yticks(y_pos)
ax2.set_yticklabels(labels)
ax2.set_xlabel('Lake Area (million kmÂ²)')
ax2.set_title('Lake Area Distribution')
ax2.grid(axis='x', linestyle='--', alpha=0.3)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# Plot emission intensity (third subplot)
ax3.barh(y_pos, intensity_by_band, color='orange', alpha=0.7)
ax3.set_yticks(y_pos)
ax3.set_yticklabels(labels)
ax3.set_xlabel('Nâ‚‚O Emission Intensity (Tg Nâ‚‚O yâ»Â¹ per million kmÂ²)')
ax3.set_title('Emission Intensity Distribution')
ax3.grid(axis='x', linestyle='--', alpha=0.3)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# Plot lake count (fourth subplot - æ–°å¢)
ax4.barh(y_pos, lake_count_by_band, color='green', alpha=0.7)
ax4.set_yticks(y_pos)
ax4.set_yticklabels(labels)
ax4.set_xlabel('Number of Lakes')
ax4.set_title('Lake Count Distribution')
ax4.grid(axis='x', linestyle='--', alpha=0.3)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# Adjust layout
plt.tight_layout()

# Save the plot
plt.savefig('latitude_distribution_with_lake_count0815.png', dpi=300, bbox_inches='tight')
plt.close()

# Also create a standalone lake count plot
fig, ax = plt.subplots(1, 1, figsize=(10, 8))

ax.barh(y_pos, lake_count_by_band, color='green', alpha=0.7)
ax.set_yticks(y_pos)
ax.set_yticklabels(labels)
ax.set_xlabel('Number of Lakes')
ax.set_title('Lake Count by Latitude Band')
ax.grid(axis='x', linestyle='--', alpha=0.3)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# Add value labels on bars for better readability
for i, v in enumerate(lake_count_by_band):
    if v > 0:  # Only show labels for non-zero values
        ax.text(v + max(lake_count_by_band)*0.01, i, f'{v}', 
                va='center', ha='left', fontsize=9)

plt.tight_layout()
plt.savefig('lake_count_by_latitude0815.png', dpi=300, bbox_inches='tight')
plt.close()

# Print summary statistics
print("Lake Statistics by Latitude Band:")
print("=" * 80)
print(f"{'Latitude Band':>12} {'Lake Count':>12} {'Emissions (Tg)':>15} {'Area (MkmÂ²)':>12} {'Intensity':>10}")
print("-" * 80)

for i, label in enumerate(labels):
    print(f"{label:>12} {lake_count_by_band[i]:>12} {emissions_by_band[i]:>15.3f} {area_by_band[i]:>12.3f} {intensity_by_band[i]:>10.4f}")

print("-" * 80)
print(f"{'Total':>12} {sum(lake_count_by_band):>12} {sum(emissions_by_band):>15.3f} {sum(area_by_band):>12.3f}")
print(f"\nGlobal average intensity: {sum(emissions_by_band)/sum(area_by_band):.4f} Tg Nâ‚‚O yâ»Â¹ per million kmÂ²")
print(f"Average lakes per latitude band: {sum(lake_count_by_band)/len(lake_count_by_band):.1f}")

# Additional lake count statistics
print(f"\nLake Count Statistics:")
print(f"Total number of lakes: {sum(lake_count_by_band)}")
print(f"Latitude band with most lakes: {labels[np.argmax(lake_count_by_band)]} ({max(lake_count_by_band)} lakes)")
print(f"Latitude band with fewest lakes: {labels[np.argmin(lake_count_by_band)]} ({min(lake_count_by_band)} lakes)")

#%% è®ºæ–‡åˆ†æ-ç»˜åˆ¶ä¸åŒçº¬åº¦å¸¦æ¹–æ³ŠN2Oæ’æ”¾é‡ é¢ç§¯åˆ†å¸ƒ æ’æ”¾å¼ºåº¦ æ¹–æ³Šæ•°é‡ 0902

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Read the data
df = pd.read_csv("global_N2O_predictions0728.csv")

# Define latitude bands
bands = [
    (70, 90, '>70Â° N'),
    (60, 70, '60-70Â° N'),
    (50, 60, '50-60Â° N'),
    (40, 50, '40-50Â° N'),
    (30, 40, '30-40Â° N'),
    (20, 30, '20-30Â° N'),
    (10, 20, '10-20Â° N'),
    (0, 10, '0-10Â° N'),
    (-10, 0, '0-10Â° S'),
    (-20, -10, '10-20Â° S'),
    (-30, -20, '20-30Â° S'),
    (-40, -30, '30-40Â° S'),
    (-50, -40, '40-50Â° S'),
    (-90, -50, '>50Â° S')
][::-1]  # Reverse the bands to go from North to South

# Calculate emissions, lake area, emission intensity, and lake count for each latitude band
emissions_by_band = []
area_by_band = []
intensity_by_band = []
lake_count_by_band = []
labels = []

for min_lat, max_lat, label in bands:
    mask = (df['Centr_lat'] >= min_lat) & (df['Centr_lat'] < max_lat)
    total_emissions = df.loc[mask, 'N2Oemission'].sum() / 1e9  # Convert to Tg
    total_area = df.loc[mask, 'Lake_area'].sum() / 1e6  # Convert to million kmÂ²
    lake_count = mask.sum()  # è®¡ç®—è¯¥çº¬åº¦å¸¦å†…çš„æ¹–æ³Šæ•°é‡
    
    # Calculate emission intensity (avoid division by zero)
    if total_area > 0:
        intensity = total_emissions / total_area  # Tg N2O yâ»Â¹ per million kmÂ²
    else:
        intensity = 0
    
    emissions_by_band.append(total_emissions)
    area_by_band.append(total_area)
    intensity_by_band.append(intensity)
    lake_count_by_band.append(lake_count)
    labels.append(label)

# Calculate total emissions for percentage calculation
total_global_emissions = sum(emissions_by_band)

# Calculate emission percentages
emission_percentages = [(emissions / total_global_emissions * 100) for emissions in emissions_by_band]

# Create the figure with five subplots
fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(30, 8))

# Plot emissions (first subplot)
y_pos = np.arange(len(labels))
ax1.barh(y_pos, emissions_by_band, color='gray', alpha=0.7)
ax1.set_yticks(y_pos)
ax1.set_yticklabels(labels)
ax1.set_xlabel('Nâ‚‚O emissions (Tg Nâ‚‚O yâ»Â¹)')
ax1.set_title('Emissions Distribution')
ax1.grid(axis='x', linestyle='--', alpha=0.3)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# Plot lake area (second subplot)
ax2.barh(y_pos, area_by_band, color='steelblue', alpha=0.7)
ax2.set_yticks(y_pos)
ax2.set_yticklabels(labels)
ax2.set_xlabel('Lake Area (million kmÂ²)')
ax2.set_title('Lake Area Distribution')
ax2.grid(axis='x', linestyle='--', alpha=0.3)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

# Plot emission intensity (third subplot)
ax3.barh(y_pos, intensity_by_band, color='orange', alpha=0.7)
ax3.set_yticks(y_pos)
ax3.set_yticklabels(labels)
ax3.set_xlabel('Nâ‚‚O Emission Intensity (Tg Nâ‚‚O yâ»Â¹ per million kmÂ²)')
ax3.set_title('Emission Intensity Distribution')
ax3.grid(axis='x', linestyle='--', alpha=0.3)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# Plot lake count (fourth subplot)
ax4.barh(y_pos, lake_count_by_band, color='green', alpha=0.7)
ax4.set_yticks(y_pos)
ax4.set_yticklabels(labels)
ax4.set_xlabel('Number of Lakes')
ax4.set_title('Lake Count Distribution')
ax4.grid(axis='x', linestyle='--', alpha=0.3)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

# Plot emission percentages (fifth subplot - æ–°å¢)
ax5.barh(y_pos, emission_percentages, color='purple', alpha=0.7)
ax5.set_yticks(y_pos)
ax5.set_yticklabels(labels)
ax5.set_xlabel('Emission Percentage (%)')
ax5.set_title('Emission Percentage Distribution')
ax5.grid(axis='x', linestyle='--', alpha=0.3)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

# Add percentage labels on bars
for i, v in enumerate(emission_percentages):
    if v > 1:  # Only show labels for values > 1%
        ax5.text(v + max(emission_percentages)*0.01, i, f'{v:.1f}%', 
                va='center', ha='left', fontsize=9)

# Adjust layout
plt.tight_layout()

# Save the plot
plt.savefig('latitude_distribution_with_percentages.png', dpi=300, bbox_inches='tight')
plt.close()

# Print summary statistics with percentages
print("Lake Statistics by Latitude Band:")
print("=" * 100)
print(f"{'Latitude Band':>12} {'Lake Count':>12} {'Emissions (Tg)':>15} {'Percentage (%)':>14} {'Area (MkmÂ²)':>12} {'Intensity':>10}")
print("-" * 100)

for i, label in enumerate(labels):
    print(f"{label:>12} {lake_count_by_band[i]:>12} {emissions_by_band[i]:>15.4f} {emission_percentages[i]:>13.2f} {area_by_band[i]:>12.3f} {intensity_by_band[i]:>10.4f}")

print("-" * 100)
print(f"{'Total':>12} {sum(lake_count_by_band):>12} {sum(emissions_by_band):>15.4f} {sum(emission_percentages):>13.1f} {sum(area_by_band):>12.3f}")
print(f"\nGlobal average intensity: {sum(emissions_by_band)/sum(area_by_band):.4f} Tg Nâ‚‚O yâ»Â¹ per million kmÂ²")
print(f"Average lakes per latitude band: {sum(lake_count_by_band)/len(lake_count_by_band):.1f}")

# Additional statistics for common latitude groupings
print(f"\n" + "="*60)
print("SUMMARY BY MAJOR LATITUDE ZONES:")
print("="*60)

# Calculate major zone statistics
def calculate_zone_stats(zone_name, lat_ranges):
    """Calculate statistics for a major latitude zone"""
    zone_emissions = 0
    zone_area = 0
    zone_lakes = 0
    
    for i, label in enumerate(labels):
        if any(label in lat_range for lat_range in lat_ranges):
            zone_emissions += emissions_by_band[i]
            zone_area += area_by_band[i]
            zone_lakes += lake_count_by_band[i]
    
    zone_percentage = (zone_emissions / total_global_emissions * 100)
    zone_intensity = zone_emissions / zone_area if zone_area > 0 else 0
    
    print(f"{zone_name:>20}: {zone_emissions:>8.4f} Tg ({zone_percentage:>5.1f}%), {zone_lakes:>6} lakes, Intensity: {zone_intensity:>6.4f}")
    return zone_emissions, zone_percentage

# Define major zones
arctic_boreal = calculate_zone_stats("Arctic/Boreal (>60Â°N)", [">70Â° N", "60-70Â° N"])
temperate_north = calculate_zone_stats("Temperate North (30-60Â°N)", ["50-60Â° N", "40-50Â° N", "30-40Â° N"])
tropical = calculate_zone_stats("Tropical (30Â°S-30Â°N)", ["20-30Â° N", "10-20Â° N", "0-10Â° N", "0-10Â° S", "10-20Â° S", "20-30Â° S"])
temperate_south = calculate_zone_stats("Temperate South (30-50Â°S)", ["30-40Â° S", "40-50Â° S"])
polar_south = calculate_zone_stats("Polar South (>50Â°S)", [">50Â° S"])

print("-" * 60)

# Key findings for paper
print(f"\n" + "="*60)
print("KEY FINDINGS FOR PAPER:")
print("="*60)

# Find latitude bands contributing most emissions
top_emissions_idx = np.argsort(emissions_by_band)[::-1][:3]
print(f"Top 3 emission sources:")
for i, idx in enumerate(top_emissions_idx, 1):
    print(f"  {i}. {labels[idx]}: {emissions_by_band[idx]:.4f} Tg ({emission_percentages[idx]:.1f}%)")

print(f"\nNorthern hemisphere (â‰¥0Â°N) contributes: {sum(emissions_by_band[i] for i, label in enumerate(labels) if not 'S' in label):.4f} Tg ({sum(emission_percentages[i] for i, label in enumerate(labels) if not 'S' in label):.1f}%)")
print(f"Southern hemisphere (<0Â°N) contributes: {sum(emissions_by_band[i] for i, label in enumerate(labels) if 'S' in label):.4f} Tg ({sum(emission_percentages[i] for i, label in enumerate(labels) if 'S' in label):.1f}%)")

# Calculate emissions above/below certain latitudes
# Calculate emissions above/below certain latitudes
above_40n = sum(emissions_by_band[i] for i, label in enumerate(labels) if any(x in label for x in ['>70Â°', '60-70Â°', '50-60Â°', '40-50Â°']))
above_30n = sum(emissions_by_band[i] for i, label in enumerate(labels) if not 'S' in label and label not in ['20-30Â° N', '10-20Â° N', '0-10Â° N'])
print(f"Emissions from latitudes >40Â°N: {above_40n:.4f} Tg ({above_40n/total_global_emissions*100:.1f}%)")
print(f"Emissions from latitudes >30Â°N: {above_30n:.4f} Tg ({above_30n/total_global_emissions*100:.1f}%)")



#%% ç»˜åˆ¶çº¬åº¦å¸¦ åŒXè½´ç»Ÿè®¡å›¾  0815

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Read the data
df = pd.read_csv("global_N2O_predictions0728.csv")

# Define 1-degree latitude bands from -60 to 90
lat_min = -60
lat_max = 90
lat_step = 1

# Create latitude band centers and ranges
lat_centers = np.arange(lat_min + lat_step/2, lat_max, lat_step)
lat_bands = [(lat - lat_step/2, lat + lat_step/2) for lat in lat_centers]

# Calculate emissions, lake area, emission intensity, and lake count for each 1-degree latitude band
emissions_by_band = []
area_by_band = []
intensity_by_band = []
lake_count_by_band = []

for min_lat, max_lat in lat_bands:
    mask = (df['Centr_lat'] >= min_lat) & (df['Centr_lat'] < max_lat)
    total_emissions = df.loc[mask, 'N2Oemission'].sum() / 1e9  # Convert to Tg
    total_area = df.loc[mask, 'Lake_area'].sum() / 1e4  # Convert to 10^4 kmÂ²
    lake_count = mask.sum() / 1e4  # Convert to 10^4 units
    
    # Calculate emission intensity (avoid division by zero)
    if total_area > 0:
        # intensity in Tg yâ»Â¹ per 10^6 kmÂ² = (Tg yâ»Â¹) / (10^4 kmÂ² * 100) = Tg yâ»Â¹ / (10^6 kmÂ²)
        intensity = (df.loc[mask, 'N2Oemission'].sum() / 1e9) / (df.loc[mask, 'Lake_area'].sum() / 1e6)
    else:
        intensity = 0
    
    emissions_by_band.append(total_emissions)
    area_by_band.append(total_area)
    intensity_by_band.append(intensity)
    lake_count_by_band.append(lake_count)

# Convert to numpy arrays for easier handling
emissions_by_band = np.array(emissions_by_band)
area_by_band = np.array(area_by_band)
intensity_by_band = np.array(intensity_by_band)
lake_count_by_band = np.array(lake_count_by_band)

# ====== ç¬¬ä¸€å¼ å›¾ï¼šæ¹–æ³Šé¢ç§¯å’Œæ¹–æ³Šæ•°é‡ ======
fig1, ax1 = plt.subplots(figsize=(4, 12))

# ç»˜åˆ¶æ¹–æ³Šé¢ç§¯æ›²çº¿
line1 = ax1.plot(area_by_band, lat_centers, '-', color='#a577ad', linewidth=2, 
                 label='Lake Area')
ax1.set_ylabel('Latitude', fontsize=12)
ax1.set_xlabel('Lake Area ($\mathbf{10^4}$ kmÂ²)', color='#a577ad', fontsize=12, fontweight='bold')
ax1.tick_params(axis='x', labelcolor='#a577ad')
ax1.set_ylim(-60, 90)
ax1.grid(True, linestyle='--', alpha=0.3)

# æ·»åŠ çº¬åº¦æ ‡ç­¾
ax1.set_yticks(np.arange(-60, 91, 15))
ax1.set_yticklabels([f'{lat}Â°' for lat in np.arange(-60, 91, 15)])

# åˆ›å»ºç¬¬äºŒä¸ªxè½´ç”¨äºæ¹–æ³Šæ•°é‡
ax2 = ax1.twiny()
line2 = ax2.plot(lake_count_by_band, lat_centers, '-', color='#73c79e', linewidth=2, 
                 label='Lake Count')
ax2.set_xlabel('Number of Lakes ($\mathbf{10^4}$)', color='#73c79e', fontsize=12, fontweight='bold')
ax2.tick_params(axis='x', labelcolor='#73c79e')

# è®¾ç½®å›¾ä¾‹
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

# è®¾ç½®æ ‡é¢˜å’Œæ ·å¼
# ax1.set_title('Lake Area and Count Distribution by Latitude', fontsize=14, pad=20)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('lake_area_count_distribution0815.png', dpi=300, bbox_inches='tight')
plt.show()

# ====== ç¬¬äºŒå¼ å›¾ï¼šæ’æ”¾å’Œæ’æ”¾å¼ºåº¦ ======
fig2, ax3 = plt.subplots(figsize=(4, 12))

# ç»˜åˆ¶æ’æ”¾é‡æ›²çº¿
line3 = ax3.plot(emissions_by_band, lat_centers, '-', color='#a577ad', linewidth=2, 
                 label='Nâ‚‚O Emissions')
ax3.set_ylabel('Latitude', fontsize=12)
ax3.set_xlabel('Nâ‚‚O emissions (Tg y$\mathbf{^{-1}}$)', color='#a577ad', fontsize=12, fontweight='bold')
ax3.tick_params(axis='x', labelcolor='#a577ad')
ax3.set_ylim(-60, 90)
ax3.grid(True, linestyle='--', alpha=0.3)

# æ·»åŠ çº¬åº¦æ ‡ç­¾
ax3.set_yticks(np.arange(-60, 91, 15))
ax3.set_yticklabels([f'{lat}Â°' for lat in np.arange(-60, 91, 15)])

# åˆ›å»ºç¬¬äºŒä¸ªxè½´ç”¨äºæ’æ”¾å¼ºåº¦
ax4 = ax3.twiny()
line4 = ax4.plot(intensity_by_band, lat_centers, '-', color='#73c79e', linewidth=2, 
                 label='Emission Intensity')
ax4.set_xlabel('Emission Intensity (Tg y$\mathbf{^{-1}}$/$\mathbf{10^6}$ kmÂ²)', color='#73c79e', fontsize=12, fontweight='bold')
ax4.tick_params(axis='x', labelcolor='#73c79e')

# è®¾ç½®å›¾ä¾‹
lines3, labels3 = ax3.get_legend_handles_labels()
lines4, labels4 = ax4.get_legend_handles_labels()
ax3.legend(lines3 + lines4, labels3 + labels4, loc='upper right')

# è®¾ç½®æ ‡é¢˜å’Œæ ·å¼
# ax3.set_title('Nâ‚‚O Emissions and Intensity Distribution by Latitude', fontsize=14, pad=20)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('emissions_intensity_distribution0815.png', dpi=300, bbox_inches='tight')
plt.show()

# Print summary statistics for major latitude bands (ä¿æŒåŸæœ‰æ ¼å¼çš„ç»Ÿè®¡)
bands_summary = [
    (70, 90, '>70Â° N'),
    (60, 70, '60-70Â° N'),
    (50, 60, '50-60Â° N'),
    (40, 50, '40-50Â° N'),
    (30, 40, '30-40Â° N'),
    (20, 30, '20-30Â° N'),
    (10, 20, '10-20Â° N'),
    (0, 10, '0-10Â° N'),
    (-10, 0, '0-10Â° S'),
    (-20, -10, '10-20Â° S'),
    (-30, -20, '20-30Â° S'),
    (-40, -30, '30-40Â° S'),
    (-50, -40, '40-50Â° S'),
    (-60, -50, '50-60Â° S')
][::-1]

emissions_summary = []
area_summary = []
intensity_summary = []
lake_count_summary = []
labels_summary = []

for min_lat, max_lat, label in bands_summary:
    mask = (df['Centr_lat'] >= min_lat) & (df['Centr_lat'] < max_lat)
    total_emissions = df.loc[mask, 'N2Oemission'].sum() / 1e9
    total_area = df.loc[mask, 'Lake_area'].sum() / 1e4  # Convert to 10^4 kmÂ²
    lake_count = mask.sum() / 1e4  # Convert to 10^4 units
    
    if df.loc[mask, 'Lake_area'].sum() > 0:
        intensity = (df.loc[mask, 'N2Oemission'].sum() / 1e9) / (df.loc[mask, 'Lake_area'].sum() / 1e6)
    else:
        intensity = 0
    
    emissions_summary.append(total_emissions)
    area_summary.append(total_area)
    intensity_summary.append(intensity)
    lake_count_summary.append(lake_count)
    labels_summary.append(label)

print("Lake Statistics by Latitude Band:")
print("=" * 85)
print(f"{'Latitude Band':>12} {'Lake Count':>12} {'Emissions (Tg)':>15} {'Area (10â´kmÂ²)':>15} {'Intensity':>12}")
print(f"{'':>12} {'(10â´)':>12} {'yâ»Â¹':>15} {'':>15} {'(Tg yâ»Â¹ per':>12}")
print(f"{'':>12} {'':>12} {'':>15} {'':>15} {'10â¶ kmÂ²)':>12}")
print("-" * 85)

for i, label in enumerate(labels_summary):
    print(f"{label:>12} {lake_count_summary[i]:>12.2f} {emissions_summary[i]:>15.3f} {area_summary[i]:>15.3f} {intensity_summary[i]:>12.4f}")

print("-" * 85)
print(f"{'Total':>12} {sum(lake_count_summary):>12.2f} {sum(emissions_summary):>15.3f} {sum(area_summary):>15.3f}")
total_area_original = sum([df.loc[(df['Centr_lat'] >= band[0]) & (df['Centr_lat'] < band[1]), 'Lake_area'].sum() for band in bands_summary])
print(f"\nGlobal average intensity: {sum(emissions_summary)/(total_area_original/1e6):.4f} Tg yâ»Â¹ per 10â¶ kmÂ²")
print(f"Average lakes per latitude band: {sum(lake_count_summary)/len(lake_count_summary):.2f} Ã— 10â´")

# Additional statistics for 1-degree resolution
print(f"\nDetailed Statistics (1Â° resolution):")
print(f"Total 1Â° latitude bands with lakes: {np.sum(lake_count_by_band > 0)}")
print(f"Maximum lakes in any 1Â° band: {np.max(lake_count_by_band):.2f} Ã— 10â´")
print(f"Latitude with maximum lake count: {lat_centers[np.argmax(lake_count_by_band)]:.1f}Â°")
print(f"Maximum emissions in any 1Â° band: {np.max(emissions_by_band):.4f} Tg yâ»Â¹")
print(f"Latitude with maximum emissions: {lat_centers[np.argmax(emissions_by_band)]:.1f}Â°")


#%% å°æ¹–æ³Šåˆ†æ 

import pandas as pd
import numpy as np

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# 2. åªä¿ç•™ N2O éç©ºä¸”é¢ç§¯ <= 0.1 km2 çš„æ¹–æ³Š
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['Areakm2'] <= 0.1) & (GHGdata['N2O'] >= 0)].copy()

# 3. å®šä¹‰åˆ†ç»„åŒºé—´å’Œæ ‡ç­¾
bins = [0, 0.0001, 0.001, 0.01, 0.1]
labels = ['<0.0001', '0.0001-0.001', '0.001-0.01', '0.01-0.1']

# pd.cut ä¼šæŠŠ (0,0.0001] æ˜ å°„åˆ°ç¬¬ä¸€ä¸ªåŒºé—´ï¼Œå¦‚æœå¸Œæœ›æŠŠ 0.0 ä¹Ÿç®—åˆ°ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥è®¾ç½® include_lowest=True
df['size_bin'] = pd.cut(df['Areakm2'],
                        bins=bins,
                        labels=labels,
                        include_lowest=False,
                        right=True)

# 4. åˆ†ç»„å¹¶è®¡ç®—ç»Ÿè®¡é‡
stats = df.groupby('size_bin')['N2O'].agg(
    mean=lambda x: x.mean(),
    std=lambda x: x.std(),   
    count='count'
).reindex(labels)  # ä¿æŒé¡ºåº

# 5. å°†ç»“æœè½¬æ¢æˆå­—å…¸ï¼Œç©ºç»„å¡« 0
lake_data = {}
for label in labels:
    if pd.isna(stats.loc[label, 'count']) or stats.loc[label, 'count'] == 0:
        lake_data[label] = {'mean': 0, 'std': 0, 'count': 0}
    else:
        lake_data[label] = {
            'mean': round(stats.loc[label, 'mean'], 2),
            'std': round(stats.loc[label, 'std'], 2),
            'count': int(stats.loc[label, 'count'])
        }

# 6. è¾“å‡ºæ£€æŸ¥
print("å„é¢ç§¯åŒºé—´çš„ N2O ç»Ÿè®¡ï¼š")
for k, v in lake_data.items():
    print(f"{k}: mean={v['mean']}, std={v['std']}, count={v['count']}")

# 7. æœ€ç»ˆçš„ lake_data
print("\nlake_data =")
print(lake_data)




å„é¢ç§¯åŒºé—´çš„ N2O ç»Ÿè®¡ï¼š
<0.0001: mean=0, std=0, count=0
0.0001-0.001: mean=1.46, std=3.02, count=37
0.001-0.01: mean=2.17, std=13.01, count=52
0.01-0.1: mean=0.3, std=1.88, count=105

lake_data =
{'<0.0001': {'mean': 0, 'std': 0, 'count': 0}, 
 '0.0001-0.001': {'mean': 1.46, 'std': 3.02, 'count': 37}, 
 '0.001-0.01': {'mean': 2.17, 'std': 13.01, 'count': 52}, 
 '0.01-0.1': {'mean': 0.3, 'std': 1.88, 'count': 105}}


lake_data = {
    '<0.0001': {'mean': 0, 'std': 0, 'count': 0},
    '0.0001-0.001': {'mean': 0.39, 'std': 0.69, 'count': 111},
    '0.001-0.01': {'mean': 0.73, 'std': 1.18, 'count': 195},
    '0.01-0.1': {'mean': 1.68, 'std': 9.43, 'count': 69}
}


#%% å°æ¹–æ³Šåˆ†æ å¯¹logN2Oåˆ†æ 0821

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 1. è¯»å–æ•°æ®
print("æ­£åœ¨è¯»å–æ•°æ®...")
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# 2. æ•°æ®ç­›é€‰å’Œé¢„å¤„ç†
print("æ­£åœ¨ç­›é€‰æ•°æ®...")
df = GHGdata[GHGdata['N2O'].notna() & (GHGdata['Areakm2'] <= 0.1) & (GHGdata['N2O'] > 0)].copy()

# åŸºç¡€è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„è¿‡æ»¤
df = df[
    (df['N2O'] > df['N2O'].quantile(0.01)) & 
    (df['N2O'] < df['N2O'].quantile(0.99))  # å»é™¤æç«¯å¼‚å¸¸å€¼
].copy()

# æ·»åŠ log(N2O)åˆ—
# ä½¿ç”¨å°çš„åç§»é‡é¿å…log(0)ï¼Œè¿™é‡Œç”¨1e-10
df['Log_N2O'] = np.log10(df['N2O'] + 1e-10)

print(f"ç­›é€‰åçš„æ•°æ®é‡: {len(df)} æ¡è®°å½•")
print(f"N2OèŒƒå›´: {df['N2O'].min():.6f} - {df['N2O'].max():.6f}")
print(f"Log(N2O)èŒƒå›´: {df['Log_N2O'].min():.6f} - {df['Log_N2O'].max():.6f}")

# 3. å®šä¹‰åˆ†ç»„åŒºé—´å’Œæ ‡ç­¾
bins = [0, 0.0001, 0.001, 0.01, 0.1]
labels = ['<0.0001', '0.0001-0.001', '0.001-0.01', '0.01-0.1']

# åˆ›å»ºé¢ç§¯åˆ†ç»„
df['size_bin'] = pd.cut(df['Areakm2'],
                        bins=bins,
                        labels=labels,
                        include_lowest=False,
                        right=True)

# 4. åˆ†ç»„ç»Ÿè®¡ - åŸå§‹N2O
print("\n=== åŸå§‹N2Oç»Ÿè®¡ ===")
stats_original = df.groupby('size_bin')['N2O'].agg(
    mean=lambda x: x.mean(),
    std=lambda x: x.std(),   
    count='count',
    min=lambda x: x.min(),
    max=lambda x: x.max(),
    median=lambda x: x.median()
).reindex(labels)

# 5. åˆ†ç»„ç»Ÿè®¡ - Log(N2O)
print("\n=== Log(N2O)ç»Ÿè®¡ ===")
stats_log = df.groupby('size_bin')['Log_N2O'].agg(
    log_mean=lambda x: x.mean(),
    log_std=lambda x: x.std(),   
    count='count',
    log_min=lambda x: x.min(),
    log_max=lambda x: x.max(),
    log_median=lambda x: x.median()
).reindex(labels)

# 6. åˆ›å»ºç»¼åˆç»Ÿè®¡è¡¨
print("\n=== å„é¢ç§¯åŒºé—´çš„è¯¦ç»†ç»Ÿè®¡ ===")
for label in labels:
    print(f"\nã€{label} kmÂ²ã€‘")
    
    if pd.isna(stats_original.loc[label, 'count']) or stats_original.loc[label, 'count'] == 0:
        print("  æ— æ•°æ®")
        continue
    
    count = int(stats_original.loc[label, 'count'])
    
    # åŸå§‹N2Oç»Ÿè®¡
    print(f"  æ ·æœ¬æ•°é‡: {count}")
    print(f"  N2O (mg N mâ»Â² dâ»Â¹):")
    print(f"    å‡å€¼: {stats_original.loc[label, 'mean']:.4f}")
    print(f"    æ ‡å‡†å·®: {stats_original.loc[label, 'std']:.4f}")
    print(f"    ä¸­ä½æ•°: {stats_original.loc[label, 'median']:.4f}")
    print(f"    èŒƒå›´: {stats_original.loc[label, 'min']:.4f} - {stats_original.loc[label, 'max']:.4f}")
    
    # Log(N2O)ç»Ÿè®¡
    print(f"  Logâ‚â‚€(N2O):")
    print(f"    å‡å€¼: {stats_log.loc[label, 'log_mean']:.4f}")
    print(f"    æ ‡å‡†å·®: {stats_log.loc[label, 'log_std']:.4f}")
    print(f"    ä¸­ä½æ•°: {stats_log.loc[label, 'log_median']:.4f}")
    print(f"    èŒƒå›´: {stats_log.loc[label, 'log_min']:.4f} - {stats_log.loc[label, 'log_max']:.4f}")

# 7. åˆ›å»ºç”¨äºè’™ç‰¹å¡æ´›çš„æ•°æ®å­—å…¸
print("\n=== ç”¨äºè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿçš„æ•°æ®å­—å…¸ ===")

# åŸå§‹N2Oæ•°æ®å­—å…¸
lake_data_original = {}
for label in labels:
    if pd.isna(stats_original.loc[label, 'count']) or stats_original.loc[label, 'count'] == 0:
        lake_data_original[label] = {'mean': 0, 'std': 0, 'count': 0}
    else:
        lake_data_original[label] = {
            'mean': round(stats_original.loc[label, 'mean'], 4),
            'std': round(stats_original.loc[label, 'std'], 4),
            'count': int(stats_original.loc[label, 'count'])
        }

# Log(N2O)æ•°æ®å­—å…¸
lake_data_log = {}
for label in labels:
    if pd.isna(stats_log.loc[label, 'count']) or stats_log.loc[label, 'count'] == 0:
        lake_data_log[label] = {'log_mean': 0, 'log_std': 0, 'count': 0}
    else:
        lake_data_log[label] = {
            'log_mean': round(stats_log.loc[label, 'log_mean'], 4),
            'log_std': round(stats_log.loc[label, 'log_std'], 4),
            'count': int(stats_log.loc[label, 'count'])
        }

print("\nåŸå§‹N2Oæ•°æ®:")
print("lake_data_original =", lake_data_original)

print("\nLog(N2O)æ•°æ®:")
print("lake_data_log =", lake_data_log)

# 8. æ•°æ®åˆ†å¸ƒå¯è§†åŒ–
def plot_distributions():
    """ç»˜åˆ¶N2Oå’ŒLog(N2O)çš„åˆ†å¸ƒå›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # è¿‡æ»¤æœ‰æ•°æ®çš„ç»„
    df_with_data = df[df['size_bin'].notna()].copy()
    
    # åŸå§‹N2Oåˆ†å¸ƒ
    axes[0, 0].hist(df_with_data['N2O'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].set_xlabel('Nâ‚‚O (mg N mâ»Â² dâ»Â¹)')
    axes[0, 0].set_ylabel('é¢‘æ•°')
    axes[0, 0].set_title('åŸå§‹Nâ‚‚Oåˆ†å¸ƒ')
    axes[0, 0].set_yscale('log')
    
    # Log(N2O)åˆ†å¸ƒ
    axes[0, 1].hist(df_with_data['Log_N2O'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 1].set_xlabel('Logâ‚â‚€(Nâ‚‚O)')
    axes[0, 1].set_ylabel('é¢‘æ•°')
    axes[0, 1].set_title('Logâ‚â‚€(Nâ‚‚O)åˆ†å¸ƒ')
    
    # æŒ‰é¢ç§¯åˆ†ç»„çš„ç®±çº¿å›¾ - åŸå§‹N2O
    df_with_data.boxplot(column='N2O', by='size_bin', ax=axes[1, 0])
    axes[1, 0].set_xlabel('é¢ç§¯åŒºé—´ (kmÂ²)')
    axes[1, 0].set_ylabel('Nâ‚‚O (mg N mâ»Â² dâ»Â¹)')
    axes[1, 0].set_title('å„é¢ç§¯åŒºé—´Nâ‚‚Oåˆ†å¸ƒ')
    axes[1, 0].set_yscale('log')
    
    # æŒ‰é¢ç§¯åˆ†ç»„çš„ç®±çº¿å›¾ - Log(N2O)
    df_with_data.boxplot(column='Log_N2O', by='size_bin', ax=axes[1, 1])
    axes[1, 1].set_xlabel('é¢ç§¯åŒºé—´ (kmÂ²)')
    axes[1, 1].set_ylabel('Logâ‚â‚€(Nâ‚‚O)')
    axes[1, 1].set_title('å„é¢ç§¯åŒºé—´Logâ‚â‚€(Nâ‚‚O)åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.show()

# 9. æ­£æ€æ€§æ£€éªŒ
from scipy import stats as scipy_stats

print("\n=== æ­£æ€æ€§æ£€éªŒ (Shapiro-Wilkæ£€éªŒ) ===")
for label in labels:
    subset = df[df['size_bin'] == label]
    if len(subset) > 3:  # Shapiro-Wilkéœ€è¦è‡³å°‘3ä¸ªæ ·æœ¬
        # åŸå§‹N2O
        stat_orig, p_orig = scipy_stats.shapiro(subset['N2O'])
        # Log(N2O)
        stat_log, p_log = scipy_stats.shapiro(subset['Log_N2O'])
        
        print(f"\n{label}:")
        print(f"  åŸå§‹N2O: W={stat_orig:.4f}, p={p_orig:.6f} {'(æ­£æ€)' if p_orig > 0.05 else '(éæ­£æ€)'}")
        print(f"  Log(N2O): W={stat_log:.4f}, p={p_log:.6f} {'(æ­£æ€)' if p_log > 0.05 else '(éæ­£æ€)'}")

# 10. è¿è¡Œå¯è§†åŒ–
try:
    plot_distributions()
except Exception as e:
    print(f"\næ³¨æ„: æ— æ³•ç»˜åˆ¶å›¾è¡¨ ({e})")
    print("å¦‚éœ€æŸ¥çœ‹åˆ†å¸ƒå›¾ï¼Œè¯·ç¡®ä¿å·²å®‰è£…matplotlib, seabornå’ŒscipyåŒ…")

print("\n=== åˆ†æå®Œæˆ ===")
print("å»ºè®®ä½¿ç”¨ lake_data_log è¿›è¡Œåç»­çš„è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œå› ä¸º:")
print("1. Logå˜æ¢åæ•°æ®æ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒ")
print("2. é¿å…è´Ÿå€¼é—®é¢˜") 
print("3. æ›´ç¬¦åˆç¯å¢ƒæ•°æ®ç‰¹å¾")


ç­›é€‰åçš„æ•°æ®é‡: 159 æ¡è®°å½•
N2OèŒƒå›´: 0.000513 - 11.520649
Log(N2O)èŒƒå›´: -3.289883 - 1.061477

åŸå§‹N2Oæ•°æ®:
lake_data_original = {'<0.0001': {'mean': 0, 'std': 0, 'count': 0}, '0.0001-0.001': {'mean': 1.4445, 'std': 1.9072, 'count': 28}, '0.001-0.01': {'mean': 0.6287, 'std': 1.01, 'count': 33}, '0.01-0.1': {'mean': 0.4221, 'std': 1.6408, 'count': 98}}

Log(N2O)æ•°æ®:
lake_data_log = {'<0.0001': {'log_mean': 0, 'log_std': 0, 'count': 0}, '0.0001-0.001': {'log_mean': -0.2098, 'log_std': 0.5869, 'count': 28}, '0.001-0.01': {'log_mean': -0.7274, 'log_std': 0.804, 'count': 33}, '0.01-0.1': {'log_mean': -1.2103, 'log_std': 0.8405, 'count': 98}}

=== æ­£æ€æ€§æ£€éªŒ (Shapiro-Wilkæ£€éªŒ) ===

0.0001-0.001:
  åŸå§‹N2O: W=0.7211, p=0.000006 (éæ­£æ€)
  Log(N2O): W=0.9366, p=0.090555 (æ­£æ€)

0.001-0.01:
  åŸå§‹N2O: W=0.6017, p=0.000000 (éæ­£æ€)
  Log(N2O): W=0.9471, p=0.109204 (æ­£æ€)

0.01-0.1:
  åŸå§‹N2O: W=0.2336, p=0.000000 (éæ­£æ€)
  Log(N2O): W=0.9829, p=0.232887 (æ­£æ€)


#%% å°æ¹–æ³Šå»é™¤è´Ÿå€¼å’Œæœ€å¤§å€¼ 


import pandas as pd
import numpy as np

# 1. è¯»å–æ•°æ®
GHGdata = pd.read_excel('GHGdata_All250724_attributes_means.xlsx')

# 2. åªä¿ç•™ N2O éç©ºä¸”é¢ç§¯ <= 0.1 km2 çš„æ¹–æ³Šï¼Œå¹¶å»æ‰æœ€å¤§å€¼
df_filtered = GHGdata[GHGdata['N2O'].notna() & (GHGdata['Areakm2'] <= 0.1) & (GHGdata['N2O'] >= 0)].copy()

# æ‰¾åˆ°N2Oçš„æœ€å¤§å€¼å¹¶å»æ‰
max_n2o_index = df_filtered['N2O'].idxmax()
df = df_filtered.drop(max_n2o_index).copy()

print(f"åŸå§‹ç¬¦åˆæ¡ä»¶çš„æ•°æ®é‡: {len(df_filtered)}")
print(f"å»æ‰æœ€å¤§å€¼åçš„æ•°æ®é‡: {len(df)}")
print(f"å»æ‰çš„æœ€å¤§å€¼: {df_filtered.loc[max_n2o_index, 'N2O']}")

# æ‰“å°N2Oçš„åˆ†ä½æ•°ç»Ÿè®¡
print("\nN2O åˆ†ä½æ•°ç»Ÿè®¡ï¼š")
quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
for q in quantiles:
    value = df['N2O'].quantile(q)
    print(f"{int(q*100)}%åˆ†ä½æ•°: {value:.4f}")

print(f"\næœ€å°å€¼: {df['N2O'].min():.4f}")
print(f"æœ€å¤§å€¼: {df['N2O'].max():.4f}")
print(f"å¹³å‡å€¼: {df['N2O'].mean():.4f}")
print(f"æ ‡å‡†å·®: {df['N2O'].std():.4f}")

# 3. å®šä¹‰åˆ†ç»„åŒºé—´å’Œæ ‡ç­¾
bins = [0, 0.0001, 0.001, 0.01, 0.1]
labels = ['<0.0001', '0.0001-0.001', '0.001-0.01', '0.01-0.1']

# pd.cut ä¼šæŠŠ (0,0.0001] æ˜ å°„åˆ°ç¬¬ä¸€ä¸ªåŒºé—´ï¼Œå¦‚æœå¸Œæœ›æŠŠ 0.0 ä¹Ÿç®—åˆ°ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥è®¾ç½® include_lowest=True
df['size_bin'] = pd.cut(df['Areakm2'],
                        bins=bins,
                        labels=labels,
                        include_lowest=False,
                        right=True)

# 4. åˆ†ç»„å¹¶è®¡ç®—ç»Ÿè®¡é‡
stats = df.groupby('size_bin')['N2O'].agg(
    mean=lambda x: x.mean(),
    std=lambda x: x.std(),   
    count='count'
).reindex(labels)  # ä¿æŒé¡ºåº

# 5. å°†ç»“æœè½¬æ¢æˆå­—å…¸ï¼Œç©ºç»„å¡« 0
lake_data = {}
for label in labels:
    if pd.isna(stats.loc[label, 'count']) or stats.loc[label, 'count'] == 0:
        lake_data[label] = {'mean': 0, 'std': 0, 'count': 0}
    else:
        lake_data[label] = {
            'mean': round(stats.loc[label, 'mean'], 2),
            'std': round(stats.loc[label, 'std'], 2),
            'count': int(stats.loc[label, 'count'])
        }

# 6. è¾“å‡ºæ£€æŸ¥
print("\n" + "="*50)
print("å„é¢ç§¯åŒºé—´çš„ N2O ç»Ÿè®¡ï¼š")
for k, v in lake_data.items():
    print(f"{k}: mean={v['mean']}, std={v['std']}, count={v['count']}")

# 7. æœ€ç»ˆçš„ lake_data
print("\nlake_data =")
print(lake_data)


# å„é¢ç§¯åŒºé—´çš„ N2O ç»Ÿè®¡ï¼š
# <0.0001: mean=0, std=0, count=0
# 0.0001-0.001: mean=1.94, std=3.26, count=29
# 0.001-0.01: mean=0.61, std=1.0, count=34
# 0.01-0.1: mean=0.41, std=1.62, count=101

# lake_data =
# {'<0.0001': {'mean': 0, 'std': 0, 'count': 0}, 
#  '0.0001-0.001': {'mean': 1.94, 'std': 3.26, 'count': 29}, 
#  '0.001-0.01': {'mean': 0.61, 'std': 1.0, 'count': 34}, 
#  '0.01-0.1': {'mean': 0.41, 'std': 1.62, 'count': 101}}




#%% å°æ¹–æ³Šè’™ç‰¹å¡ç½—åˆ†æ


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple

# å®šä¹‰æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ•°æ®ï¼ˆåŒ…å«è¡¨é¢ç§¯ä¿¡æ¯ï¼‰
lake_data = {
    '<0.0001': {
        'mean': 0, 
        'std': 0, 
        'count': 0,
        'surface_area': 0  # 10Â³ kmÂ²
    }, 
    '0.0001-0.001': {
        'mean': 1.94, 
        'std': 3.26, 
        'count': 29,
        'surface_area': 15.04  # 10Â³ kmÂ²
    }, 
    '0.001-0.01': {
        'mean': 0.61, 
        'std': 1, 
        'count': 34,
        'surface_area': 71.60  # 10Â³ kmÂ²
    }, 
    '0.01-0.1': {
        'mean': 0.41, 
        'std': 1.62, 
        'count': 101,
        'surface_area': 223.67  # 10Â³ kmÂ²
    }
}

def run_monte_carlo_with_emissions(data: Dict[str, Dict[str, float]], 
                                  n_iterations: int = 10000) -> Dict[str, Dict[str, List[float]]]:
    """
    å¯¹æ¯ä¸ªæ¹–æ³Šå¤§å°ç±»åˆ«è¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œè®¡ç®—é€šé‡å’Œæ’æ”¾é‡
    
    å‚æ•°:
    - data: åŒ…å«æ¯ä¸ªå¤§å°ç±»åˆ«meanã€stdã€surface_areaçš„å­—å…¸
    - n_iterations: è¿­ä»£æ¬¡æ•°
    
    è¿”å›:
    - æ¯ä¸ªå¤§å°ç±»åˆ«çš„æ¨¡æ‹Ÿç»“æœå­—å…¸ï¼ŒåŒ…å«é€šé‡å’Œæ’æ”¾é‡
    """
    results = {}
    
    # å•ä½è½¬æ¢å› å­ï¼šmg N m-2 d-1 â†’ Tg N y-1
    # 1 mg = 10^-15 Tg, 1 kmÂ² = 10^6 mÂ², 1 year = 365 days
    # è¡¨é¢ç§¯å•ä½ï¼š10Â³ kmÂ²
    conversion_factor = 10**-15 * 10**6 * 365 * 10**3  # = 0.000365
    
    for size_class, values in data.items():
        if values['std'] == 0 and values['mean'] == 0:
            results[size_class] = {
                'flux': [0] * n_iterations,
                'emission': [0] * n_iterations
            }
            continue
            
        # ä½¿ç”¨æ­£æ€åˆ†å¸ƒç”ŸæˆéšæœºN2Oé€šé‡ (mg N m-2 d-1)
        simulated_flux = np.random.normal(
            loc=values['mean'],
            scale=values['std'],
            size=n_iterations
        )
        
        # è®¡ç®—æ’æ”¾é‡ (Tg N y-1)
        # æ’æ”¾é‡ = é€šé‡ Ã— è¡¨é¢ç§¯ Ã— è½¬æ¢å› å­
        simulated_emission = simulated_flux * values['surface_area'] * conversion_factor
        
        results[size_class] = {
            'flux': simulated_flux.tolist(),
            'emission': simulated_emission.tolist()
        }
    
    return results

def analyze_emissions_results(results: Dict[str, Dict[str, List[float]]], 
                            data: Dict[str, Dict[str, float]]) -> pd.DataFrame:
    """
    åˆ†æè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœï¼ŒåŒ…å«é€šé‡å’Œæ’æ”¾é‡åˆ†æ
    
    å‚æ•°:
    - results: æ¨¡æ‹Ÿç»“æœå­—å…¸
    - data: åŸå§‹æ•°æ®å­—å…¸
    
    è¿”å›:
    - åŒ…å«åˆ†æç»“æœçš„DataFrame
    """
    analysis = []
    
    for size_class in results:
        flux_values = np.array(results[size_class]['flux'])
        emission_values = np.array(results[size_class]['emission'])
        
        analysis.append({
            'Size Class': size_class,
            'Surface Area (10Â³ kmÂ²)': data[size_class]['surface_area'],
            'Original Flux Mean (mg N mâ»Â² dâ»Â¹)': data[size_class]['mean'],
            'Original Flux Std (mg N mâ»Â² dâ»Â¹)': data[size_class]['std'],
            'Simulated Flux Mean (mg N mâ»Â² dâ»Â¹)': np.mean(flux_values),
            'Flux 95% CI Lower (mg N mâ»Â² dâ»Â¹)': np.percentile(flux_values, 2.5),
            'Flux 95% CI Upper (mg N mâ»Â² dâ»Â¹)': np.percentile(flux_values, 97.5),
            'Emission Mean (Tg N yâ»Â¹)': np.mean(emission_values),
            'Emission Std (Tg N yâ»Â¹)': np.std(emission_values),
            'Emission 95% CI Lower (Tg N yâ»Â¹)': np.percentile(emission_values, 2.5),
            'Emission 95% CI Upper (Tg N yâ»Â¹)': np.percentile(emission_values, 97.5),
            'Emission 5% CI Lower (Tg N yâ»Â¹)': np.percentile(emission_values, 5),
            'Emission 95% CI Upper (Tg N yâ»Â¹)': np.percentile(emission_values, 95)
        })
    
    return pd.DataFrame(analysis)

def calculate_total_emissions(results: Dict[str, Dict[str, List[float]]]) -> Dict[str, float]:
    """
    è®¡ç®—æ€»ä½“N2Oæ’æ”¾é‡åŠå…¶ä¸ç¡®å®šæ€§
    
    å‚æ•°:
    - results: æ¨¡æ‹Ÿç»“æœå­—å…¸
    
    è¿”å›:
    - æ€»æ’æ”¾é‡ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰è¿­ä»£çš„æ€»æ’æ”¾é‡
    total_emissions = []
    n_iterations = len(list(results.values())[0]['emission'])
    
    for i in range(n_iterations):
        total = sum(results[size_class]['emission'][i] for size_class in results)
        total_emissions.append(total)
    
    total_emissions = np.array(total_emissions)
    
    return {
        'mean': np.mean(total_emissions),
        'std': np.std(total_emissions),
        '95% CI Lower': np.percentile(total_emissions, 2.5),
        '95% CI Upper': np.percentile(total_emissions, 97.5),
        '90% CI Lower': np.percentile(total_emissions, 5),
        '90% CI Upper': np.percentile(total_emissions, 95),
        'median': np.median(total_emissions)
    }

def plot_emission_distributions(results: Dict[str, Dict[str, List[float]]], 
                              data: Dict[str, Dict[str, float]]):
    """
    ç»˜åˆ¶æ’æ”¾é‡åˆ†å¸ƒå›¾
    """
    # è¿‡æ»¤æ‰æ²¡æœ‰æ•°æ®çš„ç±»åˆ«
    active_classes = {k: v for k, v in results.items() 
                     if data[k]['surface_area'] > 0}
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    for i, (size_class, values) in enumerate(active_classes.items()):
        emission_values = values['emission']
        
        axes[i].hist(emission_values, bins=50, alpha=0.7, density=True, 
                    color=f'C{i}', edgecolor='black', linewidth=0.5)
        axes[i].axvline(np.mean(emission_values), color='red', linestyle='--', 
                       linewidth=2, label=f'Mean: {np.mean(emission_values):.4f}')
        axes[i].axvline(np.percentile(emission_values, 2.5), color='orange', 
                       linestyle=':', label='95% CI')
        axes[i].axvline(np.percentile(emission_values, 97.5), color='orange', 
                       linestyle=':', alpha=0.7)
        
        axes[i].set_title(f'Size Class: {size_class}')
        axes[i].set_xlabel('Nâ‚‚O Emission (Tg N yâ»Â¹)')
        axes[i].set_ylabel('Probability Density')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    # å¦‚æœæœ‰ç©ºçš„å­å›¾ï¼Œéšè—å®ƒ
    for j in range(i+1, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    plt.suptitle('Nâ‚‚O Emission Distributions by Lake Size Class', 
                 fontsize=16, y=1.02)
    plt.show()

def create_summary_table(analysis_df: pd.DataFrame, 
                        total_stats: Dict[str, float]) -> pd.DataFrame:
    """
    åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    """
    # é€‰æ‹©å…³é”®åˆ—ç”¨äºæ±‡æ€»è¡¨
    summary_cols = ['Size Class', 'Surface Area (10Â³ kmÂ²)', 
                   'Emission Mean (Tg N yâ»Â¹)', 'Emission 95% CI Lower (Tg N yâ»Â¹)', 
                   'Emission 95% CI Upper (Tg N yâ»Â¹)']
    
    summary_df = analysis_df[summary_cols].copy()
    
    # æ·»åŠ æ€»è®¡è¡Œ
    total_row = pd.DataFrame({
        'Size Class': ['TOTAL'],
        'Surface Area (10Â³ kmÂ²)': [analysis_df['Surface Area (10Â³ kmÂ²)'].sum()],
        'Emission Mean (Tg N yâ»Â¹)': [total_stats['mean']],
        'Emission 95% CI Lower (Tg N yâ»Â¹)': [total_stats['95% CI Lower']],
        'Emission 95% CI Upper (Tg N yâ»Â¹)': [total_stats['95% CI Upper']]
    })
    
    summary_df = pd.concat([summary_df, total_row], ignore_index=True)
    
    return summary_df

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
    np.random.seed(42)
    
    print("=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ’æ”¾é‡è’™ç‰¹å¡æ´›åˆ†æ ===")
    print(f"è¿­ä»£æ¬¡æ•°: 10,000")
    print("-" * 60)
    
    # è¿è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
    print("æ­£åœ¨è¿è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ...")
    results = run_monte_carlo_with_emissions(lake_data)
    
    # åˆ†æç»“æœ
    analysis_df = analyze_emissions_results(results, lake_data)
    
    # è®¡ç®—æ€»ä½“æ’æ”¾é‡ç»Ÿè®¡
    total_stats = calculate_total_emissions(results)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n=== è¯¦ç»†åˆ†æç»“æœ ===")
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    print(analysis_df.round(6))
    
    # æ‰“å°æ±‡æ€»è¡¨
    print("\n=== æ±‡æ€»è¡¨ ===")
    summary_df = create_summary_table(analysis_df, total_stats)
    print(summary_df.round(6))
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"\n=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ€»æ’æ”¾é‡ç»Ÿè®¡ ===")
    print(f"å¹³å‡å€¼: {total_stats['mean']:.6f} Tg N yâ»Â¹")
    print(f"æ ‡å‡†å·®: {total_stats['std']:.6f} Tg N yâ»Â¹")
    print(f"ä¸­ä½æ•°: {total_stats['median']:.6f} Tg N yâ»Â¹")
    print(f"95%ç½®ä¿¡åŒºé—´: [{total_stats['95% CI Lower']:.6f}, {total_stats['95% CI Upper']:.6f}] Tg N yâ»Â¹")
    print(f"90%ç½®ä¿¡åŒºé—´: [{total_stats['90% CI Lower']:.6f}, {total_stats['90% CI Upper']:.6f}] Tg N yâ»Â¹")
    
    # è®¡ç®—å„å¤§å°ç±»åˆ«å¯¹æ€»æ’æ”¾é‡çš„è´¡çŒ®
    print(f"\n=== å„å¤§å°ç±»åˆ«è´¡çŒ®åˆ†æ ===")
    active_classes = [k for k in lake_data.keys() if lake_data[k]['surface_area'] > 0]
    for size_class in active_classes:
        class_mean = analysis_df[analysis_df['Size Class'] == size_class]['Emission Mean (Tg N yâ»Â¹)'].iloc[0]
        contribution = (class_mean / total_stats['mean']) * 100
        print(f"{size_class}: {class_mean:.6f} Tg N yâ»Â¹ ({contribution:.1f}%)")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    try:
        plot_emission_distributions(results, lake_data)
    except Exception as e:
        print(f"\næ³¨æ„: æ— æ³•ç»˜åˆ¶å›¾è¡¨ ({e})")
        print("å¦‚éœ€æŸ¥çœ‹åˆ†å¸ƒå›¾ï¼Œè¯·ç¡®ä¿å·²å®‰è£…matplotlibå’ŒseabornåŒ…")
        
        
=== æ±‡æ€»è¡¨ ===
     Size Class  Surface Area (10Â³ kmÂ²)  Emission Mean (Tg N yâ»Â¹)  \
0       <0.0001                    0.00                  0.000000   
1  0.0001-0.001                   15.04                  0.005288   
2    0.001-0.01                   71.60                  0.017657   
3      0.01-0.1                  223.67                  0.034047   
4         TOTAL                  310.31                  0.056993   

   Emission 95% CI Lower (Tg N yâ»Â¹)  Emission 95% CI Upper (Tg N yâ»Â¹)  
0                          0.000000                          0.000000  
1                          0.001402                          0.014178  
2                          0.002629                          0.061230  
3                          0.004760                          0.122632  
4                          0.016967                          0.153160  

=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ€»æ’æ”¾é‡ç»Ÿè®¡ï¼ˆåŸºäºLogæ­£æ€åˆ†å¸ƒï¼‰===
å¹³å‡å€¼: 0.056993 Tg N yâ»Â¹
æ ‡å‡†å·®: 0.038260 Tg N yâ»Â¹
ä¸­ä½æ•°: 0.046650 Tg N yâ»Â¹
95%ç½®ä¿¡åŒºé—´: [0.016967, 0.153160] Tg N yâ»Â¹
90%ç½®ä¿¡åŒºé—´: [0.019716, 0.126375] Tg N yâ»Â¹

=== å„å¤§å°ç±»åˆ«è´¡çŒ®åˆ†æ ===
0.0001-0.001: 0.005288 Tg N yâ»Â¹ (9.3%)
0.001-0.01: 0.017657 Tg N yâ»Â¹ (31.0%)
0.01-0.1: 0.034047 Tg N yâ»Â¹ (59.7%)


#%% è’™ç‰¹å¡ç½—åˆ†æ ä½¿ç”¨logN2O  0821

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple

# å®šä¹‰æ¹–æ³Šå¤§å°ç±»åˆ«çš„æ•°æ®ï¼ˆåŸºäºlog(N2O)ç»Ÿè®¡å€¼ï¼‰
lake_data_log = {
    '<0.0001': {
        'log_mean': 0, 
        'log_std': 0, 
        'count': 0,
        'surface_area': 0  # 10Â³ kmÂ²
    }, 
    '0.0001-0.001': {
        'log_mean': -0.2098, 
        'log_std': 0.5869, 
        'count': 28,
        'surface_area': 15.04  # 10Â³ kmÂ²
    }, 
    '0.001-0.01': {
        'log_mean': -0.7274, 
        'log_std': 0.804, 
        'count': 33,
        'surface_area': 71.60  # 10Â³ kmÂ²
    }, 
    '0.01-0.1': {
        'log_mean': -1.2103, 
        'log_std': 0.8405, 
        'count': 98,
        'surface_area': 223.67  # 10Â³ kmÂ²
    }
}

def run_monte_carlo_lognormal(data: Dict[str, Dict[str, float]], 
                             n_iterations: int = 10000) -> Dict[str, Dict[str, List[float]]]:
    """
    ä½¿ç”¨log(N2O)æ•°æ®è¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œè®¡ç®—é€šé‡å’Œæ’æ”¾é‡
    
    å‚æ•°:
    - data: åŒ…å«æ¯ä¸ªå¤§å°ç±»åˆ«log_meanã€log_stdã€surface_areaçš„å­—å…¸
    - n_iterations: è¿­ä»£æ¬¡æ•°
    
    è¿”å›:
    - æ¯ä¸ªå¤§å°ç±»åˆ«çš„æ¨¡æ‹Ÿç»“æœå­—å…¸ï¼ŒåŒ…å«é€šé‡å’Œæ’æ”¾é‡
    """
    results = {}
    
    # å•ä½è½¬æ¢å› å­ï¼šmg N m-2 d-1 â†’ Tg N y-1
    # 1 mg = 10^-15 Tg, 1 kmÂ² = 10^6 mÂ², 1 year = 365 days
    # è¡¨é¢ç§¯å•ä½ï¼š10Â³ kmÂ²
    conversion_factor = 10**-15 * 10**6 * 365 * 10**3  # = 0.000365
    
    for size_class, values in data.items():
        if values['log_std'] == 0 and values['log_mean'] == 0:
            results[size_class] = {
                'log_flux': [0] * n_iterations,
                'flux': [0] * n_iterations,
                'emission': [0] * n_iterations
            }
            continue
            
        # æ­¥éª¤1: ä½¿ç”¨æ­£æ€åˆ†å¸ƒç”Ÿæˆlog(N2O)å€¼
        simulated_log_flux = np.random.normal(
            loc=values['log_mean'],
            scale=values['log_std'],
            size=n_iterations
        )
        
        # æ­¥éª¤2: è½¬æ¢å›åŸå°ºåº¦ (mg N m-2 d-1)
        # N2O = exp(log(N2O))
        simulated_flux = np.exp(simulated_log_flux)
        
        # æ­¥éª¤3: è®¡ç®—æ’æ”¾é‡ (Tg N y-1)
        # æ’æ”¾é‡ = é€šé‡ Ã— è¡¨é¢ç§¯ Ã— è½¬æ¢å› å­
        simulated_emission = simulated_flux * values['surface_area'] * conversion_factor
        
        results[size_class] = {
            'log_flux': simulated_log_flux.tolist(),
            'flux': simulated_flux.tolist(),
            'emission': simulated_emission.tolist()
        }
    
    return results

def analyze_lognormal_results(results: Dict[str, Dict[str, List[float]]], 
                             data: Dict[str, Dict[str, float]]) -> pd.DataFrame:
    """
    åˆ†æåŸºäºlogæ­£æ€åˆ†å¸ƒçš„è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœ
    
    å‚æ•°:
    - results: æ¨¡æ‹Ÿç»“æœå­—å…¸
    - data: åŸå§‹æ•°æ®å­—å…¸
    
    è¿”å›:
    - åŒ…å«åˆ†æç»“æœçš„DataFrame
    """
    analysis = []
    
    for size_class in results:
        log_flux_values = np.array(results[size_class]['log_flux'])
        flux_values = np.array(results[size_class]['flux'])
        emission_values = np.array(results[size_class]['emission'])
        
        # è®¡ç®—ç†è®ºå€¼ï¼ˆåŸºäºå¯¹æ•°æ­£æ€åˆ†å¸ƒçš„æ€§è´¨ï¼‰
        if data[size_class]['log_std'] > 0:
            # å¯¹äºå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼ŒåŸå°ºåº¦çš„ç†è®ºå‡å€¼å’Œæ–¹å·®
            theoretical_mean = np.exp(data[size_class]['log_mean'] + 0.5 * data[size_class]['log_std']**2)
            theoretical_var = (np.exp(data[size_class]['log_std']**2) - 1) * np.exp(2 * data[size_class]['log_mean'] + data[size_class]['log_std']**2)
            theoretical_std = np.sqrt(theoretical_var)
        else:
            theoretical_mean = 0
            theoretical_std = 0
        
        analysis.append({
            'Size Class': size_class,
            'Surface Area (10Â³ kmÂ²)': data[size_class]['surface_area'],
            'Original Log Mean': data[size_class]['log_mean'],
            'Original Log Std': data[size_class]['log_std'],
            'Theoretical Flux Mean (mg N mâ»Â² dâ»Â¹)': theoretical_mean,
            'Theoretical Flux Std (mg N mâ»Â² dâ»Â¹)': theoretical_std,
            'Simulated Log Flux Mean': np.mean(log_flux_values) if len(log_flux_values) > 0 else 0,
            'Simulated Log Flux Std': np.std(log_flux_values) if len(log_flux_values) > 0 else 0,
            'Simulated Flux Mean (mg N mâ»Â² dâ»Â¹)': np.mean(flux_values) if len(flux_values) > 0 else 0,
            'Simulated Flux Std (mg N mâ»Â² dâ»Â¹)': np.std(flux_values) if len(flux_values) > 0 else 0,
            'Flux 95% CI Lower (mg N mâ»Â² dâ»Â¹)': np.percentile(flux_values, 2.5) if len(flux_values) > 0 else 0,
            'Flux 95% CI Upper (mg N mâ»Â² dâ»Â¹)': np.percentile(flux_values, 97.5) if len(flux_values) > 0 else 0,
            'Emission Mean (Tg N yâ»Â¹)': np.mean(emission_values) if len(emission_values) > 0 else 0,
            'Emission Std (Tg N yâ»Â¹)': np.std(emission_values) if len(emission_values) > 0 else 0,
            'Emission 95% CI Lower (Tg N yâ»Â¹)': np.percentile(emission_values, 2.5) if len(emission_values) > 0 else 0,
            'Emission 95% CI Upper (Tg N yâ»Â¹)': np.percentile(emission_values, 97.5) if len(emission_values) > 0 else 0,
            'Emission 90% CI Lower (Tg N yâ»Â¹)': np.percentile(emission_values, 5) if len(emission_values) > 0 else 0,
            'Emission 90% CI Upper (Tg N yâ»Â¹)': np.percentile(emission_values, 95) if len(emission_values) > 0 else 0
        })
    
    return pd.DataFrame(analysis)

def calculate_total_emissions(results: Dict[str, Dict[str, List[float]]]) -> Dict[str, float]:
    """
    è®¡ç®—æ€»ä½“N2Oæ’æ”¾é‡åŠå…¶ä¸ç¡®å®šæ€§
    
    å‚æ•°:
    - results: æ¨¡æ‹Ÿç»“æœå­—å…¸
    
    è¿”å›:
    - æ€»æ’æ”¾é‡ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰è¿­ä»£çš„æ€»æ’æ”¾é‡
    total_emissions = []
    n_iterations = len(list(results.values())[0]['emission'])
    
    for i in range(n_iterations):
        total = sum(results[size_class]['emission'][i] for size_class in results)
        total_emissions.append(total)
    
    total_emissions = np.array(total_emissions)
    
    return {
        'mean': np.mean(total_emissions),
        'std': np.std(total_emissions),
        '95% CI Lower': np.percentile(total_emissions, 2.5),
        '95% CI Upper': np.percentile(total_emissions, 97.5),
        '90% CI Lower': np.percentile(total_emissions, 5),
        '90% CI Upper': np.percentile(total_emissions, 95),
        'median': np.median(total_emissions)
    }

def plot_lognormal_distributions(results: Dict[str, Dict[str, List[float]]], 
                                data: Dict[str, Dict[str, float]]):
    """
    ç»˜åˆ¶logæ­£æ€åˆ†å¸ƒçš„æ’æ”¾é‡åˆ†å¸ƒå›¾
    """
    # è¿‡æ»¤æ‰æ²¡æœ‰æ•°æ®çš„ç±»åˆ«
    active_classes = {k: v for k, v in results.items() 
                     if data[k]['surface_area'] > 0}
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    # ç»˜åˆ¶é€šé‡åˆ†å¸ƒï¼ˆåŸå°ºåº¦ï¼‰
    for i, (size_class, values) in enumerate(active_classes.items()):
        flux_values = values['flux']
        
        # åŸå°ºåº¦é€šé‡åˆ†å¸ƒ
        axes[i].hist(flux_values, bins=50, alpha=0.7, density=True, 
                    color=f'C{i}', edgecolor='black', linewidth=0.5)
        axes[i].axvline(np.mean(flux_values), color='red', linestyle='--', 
                       linewidth=2, label=f'Mean: {np.mean(flux_values):.3f}')
        axes[i].axvline(np.percentile(flux_values, 2.5), color='orange', 
                       linestyle=':', label='95% CI')
        axes[i].axvline(np.percentile(flux_values, 97.5), color='orange', 
                       linestyle=':', alpha=0.7)
        
        axes[i].set_title(f'Flux Distribution: {size_class}')
        axes[i].set_xlabel('Nâ‚‚O Flux (mg N mâ»Â² dâ»Â¹)')
        axes[i].set_ylabel('Probability Density')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶æ’æ”¾é‡åˆ†å¸ƒ
    for i, (size_class, values) in enumerate(active_classes.items()):
        emission_values = values['emission']
        idx = i + 3  # ç¬¬äºŒè¡Œ
        
        axes[idx].hist(emission_values, bins=50, alpha=0.7, density=True, 
                      color=f'C{i}', edgecolor='black', linewidth=0.5)
        axes[idx].axvline(np.mean(emission_values), color='red', linestyle='--', 
                         linewidth=2, label=f'Mean: {np.mean(emission_values):.4f}')
        axes[idx].axvline(np.percentile(emission_values, 2.5), color='orange', 
                         linestyle=':', label='95% CI')
        axes[idx].axvline(np.percentile(emission_values, 97.5), color='orange', 
                         linestyle=':', alpha=0.7)
        
        axes[idx].set_title(f'Emission Distribution: {size_class}')
        axes[idx].set_xlabel('Nâ‚‚O Emission (Tg N yâ»Â¹)')
        axes[idx].set_ylabel('Probability Density')
        axes[idx].legend()
        axes[idx].grid(True, alpha=0.3)
    
    # éšè—å¤šä½™çš„å­å›¾
    for j in range(len(active_classes) + 3, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    plt.suptitle('Nâ‚‚O Flux and Emission Distributions (Log-Normal Based)', 
                 fontsize=16, y=1.02)
    plt.show()

def create_summary_table(analysis_df: pd.DataFrame, 
                        total_stats: Dict[str, float]) -> pd.DataFrame:
    """
    åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    """
    # é€‰æ‹©å…³é”®åˆ—ç”¨äºæ±‡æ€»è¡¨
    summary_cols = ['Size Class', 'Surface Area (10Â³ kmÂ²)', 
                   'Emission Mean (Tg N yâ»Â¹)', 'Emission 95% CI Lower (Tg N yâ»Â¹)', 
                   'Emission 95% CI Upper (Tg N yâ»Â¹)']
    
    summary_df = analysis_df[summary_cols].copy()
    
    # æ·»åŠ æ€»è®¡è¡Œ
    total_row = pd.DataFrame({
        'Size Class': ['TOTAL'],
        'Surface Area (10Â³ kmÂ²)': [analysis_df['Surface Area (10Â³ kmÂ²)'].sum()],
        'Emission Mean (Tg N yâ»Â¹)': [total_stats['mean']],
        'Emission 95% CI Lower (Tg N yâ»Â¹)': [total_stats['95% CI Lower']],
        'Emission 95% CI Upper (Tg N yâ»Â¹)': [total_stats['95% CI Upper']]
    })
    
    summary_df = pd.concat([summary_df, total_row], ignore_index=True)
    
    return summary_df

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
    np.random.seed(42)
    
    print("=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ’æ”¾é‡è’™ç‰¹å¡æ´›åˆ†æï¼ˆåŸºäºLogæ­£æ€åˆ†å¸ƒï¼‰===")
    print(f"è¿­ä»£æ¬¡æ•°: 10,000")
    print("æ–¹æ³•: ä½¿ç”¨log(Nâ‚‚O)æ­£æ€åˆ†å¸ƒ â†’ æŒ‡æ•°å˜æ¢åˆ°åŸå°ºåº¦")
    print("-" * 70)
    
    # è¿è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
    print("æ­£åœ¨è¿è¡ŒåŸºäºlogæ­£æ€åˆ†å¸ƒçš„è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ...")
    results = run_monte_carlo_lognormal(lake_data_log)
    
    # åˆ†æç»“æœ
    analysis_df = analyze_lognormal_results(results, lake_data_log)
    
    # è®¡ç®—æ€»ä½“æ’æ”¾é‡ç»Ÿè®¡
    total_stats = calculate_total_emissions(results)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n=== è¯¦ç»†åˆ†æç»“æœ ===")
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    print(analysis_df.round(6))
    
    # éªŒè¯logå°ºåº¦çš„ç»Ÿè®¡é‡
    print("\n=== Logå°ºåº¦ç»Ÿè®¡éªŒè¯ ===")
    for size_class in lake_data_log:
        if lake_data_log[size_class]['surface_area'] > 0:
            original_log_mean = lake_data_log[size_class]['log_mean']
            original_log_std = lake_data_log[size_class]['log_std']
            simulated_log_mean = analysis_df[analysis_df['Size Class'] == size_class]['Simulated Log Flux Mean'].iloc[0]
            simulated_log_std = analysis_df[analysis_df['Size Class'] == size_class]['Simulated Log Flux Std'].iloc[0]
            
            print(f"{size_class}:")
            print(f"  åŸå§‹logå‡å€¼: {original_log_mean:.4f}, æ¨¡æ‹Ÿlogå‡å€¼: {simulated_log_mean:.4f}")
            print(f"  åŸå§‹logæ ‡å‡†å·®: {original_log_std:.4f}, æ¨¡æ‹Ÿlogæ ‡å‡†å·®: {simulated_log_std:.4f}")
    
    # æ‰“å°æ±‡æ€»è¡¨
    print("\n=== æ±‡æ€»è¡¨ ===")
    summary_df = create_summary_table(analysis_df, total_stats)
    print(summary_df.round(6))
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"\n=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ€»æ’æ”¾é‡ç»Ÿè®¡ï¼ˆåŸºäºLogæ­£æ€åˆ†å¸ƒï¼‰===")
    print(f"å¹³å‡å€¼: {total_stats['mean']:.6f} Tg N yâ»Â¹")
    print(f"æ ‡å‡†å·®: {total_stats['std']:.6f} Tg N yâ»Â¹")
    print(f"ä¸­ä½æ•°: {total_stats['median']:.6f} Tg N yâ»Â¹")
    print(f"95%ç½®ä¿¡åŒºé—´: [{total_stats['95% CI Lower']:.6f}, {total_stats['95% CI Upper']:.6f}] Tg N yâ»Â¹")
    print(f"90%ç½®ä¿¡åŒºé—´: [{total_stats['90% CI Lower']:.6f}, {total_stats['90% CI Upper']:.6f}] Tg N yâ»Â¹")
    
    # è®¡ç®—å„å¤§å°ç±»åˆ«å¯¹æ€»æ’æ”¾é‡çš„è´¡çŒ®
    print(f"\n=== å„å¤§å°ç±»åˆ«è´¡çŒ®åˆ†æ ===")
    active_classes = [k for k in lake_data_log.keys() if lake_data_log[k]['surface_area'] > 0]
    for size_class in active_classes:
        class_mean = analysis_df[analysis_df['Size Class'] == size_class]['Emission Mean (Tg N yâ»Â¹)'].iloc[0]
        contribution = (class_mean / total_stats['mean']) * 100
        print(f"{size_class}: {class_mean:.6f} Tg N yâ»Â¹ ({contribution:.1f}%)")
    
    # ç†è®ºvsæ¨¡æ‹Ÿå¯¹æ¯”
    print(f"\n=== ç†è®ºå€¼vsæ¨¡æ‹Ÿå€¼å¯¹æ¯” ===")
    for size_class in active_classes:
        row = analysis_df[analysis_df['Size Class'] == size_class].iloc[0]
        theoretical_mean = row['Theoretical Flux Mean (mg N mâ»Â² dâ»Â¹)']
        simulated_mean = row['Simulated Flux Mean (mg N mâ»Â² dâ»Â¹)']
        print(f"{size_class}:")
        print(f"  ç†è®ºå‡å€¼: {theoretical_mean:.4f}, æ¨¡æ‹Ÿå‡å€¼: {simulated_mean:.4f}")
        print(f"  ç›¸å¯¹è¯¯å·®: {abs(theoretical_mean - simulated_mean) / theoretical_mean * 100:.2f}%")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    try:
        plot_lognormal_distributions(results, lake_data_log)
    except Exception as e:
        print(f"\næ³¨æ„: æ— æ³•ç»˜åˆ¶å›¾è¡¨ ({e})")
        print("å¦‚éœ€æŸ¥çœ‹åˆ†å¸ƒå›¾ï¼Œè¯·ç¡®ä¿å·²å®‰è£…matplotlibå’ŒseabornåŒ…")


=== æ±‡æ€»è¡¨ ===
     Size Class  Surface Area (10Â³ kmÂ²)  Emission Mean (Tg N yâ»Â¹)  \
0       <0.0001                    0.00                  0.000000   
1  0.0001-0.001                   15.04                  0.005288   
2    0.001-0.01                   71.60                  0.017657   
3      0.01-0.1                  223.67                  0.034047   
4         TOTAL                  310.31                  0.056993   

   Emission 95% CI Lower (Tg N yâ»Â¹)  Emission 95% CI Upper (Tg N yâ»Â¹)  
0                          0.000000                          0.000000  
1                          0.001402                          0.014178  
2                          0.002629                          0.061230  
3                          0.004760                          0.122632  
4                          0.016967                          0.153160  

=== å…¨çƒå°æ¹–æ³ŠNâ‚‚Oæ€»æ’æ”¾é‡ç»Ÿè®¡ï¼ˆåŸºäºLogæ­£æ€åˆ†å¸ƒï¼‰===
å¹³å‡å€¼: 0.056993 Tg N yâ»Â¹
æ ‡å‡†å·®: 0.038260 Tg N yâ»Â¹
ä¸­ä½æ•°: 0.046650 Tg N yâ»Â¹
95%ç½®ä¿¡åŒºé—´: [0.016967, 0.153160] Tg N yâ»Â¹
90%ç½®ä¿¡åŒºé—´: [0.019716, 0.126375] Tg N yâ»Â¹

=== å„å¤§å°ç±»åˆ«è´¡çŒ®åˆ†æ ===
0.0001-0.001: 0.005288 Tg N yâ»Â¹ (9.3%)
0.001-0.01: 0.017657 Tg N yâ»Â¹ (31.0%)
0.01-0.1: 0.034047 Tg N yâ»Â¹ (59.7%)
